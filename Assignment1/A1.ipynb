{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from collections import OrderedDict\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './Dataset/train/'# Should we use: os.path.join(os.getcwd(), \"OriginalDataset\")\n",
    "val_path = './Dataset/val/'\n",
    "test_path = './Dataset/test/'\n",
    "dataset_folder = os.path.join(os.getcwd(), \"OriginalDataset\")\n",
    "original_path = dataset_folder +'/dependency_treebank/'\n",
    "\n",
    "embedding_dimension = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request  #  download files\n",
    "import zipfile  #  unzip files\n",
    "\n",
    "def download_dataset():\n",
    "    dataset_folder = os.path.join(os.getcwd(), \"OriginalDataset\")\n",
    "\n",
    "    if not os.path.exists(dataset_folder):\n",
    "        os.makedirs(dataset_folder)\n",
    "\n",
    "    url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
    "\n",
    "    dataset_path = os.path.join(dataset_folder, \"dependency_treebank.zip\")\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        urllib.request.urlretrieve(url, dataset_path)\n",
    "        print(\"Successful download\")\n",
    "\n",
    "    with zipfile.ZipFile(dataset_path,\"r\") as zip_ref:\n",
    "        zip_ref.extractall(dataset_folder)\n",
    "    print(\"Successful extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful extraction\n"
     ]
    }
   ],
   "source": [
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(train_path, val_path, test_path, original_path):\n",
    "    if not os.path.exists(train_path):\n",
    "        print(\"making directory\")\n",
    "        os.makedirs(train_path)\n",
    "    if not os.path.exists(val_path):\n",
    "        os.makedirs(val_path)\n",
    "    if not os.path.exists(test_path):\n",
    "        os.makedirs(test_path)\n",
    "\n",
    "    original_dataset = os.listdir(original_path)\n",
    "    \n",
    "    original_dataset.sort()\n",
    "    org_train = original_dataset[0:100]\n",
    "    org_val = original_dataset[100:150]\n",
    "    org_test = original_dataset[150:]\n",
    "\n",
    "    for f in org_train:\n",
    "        os.rename(original_path+f, train_path+f)\n",
    "    for f in org_val:\n",
    "        os.rename(original_path+f, val_path+f)\n",
    "    for f in org_test:\n",
    "        os.rename(original_path+f, test_path+f)\n",
    "    print(\"Successful spilt\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful spilt\n"
     ]
    }
   ],
   "source": [
    "split_dataset(train_path, val_path, test_path, original_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(lst): # DO WE NEED THIS?\n",
    "    lowercase_list = [x.lower() for x in lst]\n",
    "    return lowercase_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(dataset_path: str):\n",
    "    dataframe_rows = []\n",
    "    documents = os.listdir(dataset_path)\n",
    "    for document in documents:\n",
    "        path = os.path.join(dataset_path, document)\n",
    "        with open(path, 'r') as f:\n",
    "            a = [[x for x in ln.split()] for ln in f]\n",
    "        a2 = [x for x in a if x != []] # OBS! Removing all empty lines in file so we can make an array\n",
    "        arr = np.array(a2)\n",
    "        text = list(arr[:, 0])\n",
    "        tagg = list(arr[:, 1])\n",
    "        #POStuple =  [tagg[x] for x in range(len(text))] #[(text[x], tagg[x]) for x in range(len(text))]\n",
    "        document_id = int(document[4:8])\n",
    "        dataframe_row = {\n",
    "            \"document_id\": document_id,\n",
    "            \"text\": text,\n",
    "            \"POS\": tagg\n",
    "        }\n",
    "        dataframe_rows.append(dataframe_row)\n",
    "    # transform the list of rows in a proper dataframe\n",
    "    df = pd.DataFrame(dataframe_rows)\n",
    "    df = df[[\"document_id\", \"text\", \"POS\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>text</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95</td>\n",
       "      <td>[In, reference, to, your, Oct., 9, page-one, a...</td>\n",
       "      <td>[IN, NN, TO, PRP$, NNP, CD, NN, NN, ``, NNP, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>[Judging, from, the, Americana, in, Haruki, Mu...</td>\n",
       "      <td>[VBG, IN, DT, NNS, IN, NNP, NNP, POS, ``, DT, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>[Sir, Peter, Walters, ,, 58-year-old, chairman...</td>\n",
       "      <td>[NNP, NNP, NNP, ,, JJ, NN, IN, NNP, NNP, NNP, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>[PAPERS, :, Backe, Group, Inc., agreed, to, ac...</td>\n",
       "      <td>[NNS, :, NNP, NNP, NNP, VBD, TO, VB, NNP, NNP,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
       "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>82</td>\n",
       "      <td>[Criticism, in, the, U.S., over, recent, Japan...</td>\n",
       "      <td>[NNP, IN, DT, NNP, IN, JJ, JJ, NNS, VBZ, VBG, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>71</td>\n",
       "      <td>[When, Warren, Winiarski, ,, proprietor, of, S...</td>\n",
       "      <td>[WRB, NNP, NNP, ,, NN, IN, NNP, POS, NNP, NNP,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>20</td>\n",
       "      <td>[The, U.S., ,, claiming, some, success, in, it...</td>\n",
       "      <td>[DT, NNP, ,, VBG, DT, NN, IN, PRP$, NN, NN, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>14</td>\n",
       "      <td>[Norman, Ricken, ,, 52, years, old, and, forme...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, CC, NN,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>45</td>\n",
       "      <td>[Since, chalk, first, touched, slate, ,, schoo...</td>\n",
       "      <td>[IN, NN, RB, VBD, NN, ,, NN, VBP, VBN, TO, VB,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    document_id                                               text  \\\n",
       "0            95  [In, reference, to, your, Oct., 9, page-one, a...   \n",
       "1            37  [Judging, from, the, Americana, in, Haruki, Mu...   \n",
       "2            66  [Sir, Peter, Walters, ,, 58-year-old, chairman...   \n",
       "3            52  [PAPERS, :, Backe, Group, Inc., agreed, to, ac...   \n",
       "4             3  [A, form, of, asbestos, once, used, to, make, ...   \n",
       "..          ...                                                ...   \n",
       "95           82  [Criticism, in, the, U.S., over, recent, Japan...   \n",
       "96           71  [When, Warren, Winiarski, ,, proprietor, of, S...   \n",
       "97           20  [The, U.S., ,, claiming, some, success, in, it...   \n",
       "98           14  [Norman, Ricken, ,, 52, years, old, and, forme...   \n",
       "99           45  [Since, chalk, first, touched, slate, ,, schoo...   \n",
       "\n",
       "                                                  POS  \n",
       "0   [IN, NN, TO, PRP$, NNP, CD, NN, NN, ``, NNP, N...  \n",
       "1   [VBG, IN, DT, NNS, IN, NNP, NNP, POS, ``, DT, ...  \n",
       "2   [NNP, NNP, NNP, ,, JJ, NN, IN, NNP, NNP, NNP, ...  \n",
       "3   [NNS, :, NNP, NNP, NNP, VBD, TO, VB, NNP, NNP,...  \n",
       "4   [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...  \n",
       "..                                                ...  \n",
       "95  [NNP, IN, DT, NNP, IN, JJ, JJ, NNS, VBZ, VBG, ...  \n",
       "96  [WRB, NNP, NNP, ,, NN, IN, NNP, POS, NNP, NNP,...  \n",
       "97  [DT, NNP, ,, VBG, DT, NN, IN, PRP$, NN, NN, ,,...  \n",
       "98  [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, CC, NN,...  \n",
       "99  [IN, NN, RB, VBD, NN, ,, NN, VBP, VBN, TO, VB,...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = create_dataframe(train_path)\n",
    "df_val = create_dataframe(val_path)\n",
    "df_test = create_dataframe(test_path)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer_train = Tokenizer()\n",
    "\n",
    "# UNCOMMENT IF WE WANT TO INCLUDE GOLVE vocabualry\n",
    "# word_tokenizer_train.fit_on_texts(embedding_model.index_to_key) \n",
    "word_tokenizer_train.fit_on_texts(df_train[\"text\"])\n",
    "\n",
    "x_encoded_train = word_tokenizer_train.texts_to_sequences(df_train[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_tokenizer_train = Tokenizer()\n",
    "\n",
    "tag_tokenizer_train.fit_on_texts(df_train[\"POS\"])\n",
    "y_encoded_train = tag_tokenizer_train.texts_to_sequences(df_train[\"POS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_length(seqs):\n",
    "    max_len = 0\n",
    "    for seq in seqs:\n",
    "        if len(seq) > max_len:\n",
    "            max_len = len(seq)\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_padded_train = pad_idx_seqs(x_encoded_train, find_max_length(x_encoded_train))\n",
    "y_padded_train = pad_idx_seqs(y_encoded_train, find_max_length(y_encoded_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed the words using GloVe embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    # Check download\n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Word2Vec: 300\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = load_embedding_model(embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW METHODE TO GET WORD TO INDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7404\n"
     ]
    }
   ],
   "source": [
    "VOCABULARY_SIZE = len(word_tokenizer_train.word_index) + 1\n",
    "\n",
    "word2idx_train = word_tokenizer_train.word_index\n",
    "idx2word_train = word_tokenizer_train.index_word\n",
    "print(len(word2idx_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx_train = tag_tokenizer_train.word_index\n",
    "idx2tag_train = tag_tokenizer_train.index_word\n",
    "len(tag2idx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_VOCABULARY_SIZE = len(tag2idx_train) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBS Computes the OOV with random embeddings\n",
    "def create_embedding_matrix(embedding_model, embedding_dimension, word_to_idx):\n",
    "    embedding_matrix = np.zeros((len(word_to_idx)+1, embedding_dimension), dtype=np.float32)\n",
    "    for word, idx in word_to_idx.items():\n",
    "        try:\n",
    "            embedding_vector = embedding_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "                                \n",
    "    return embedding_matrix\n",
    "\n",
    "def expand_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, old_embedding_matrix):\n",
    "    embedding_matrix = np.zeros((len(word_to_idx), embedding_dimension), dtype=np.float32)\n",
    "    embedding_matrix[0:len(old_embedding_matrix)] = old_embedding_matrix\n",
    "    \n",
    "    for word, idx in word_to_idx.items():\n",
    "        if idx >= len(old_embedding_matrix):\n",
    "            try:\n",
    "                embedding_vector = embedding_model[word]\n",
    "            except (KeyError, TypeError):\n",
    "                embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "                \n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = create_embedding_matrix(embedding_model, embedding_dimension, word2idx_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define x train and y train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_padded_train\n",
    "y_train = to_categorical(y_padded_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create baseline model\n",
    "\n",
    "##### two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://www.kaggle.com/tanyadayanand/pos-tagging-using-rnn/notebook\n",
    "\n",
    "Not sure about the parameters etc.. but at least it works to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_model():\n",
    "    bidirect_model = keras.Sequential()\n",
    "    bidirect_model.add(layers.Embedding(input_dim = VOCABULARY_SIZE,\n",
    "                                 output_dim    = embedding_dimension,\n",
    "                                 input_length  = find_max_length(x_train),\n",
    "                                 weights       = [embedding_matrix],\n",
    "                                 mask_zero     = True,\n",
    "                                 trainable = False\n",
    "                                ))\n",
    "    bidirect_model.add(layers.Bidirectional(layers.LSTM(64, return_sequences=True)))\n",
    "    #bidirect_model.add(layers.TimeDistributed(layers.Dense(len(tag_to_idx), activation='softmax')))\n",
    "    bidirect_model.add(layers.Dense(TAG_VOCABULARY_SIZE, activation='softmax'))\n",
    "    \n",
    "    return bidirect_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 14:26:00.385710: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2900, 50)          370250    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 2900, 128)        58880     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2900, 46)          5934      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 435,064\n",
      "Trainable params: 64,814\n",
      "Non-trainable params: 370,250\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline_model = create_baseline_model()\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 12s 2s/step - loss: 0.0035 - mae: 0.0425 - acc: 0.0259\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 7s 1s/step - loss: 0.0035 - mae: 0.0424 - acc: 0.0712\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0034 - mae: 0.0422 - acc: 0.1053\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0034 - mae: 0.0420 - acc: 0.1146\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0034 - mae: 0.0418 - acc: 0.1287\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0033 - mae: 0.0414 - acc: 0.1442\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0033 - mae: 0.0411 - acc: 0.1622\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0033 - mae: 0.0407 - acc: 0.1997\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0033 - mae: 0.0404 - acc: 0.2035\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0033 - mae: 0.0402 - acc: 0.2341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16a49fc10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.compile(loss=\"mse\",\n",
    "              optimizer=\"Adam\",\n",
    "              metrics=['mae', \"acc\"])\n",
    "baseline_model.fit(x=x_train, batch_size=25, y=y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
