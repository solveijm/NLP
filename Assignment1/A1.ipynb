{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp39-cp39-macosx_10_9_x86_64.whl (24.0 MB)\n",
      "     |████████████████████████████████| 24.0 MB 57 kB/s             \n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "     |████████████████████████████████| 58 kB 337 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/site-packages (from gensim) (1.20.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/site-packages (from gensim) (1.6.2)\n",
      "Installing collected packages: smart-open, gensim\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Successfully installed gensim-4.1.2 smart-open-5.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful extraction\n"
     ]
    }
   ],
   "source": [
    "import urllib.request  #  download files\n",
    "import zipfile  #  unzip files\n",
    "\n",
    "dataset_folder = os.path.join(os.getcwd(), \"OriginalDataset\")\n",
    "\n",
    "if not os.path.exists(dataset_folder):\n",
    "    os.makedirs(dataset_folder)\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
    "\n",
    "dataset_path = os.path.join(dataset_folder, \"dependency_treebank.zip\")\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    urllib.request.urlretrieve(url, dataset_path)\n",
    "    print(\"Successful download\")\n",
    "\n",
    "with zipfile.ZipFile(dataset_path,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(dataset_folder)\n",
    "print(\"Successful extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './Dataset/train/'# Should we use: os.path.join(os.getcwd(), \"OriginalDataset\")\n",
    "val_path = './Dataset/val/'\n",
    "test_path = './Dataset/test/'\n",
    "original_path = dataset_folder +'/dependency_treebank/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(train_path):\n",
    "    print(\"making directory\")\n",
    "    os.makedirs(train_path)\n",
    "if not os.path.exists(val_path):\n",
    "    os.makedirs(val_path)\n",
    "if not os.path.exists(test_path):\n",
    "    os.makedirs(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wsj_0095.dp', 'wsj_0184.dp', 'wsj_0177.dp', 'wsj_0037.dp', 'wsj_0126.dp', 'wsj_0066.dp', 'wsj_0052.dp', 'wsj_0112.dp', 'wsj_0003.dp', 'wsj_0143.dp', 'wsj_0153.dp', 'wsj_0013.dp', 'wsj_0102.dp', 'wsj_0042.dp', 'wsj_0076.dp', 'wsj_0136.dp', 'wsj_0027.dp', 'wsj_0167.dp', 'wsj_0194.dp', 'wsj_0085.dp', 'wsj_0007.dp', 'wsj_0147.dp', 'wsj_0056.dp', 'wsj_0116.dp', 'wsj_0180.dp', 'wsj_0091.dp', 'wsj_0122.dp', 'wsj_0062.dp', 'wsj_0173.dp', 'wsj_0033.dp', 'wsj_0023.dp', 'wsj_0163.dp', 'wsj_0072.dp', 'wsj_0132.dp', 'wsj_0081.dp', 'wsj_0190.dp', 'wsj_0106.dp', 'wsj_0046.dp', 'wsj_0157.dp', 'wsj_0017.dp', 'wsj_0006.dp', 'wsj_0146.dp', 'wsj_0057.dp', 'wsj_0117.dp', 'wsj_0181.dp', 'wsj_0090.dp', 'wsj_0123.dp', 'wsj_0063.dp', 'wsj_0172.dp', 'wsj_0032.dp', 'wsj_0022.dp', 'wsj_0162.dp', 'wsj_0073.dp', 'wsj_0133.dp', 'wsj_0080.dp', 'wsj_0191.dp', 'wsj_0107.dp', 'wsj_0047.dp', 'wsj_0156.dp', 'wsj_0016.dp', 'wsj_0094.dp', 'wsj_0185.dp', 'wsj_0176.dp', 'wsj_0036.dp', 'wsj_0127.dp', 'wsj_0067.dp', 'wsj_0053.dp', 'wsj_0113.dp', 'wsj_0002.dp', 'wsj_0142.dp', 'wsj_0152.dp', 'wsj_0012.dp', 'wsj_0103.dp', 'wsj_0043.dp', 'wsj_0077.dp', 'wsj_0137.dp', 'wsj_0026.dp', 'wsj_0166.dp', 'wsj_0195.dp', 'wsj_0084.dp', 'wsj_0058.dp', 'wsj_0118.dp', 'wsj_0009.dp', 'wsj_0149.dp', 'wsj_0159.dp', 'wsj_0019.dp', 'wsj_0108.dp', 'wsj_0048.dp', 'wsj_0128.dp', 'wsj_0068.dp', 'wsj_0179.dp', 'wsj_0039.dp', 'wsj_0029.dp', 'wsj_0169.dp', 'wsj_0078.dp', 'wsj_0138.dp', 'wsj_0129.dp', 'wsj_0069.dp', 'wsj_0178.dp', 'wsj_0038.dp', 'wsj_0028.dp', 'wsj_0168.dp', 'wsj_0079.dp', 'wsj_0139.dp', 'wsj_0059.dp', 'wsj_0119.dp', 'wsj_0008.dp', 'wsj_0148.dp', 'wsj_0158.dp', 'wsj_0018.dp', 'wsj_0109.dp', 'wsj_0049.dp', 'wsj_0099.dp', 'wsj_0188.dp', 'wsj_0198.dp', 'wsj_0089.dp', 'wsj_0098.dp', 'wsj_0189.dp', 'wsj_0199.dp', 'wsj_0088.dp', 'wsj_0054.dp', 'wsj_0114.dp', 'wsj_0005.dp', 'wsj_0145.dp', 'wsj_0171.dp', 'wsj_0031.dp', 'wsj_0120.dp', 'wsj_0060.dp', 'wsj_0093.dp', 'wsj_0182.dp', 'wsj_0192.dp', 'wsj_0083.dp', 'wsj_0070.dp', 'wsj_0130.dp', 'wsj_0021.dp', 'wsj_0161.dp', 'wsj_0155.dp', 'wsj_0015.dp', 'wsj_0104.dp', 'wsj_0044.dp', 'wsj_0124.dp', 'wsj_0064.dp', 'wsj_0175.dp', 'wsj_0035.dp', 'wsj_0186.dp', 'wsj_0097.dp', 'wsj_0001.dp', 'wsj_0141.dp', 'wsj_0050.dp', 'wsj_0110.dp', 'wsj_0100.dp', 'wsj_0040.dp', 'wsj_0151.dp', 'wsj_0011.dp', 'wsj_0087.dp', 'wsj_0196.dp', 'wsj_0025.dp', 'wsj_0165.dp', 'wsj_0074.dp', 'wsj_0134.dp', 'wsj_0125.dp', 'wsj_0065.dp', 'wsj_0174.dp', 'wsj_0034.dp', 'wsj_0187.dp', 'wsj_0096.dp', 'wsj_0140.dp', 'wsj_0051.dp', 'wsj_0111.dp', 'wsj_0101.dp', 'wsj_0041.dp', 'wsj_0150.dp', 'wsj_0010.dp', 'wsj_0086.dp', 'wsj_0197.dp', 'wsj_0024.dp', 'wsj_0164.dp', 'wsj_0075.dp', 'wsj_0135.dp', 'wsj_0055.dp', 'wsj_0115.dp', 'wsj_0004.dp', 'wsj_0144.dp', 'wsj_0170.dp', 'wsj_0030.dp', 'wsj_0121.dp', 'wsj_0061.dp', 'wsj_0092.dp', 'wsj_0183.dp', 'wsj_0193.dp', 'wsj_0082.dp', 'wsj_0071.dp', 'wsj_0131.dp', 'wsj_0020.dp', 'wsj_0160.dp', 'wsj_0154.dp', 'wsj_0014.dp', 'wsj_0105.dp', 'wsj_0045.dp']\n"
     ]
    }
   ],
   "source": [
    "original_dataset = os.listdir(original_path)\n",
    "print(original_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wsj_0001.dp', 'wsj_0002.dp', 'wsj_0003.dp', 'wsj_0004.dp', 'wsj_0005.dp', 'wsj_0006.dp', 'wsj_0007.dp', 'wsj_0008.dp', 'wsj_0009.dp', 'wsj_0010.dp', 'wsj_0011.dp', 'wsj_0012.dp', 'wsj_0013.dp', 'wsj_0014.dp', 'wsj_0015.dp', 'wsj_0016.dp', 'wsj_0017.dp', 'wsj_0018.dp', 'wsj_0019.dp', 'wsj_0020.dp', 'wsj_0021.dp', 'wsj_0022.dp', 'wsj_0023.dp', 'wsj_0024.dp', 'wsj_0025.dp', 'wsj_0026.dp', 'wsj_0027.dp', 'wsj_0028.dp', 'wsj_0029.dp', 'wsj_0030.dp', 'wsj_0031.dp', 'wsj_0032.dp', 'wsj_0033.dp', 'wsj_0034.dp', 'wsj_0035.dp', 'wsj_0036.dp', 'wsj_0037.dp', 'wsj_0038.dp', 'wsj_0039.dp', 'wsj_0040.dp', 'wsj_0041.dp', 'wsj_0042.dp', 'wsj_0043.dp', 'wsj_0044.dp', 'wsj_0045.dp', 'wsj_0046.dp', 'wsj_0047.dp', 'wsj_0048.dp', 'wsj_0049.dp', 'wsj_0050.dp', 'wsj_0051.dp', 'wsj_0052.dp', 'wsj_0053.dp', 'wsj_0054.dp', 'wsj_0055.dp', 'wsj_0056.dp', 'wsj_0057.dp', 'wsj_0058.dp', 'wsj_0059.dp', 'wsj_0060.dp', 'wsj_0061.dp', 'wsj_0062.dp', 'wsj_0063.dp', 'wsj_0064.dp', 'wsj_0065.dp', 'wsj_0066.dp', 'wsj_0067.dp', 'wsj_0068.dp', 'wsj_0069.dp', 'wsj_0070.dp', 'wsj_0071.dp', 'wsj_0072.dp', 'wsj_0073.dp', 'wsj_0074.dp', 'wsj_0075.dp', 'wsj_0076.dp', 'wsj_0077.dp', 'wsj_0078.dp', 'wsj_0079.dp', 'wsj_0080.dp', 'wsj_0081.dp', 'wsj_0082.dp', 'wsj_0083.dp', 'wsj_0084.dp', 'wsj_0085.dp', 'wsj_0086.dp', 'wsj_0087.dp', 'wsj_0088.dp', 'wsj_0089.dp', 'wsj_0090.dp', 'wsj_0091.dp', 'wsj_0092.dp', 'wsj_0093.dp', 'wsj_0094.dp', 'wsj_0095.dp', 'wsj_0096.dp', 'wsj_0097.dp', 'wsj_0098.dp', 'wsj_0099.dp', 'wsj_0100.dp']\n",
      "['wsj_0101.dp', 'wsj_0102.dp', 'wsj_0103.dp', 'wsj_0104.dp', 'wsj_0105.dp', 'wsj_0106.dp', 'wsj_0107.dp', 'wsj_0108.dp', 'wsj_0109.dp', 'wsj_0110.dp', 'wsj_0111.dp', 'wsj_0112.dp', 'wsj_0113.dp', 'wsj_0114.dp', 'wsj_0115.dp', 'wsj_0116.dp', 'wsj_0117.dp', 'wsj_0118.dp', 'wsj_0119.dp', 'wsj_0120.dp', 'wsj_0121.dp', 'wsj_0122.dp', 'wsj_0123.dp', 'wsj_0124.dp', 'wsj_0125.dp', 'wsj_0126.dp', 'wsj_0127.dp', 'wsj_0128.dp', 'wsj_0129.dp', 'wsj_0130.dp', 'wsj_0131.dp', 'wsj_0132.dp', 'wsj_0133.dp', 'wsj_0134.dp', 'wsj_0135.dp', 'wsj_0136.dp', 'wsj_0137.dp', 'wsj_0138.dp', 'wsj_0139.dp', 'wsj_0140.dp', 'wsj_0141.dp', 'wsj_0142.dp', 'wsj_0143.dp', 'wsj_0144.dp', 'wsj_0145.dp', 'wsj_0146.dp', 'wsj_0147.dp', 'wsj_0148.dp', 'wsj_0149.dp', 'wsj_0150.dp']\n",
      "['wsj_0151.dp', 'wsj_0152.dp', 'wsj_0153.dp', 'wsj_0154.dp', 'wsj_0155.dp', 'wsj_0156.dp', 'wsj_0157.dp', 'wsj_0158.dp', 'wsj_0159.dp', 'wsj_0160.dp', 'wsj_0161.dp', 'wsj_0162.dp', 'wsj_0163.dp', 'wsj_0164.dp', 'wsj_0165.dp', 'wsj_0166.dp', 'wsj_0167.dp', 'wsj_0168.dp', 'wsj_0169.dp', 'wsj_0170.dp', 'wsj_0171.dp', 'wsj_0172.dp', 'wsj_0173.dp', 'wsj_0174.dp', 'wsj_0175.dp', 'wsj_0176.dp', 'wsj_0177.dp', 'wsj_0178.dp', 'wsj_0179.dp', 'wsj_0180.dp', 'wsj_0181.dp', 'wsj_0182.dp', 'wsj_0183.dp', 'wsj_0184.dp', 'wsj_0185.dp', 'wsj_0186.dp', 'wsj_0187.dp', 'wsj_0188.dp', 'wsj_0189.dp', 'wsj_0190.dp', 'wsj_0191.dp', 'wsj_0192.dp', 'wsj_0193.dp', 'wsj_0194.dp', 'wsj_0195.dp', 'wsj_0196.dp', 'wsj_0197.dp', 'wsj_0198.dp', 'wsj_0199.dp']\n"
     ]
    }
   ],
   "source": [
    "original_dataset.sort()\n",
    "org_train = original_dataset[0:100]\n",
    "org_val = original_dataset[100:150]\n",
    "org_test = original_dataset[150:]\n",
    "\n",
    "print(org_train)\n",
    "print(org_val)\n",
    "print(org_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in org_train:\n",
    "    os.rename(original_path+f, train_path+f)\n",
    "for f in org_val:\n",
    "    os.rename(original_path+f, val_path+f)\n",
    "for f in org_test:\n",
    "    os.rename(original_path+f, test_path+f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(lst): # DO WE NEED THIS?\n",
    "    lowercase_list = [x.lower() for x in lst]\n",
    "    return lowercase_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(dataset_path: str):\n",
    "    dataframe_rows = []\n",
    "    documents = os.listdir(dataset_path)\n",
    "    for document in documents:\n",
    "        path = os.path.join(dataset_path, document)\n",
    "        with open(path, 'r') as f:\n",
    "            a = [[x for x in ln.split()] for ln in f]\n",
    "        a2 = [x for x in a if x != []] # OBS! Removing all empty lines in file so we can make an array\n",
    "        arr = np.array(a2)\n",
    "        text = arr[:, 0]\n",
    "        tagg = arr[:, 1]\n",
    "        POStuple = [(text[x], tagg[x]) for x in range(len(text))]\n",
    "        document_id = int(document[4:8])\n",
    "        dataframe_row = {\n",
    "            \"document_id\": document_id,\n",
    "            \"text\": text,\n",
    "            \"POS\": POStuple\n",
    "        }\n",
    "        dataframe_rows.append(dataframe_row)\n",
    "    # transform the list of rows in a proper dataframe\n",
    "    df = pd.DataFrame(dataframe_rows)\n",
    "    df = df[[\"document_id\", \"text\", \"POS\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = create_dataframe(train_path)\n",
    "df_val = create_dataframe(val_path)\n",
    "df_test = create_dataframe(test_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed the words using GloVe embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    # Check download\n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Word2Vec: 300\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 50\n",
    "\n",
    "embedding_model = load_embedding_model(embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list(data_frame):\n",
    "    big_list = []\n",
    "    text = data_frame[\"text\"]\n",
    "    for row in data_frame[\"text\"]:\n",
    "        big_list += row.tolist()\n",
    "\n",
    "    word_list = set(big_list)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_train = get_word_list(df_train)\n",
    "word_list_val = get_word_list(df_val)\n",
    "word_list_test = get_word_list(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition\n",
    "\n",
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                    word_list):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_listing: dataset specific vocabulary (list)\n",
    "\n",
    "    :return\n",
    "        - list of OOV terms\n",
    "    \"\"\"\n",
    "    embedding_vocabulary = set(embedding_model.index_to_key) # Was previously: set(embedding_model.vocab.keys())\n",
    "    print(len(embedding_vocabulary))\n",
    "    oov = set(word_list).difference(embedding_vocabulary)\n",
    "    return list(oov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "400000\n",
      "400000\n",
      "Total OOV terms: 2346 (0.29%)\n"
     ]
    }
   ],
   "source": [
    "oov1_terms = check_OOV_terms(embedding_model, word_list_train)\n",
    "#--------------------------OBS--------------------------------------------#\n",
    "# To make sure OOV2 does not contain words from OOV1 a check is \n",
    "# implemented during the embedding in the word to index function. \n",
    "# It might be an idea to include this check in the chekc_OOV_terms as well. \n",
    "# Question for the professor?\n",
    "#--------------------------OBS--------------------------------------------#\n",
    "oov2_terms = check_OOV_terms(embedding_model, word_list_val)\n",
    "oov3_terms = check_OOV_terms(embedding_model, word_list_test)\n",
    "\n",
    "\n",
    "print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_terms), float(len(oov_terms)) / len(word_list_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word_to_index (for one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def create_word_to_idx(vocabulary):\n",
    "    word_to_idx = OrderedDict()\n",
    "    \n",
    "    current_idx = 0\n",
    "    for word in vocabulary:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = current_idx\n",
    "            current_idx += 1\n",
    "            \n",
    "    return word_to_idx\n",
    "\n",
    "def update_word_to_idx(old_word_to_idx, new_words):\n",
    "    word_to_idx = old_word_to_idx.copy()\n",
    "    current_idx = len(word_to_idx)\n",
    "    for word in new_words:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = current_idx\n",
    "            current_idx += 1\n",
    "            \n",
    "    return word_to_idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'----------------------OBS------------------------------------\\nShould we also compute word to index for the test set and embed\\nthese words, and then use all of them for training and validation\\nor should we keep the test set seperate and only compute this for\\ntesting?\\n----------------------OBS------------------------------------'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx_v1 = create_word_to_idx(set(embedding_model.index_to_key))\n",
    "word_to_idx_v2 = update_word_to_idx(word_to_idx_v1, oov1_terms)\n",
    "word_to_idx_v3 = update_word_to_idx(word_to_idx_v2, oov2_terms)\n",
    "\n",
    "\"\"\"----------------------OBS------------------------------------\n",
    "Should we also compute word to index for the test set and embed\n",
    "these words, and then use all of them for training and validation\n",
    "or should we keep the test set seperate and only compute this for\n",
    "testing?\n",
    "----------------------OBS------------------------------------\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBS Computes the OOV with random embeddings\n",
    "def create_embedding_matrix(embedding_model, embedding_dimension, word_to_idx):\n",
    "    embedding_matrix = np.zeros((len(word_to_idx), embedding_dimension), dtype=np.float32)\n",
    "    for word, idx in word_to_idx.items():\n",
    "        try:\n",
    "            embedding_vector = embedding_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "                                \n",
    "    return embedding_matrix\n",
    "\n",
    "def expand_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, old_embedding_matrix):\n",
    "    embedding_matrix = np.zeros((len(word_to_idx), embedding_dimension), dtype=np.float32)\n",
    "    embedding_matrix[0:len(old_embedding_matrix)] = old_embedding_matrix\n",
    "    \n",
    "    for word, idx in word_to_idx.items():\n",
    "        if idx >= len(old_embedding_matrix):\n",
    "            try:\n",
    "                embedding_vector = embedding_model[word]\n",
    "            except (KeyError, TypeError):\n",
    "                embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "                \n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 50)\n",
      "(402346, 50)\n",
      "(403290, 50)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_v1 = create_embedding_matrix(embedding_model, embedding_dimension, word_to_idx_v1)\n",
    "embedding_matrix_v2 = expand_embedding_matrix(embedding_model, embedding_dimension, word_to_idx_v2, embedding_matrix_v1)\n",
    "embedding_matrix_v3 = expand_embedding_matrix(embedding_model, embedding_dimension, word_to_idx_v3, embedding_matrix_v2)\n",
    "\n",
    "print(embedding_matrix_v1.shape)\n",
    "print(embedding_matrix_v2.shape)\n",
    "print(embedding_matrix_v3.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test (Remove later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Hello', 0), ('Solveig', 1), ('is', 2), ('very', 3), ('wierd', 4), ('.', 5)])\n",
      "[[ 0.02183098  0.02402802  0.03118286  0.04769417  0.01872175]\n",
      " [-0.01728713  0.02662692  0.02155588 -0.00091611 -0.01262367]\n",
      " [-0.00816416  0.03744153 -0.00933892  0.03142535 -0.04727874]\n",
      " [ 0.01423092  0.00143279 -0.02720169 -0.00628638  0.03455986]\n",
      " [-0.02372587  0.01482276 -0.0354635   0.04404848 -0.01387006]\n",
      " [ 0.03148971  0.03084763 -0.00861655  0.00823898 -0.00565485]]\n",
      "OrderedDict([('Hello', 0), ('Solveig', 1), ('is', 2), ('very', 3), ('wierd', 4), ('.', 5), ('Andrea', 6), ('super', 7), ('cool', 8)])\n",
      "[[ 0.02183098  0.02402802  0.03118286  0.04769417  0.01872175]\n",
      " [-0.01728713  0.02662692  0.02155588 -0.00091611 -0.01262367]\n",
      " [-0.00816416  0.03744153 -0.00933892  0.03142535 -0.04727874]\n",
      " [ 0.01423092  0.00143279 -0.02720169 -0.00628638  0.03455986]\n",
      " [-0.02372587  0.01482276 -0.0354635   0.04404848 -0.01387006]\n",
      " [ 0.03148971  0.03084763 -0.00861655  0.00823898 -0.00565485]\n",
      " [-0.02011299  0.04589678  0.00622369  0.04714626 -0.04880853]\n",
      " [ 0.02720829 -0.00609134 -0.02245523  0.02279024 -0.039202  ]\n",
      " [-0.00789721  0.02382589  0.04431659 -0.04894823 -0.03839992]]\n"
     ]
    }
   ],
   "source": [
    "vocab = ['Hello', 'Solveig', 'is', 'very', 'wierd', '.']\n",
    "\n",
    "w_t_idx = create_word_to_idx(vocab)\n",
    "print(w_t_idx)\n",
    "\n",
    "embed_matrix = create_embedding_matrix(None, 5, w_t_idx)\n",
    "print(embed_matrix)\n",
    "\n",
    "new_words = ['Andrea', 'is', 'super', 'cool', '.']\n",
    "new_w_t_idx = update_word_to_idx(w_t_idx, new_words)\n",
    "print(new_w_t_idx)\n",
    "\n",
    "new_embed_matrix = expand_embedding_matrix(None, 5, new_w_t_idx, embed_matrix)\n",
    "print(new_embed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
