{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from collections import OrderedDict\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './Dataset/train/'# Should we use: os.path.join(os.getcwd(), \"OriginalDataset\")\n",
    "val_path = './Dataset/val/'\n",
    "test_path = './Dataset/test/'\n",
    "dataset_folder = os.path.join(os.getcwd(), \"OriginalDataset\")\n",
    "original_path = dataset_folder +'/dependency_treebank/'\n",
    "\n",
    "embedding_dimension = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request  #  download files\n",
    "import zipfile  #  unzip files\n",
    "\n",
    "def download_dataset():\n",
    "    dataset_folder = os.path.join(os.getcwd(), \"OriginalDataset\")\n",
    "\n",
    "    if not os.path.exists(dataset_folder):\n",
    "        os.makedirs(dataset_folder)\n",
    "\n",
    "    url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
    "\n",
    "    dataset_path = os.path.join(dataset_folder, \"dependency_treebank.zip\")\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        urllib.request.urlretrieve(url, dataset_path)\n",
    "        print(\"Successful download\")\n",
    "\n",
    "    with zipfile.ZipFile(dataset_path,\"r\") as zip_ref:\n",
    "        zip_ref.extractall(dataset_folder)\n",
    "    print(\"Successful extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful extraction\n"
     ]
    }
   ],
   "source": [
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(train_path, val_path, test_path, original_path):\n",
    "    if not os.path.exists(train_path):\n",
    "        print(\"making directory\")\n",
    "        os.makedirs(train_path)\n",
    "    if not os.path.exists(val_path):\n",
    "        os.makedirs(val_path)\n",
    "    if not os.path.exists(test_path):\n",
    "        os.makedirs(test_path)\n",
    "\n",
    "    original_dataset = os.listdir(original_path)\n",
    "    \n",
    "    original_dataset.sort()\n",
    "    org_train = original_dataset[0:100]\n",
    "    org_val = original_dataset[100:150]\n",
    "    org_test = original_dataset[150:]\n",
    "\n",
    "    for f in org_train:\n",
    "        os.rename(original_path+f, train_path+f)\n",
    "    for f in org_val:\n",
    "        os.rename(original_path+f, val_path+f)\n",
    "    for f in org_test:\n",
    "        os.rename(original_path+f, test_path+f)\n",
    "    print(\"Successful spilt\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful spilt\n"
     ]
    }
   ],
   "source": [
    "split_dataset(train_path, val_path, test_path, original_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(dataset_path: str):\n",
    "    dataframe_rows = []\n",
    "    documents = os.listdir(dataset_path)\n",
    "    for document in documents:\n",
    "        path = os.path.join(dataset_path, document)\n",
    "        with open(path, 'r') as f:\n",
    "            a = [[x for x in ln.split()] for ln in f]\n",
    "        #---------------For splitting on sentence-------------------#\n",
    "        sentence = []\n",
    "        sentence_tag = []\n",
    "        for x in a:\n",
    "            if x!=[]:\n",
    "                sentence.append(x[0])\n",
    "                sentence_tag.append(x[1])\n",
    "            else:\n",
    "                dataframe_row = {\n",
    "                    \"text\": sentence,\n",
    "                    \"POS\": sentence_tag\n",
    "                }\n",
    "                sentence = []\n",
    "                sentence_tag = []\n",
    "                dataframe_rows.append(dataframe_row)\n",
    "        #----------------------------------------------------------#\n",
    "        #------------------To split on document--------------------#\n",
    "        \"\"\"\n",
    "        a2 = [x for x in a if x != []] # OBS! Removing all empty lines in file so we can make an array\n",
    "        arr = np.array(a2)\n",
    "        text = list(arr[:, 0])\n",
    "        tagg = list(arr[:, 1])\n",
    "        #POStuple =  [tagg[x] for x in range(len(text))] #[(text[x], tagg[x]) for x in range(len(text))]\n",
    "        document_id = int(document[4:8])\n",
    "        dataframe_row = {\n",
    "            \"document_id\": document_id,\n",
    "            \"text\": text,\n",
    "            \"POS\": tagg\n",
    "        }\n",
    "        dataframe_rows.append(dataframe_row)\"\"\"\n",
    "        #----------------------------------------------------------#\n",
    "    # transform the list of rows in a proper dataframe\n",
    "    df = pd.DataFrame(dataframe_rows)\n",
    "    df = df[[\"text\", \"POS\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[In, reference, to, your, Oct., 9, page-one, a...</td>\n",
       "      <td>[IN, NN, TO, PRP$, NNP, CD, NN, NN, ``, NNP, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[In, part, ,, this, may, reflect, the, fact, t...</td>\n",
       "      <td>[IN, NN, ,, DT, MD, VB, DT, NN, IN, ``, PRP, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Among, professionals, ,, 76, %, have, a, favo...</td>\n",
       "      <td>[IN, NNS, ,, CD, NN, VBP, DT, JJ, NN, IN, PRP$...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[While, a, quarter, of, black, voters, disappr...</td>\n",
       "      <td>[IN, DT, NN, IN, JJ, NNS, VBP, IN, NNP, NNP, P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, statistics, imply, that, three-quarters,...</td>\n",
       "      <td>[DT, NNS, VBP, IN, NNS, IN, NNS, VBP, IN, NNP,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>[He, said, authors, of, Scoring, High, ``, scr...</td>\n",
       "      <td>[PRP, VBD, NNS, IN, NNP, NNP, ``, RB, VBP, '',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>[When, Scoring, High, first, came, out, in, 19...</td>\n",
       "      <td>[WRB, NNP, NNP, RB, VBD, RB, IN, CD, ,, PRP, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>[McGraw-Hill, was, outraged, .]</td>\n",
       "      <td>[NNP, VBD, JJ, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>[In, a, 1985, advisory, to, educators, ,, McGr...</td>\n",
       "      <td>[IN, DT, CD, NN, TO, NNS, ,, NNP, VBD, NNP, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>[But, in, 1988, ,, McGraw-Hill, purchased, the...</td>\n",
       "      <td>[CC, IN, CD, ,, NNP, VBD, DT, NNP, NNP, NN, WD...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1863 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     [In, reference, to, your, Oct., 9, page-one, a...   \n",
       "1     [In, part, ,, this, may, reflect, the, fact, t...   \n",
       "2     [Among, professionals, ,, 76, %, have, a, favo...   \n",
       "3     [While, a, quarter, of, black, voters, disappr...   \n",
       "4     [The, statistics, imply, that, three-quarters,...   \n",
       "...                                                 ...   \n",
       "1858  [He, said, authors, of, Scoring, High, ``, scr...   \n",
       "1859  [When, Scoring, High, first, came, out, in, 19...   \n",
       "1860                    [McGraw-Hill, was, outraged, .]   \n",
       "1861  [In, a, 1985, advisory, to, educators, ,, McGr...   \n",
       "1862  [But, in, 1988, ,, McGraw-Hill, purchased, the...   \n",
       "\n",
       "                                                    POS  \n",
       "0     [IN, NN, TO, PRP$, NNP, CD, NN, NN, ``, NNP, N...  \n",
       "1     [IN, NN, ,, DT, MD, VB, DT, NN, IN, ``, PRP, V...  \n",
       "2     [IN, NNS, ,, CD, NN, VBP, DT, JJ, NN, IN, PRP$...  \n",
       "3     [IN, DT, NN, IN, JJ, NNS, VBP, IN, NNP, NNP, P...  \n",
       "4     [DT, NNS, VBP, IN, NNS, IN, NNS, VBP, IN, NNP,...  \n",
       "...                                                 ...  \n",
       "1858  [PRP, VBD, NNS, IN, NNP, NNP, ``, RB, VBP, '',...  \n",
       "1859  [WRB, NNP, NNP, RB, VBD, RB, IN, CD, ,, PRP, V...  \n",
       "1860                                  [NNP, VBD, JJ, .]  \n",
       "1861  [IN, DT, CD, NN, TO, NNS, ,, NNP, VBD, NNP, NN...  \n",
       "1862  [CC, IN, CD, ,, NNP, VBD, DT, NNP, NNP, NN, WD...  \n",
       "\n",
       "[1863 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = create_dataframe(train_path)\n",
    "df_val = create_dataframe(val_path)\n",
    "df_test = create_dataframe(test_path)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed the words using GloVe embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    # Check download\n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Word2Vec: 300\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = load_embedding_model(embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make tokenizer REMOVE THIS PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# OBS: The tokenizer removes every word with a special character. Etc: 100,000, $, u.s.\\n# The tokenizer also handles OOV terms and when adding more words, it will just add the OOV words to the word to index dictionary. \\nword_tokenizer_train2 = Tokenizer()\\n\\n# UNCOMMENT IF WE WANT TO INCLUDE GOLVE vocabualry\\nword_tokenizer_train2.fit_on_texts(embedding_model.index_to_key) \\nword_tokenizer_train2.fit_on_texts(df_train[\"text\"])\\n\\nx_encoded_train = word_tokenizer_train2.texts_to_sequences(df_train[\"text\"])'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# OBS: The tokenizer removes every word with a special character. Etc: 100,000, $, u.s.\n",
    "# The tokenizer also handles OOV terms and when adding more words, it will just add the OOV words to the word to index dictionary. \n",
    "word_tokenizer_train2 = Tokenizer()\n",
    "\n",
    "# UNCOMMENT IF WE WANT TO INCLUDE GOLVE vocabualry\n",
    "word_tokenizer_train2.fit_on_texts(embedding_model.index_to_key) \n",
    "word_tokenizer_train2.fit_on_texts(df_train[\"text\"])\n",
    "\n",
    "x_encoded_train = word_tokenizer_train2.texts_to_sequences(df_train[\"text\"])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tag_tokenizer_train2 = Tokenizer()\\n\\ntag_tokenizer_train2.fit_on_texts(df_train[\"POS\"])\\ny_encoded_train = tag_tokenizer_train2.texts_to_sequences(df_train[\"POS\"])'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"tag_tokenizer_train2 = Tokenizer()\n",
    "\n",
    "tag_tokenizer_train2.fit_on_texts(df_train[\"POS\"])\n",
    "y_encoded_train = tag_tokenizer_train2.texts_to_sequences(df_train[\"POS\"])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEW WAY TO Make tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokenizer(*vocabulary):\n",
    "    # OBS: The tokenizer removes every word with a special character. Etc: 100,000, $, u.s.\n",
    "    # The tokenizer also handles OOV terms and when adding more words, it will just add the OOV words to the word to index dictionary. \n",
    "    tokenizer = Tokenizer()\n",
    "    \n",
    "    # UNCOMMENT IF WE WANT TO INCLUDE GOLVE vocabualry\n",
    "    for v in vocabulary:\n",
    "        tokenizer.fit_on_texts(v)\n",
    "    return tokenizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = make_tokenizer(df_train[\"text\"], df_val[\"text\"], df_test[\"text\"])\n",
    "tag_tokenizer = make_tokenizer(df_train[\"POS\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_encoded_train = word_tokenizer.texts_to_sequences(df_train[\"text\"])\n",
    "y_encoded_train = tag_tokenizer.texts_to_sequences(df_train[\"POS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_encoded_val = word_tokenizer.texts_to_sequences(df_val[\"text\"])\n",
    "y_encoded_val = tag_tokenizer.texts_to_sequences(df_val[\"POS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_encoded_test = word_tokenizer.texts_to_sequences(df_test[\"text\"])\n",
    "y_encoded_test = tag_tokenizer.texts_to_sequences(df_test[\"POS\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad encoded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_length(seqs):\n",
    "    max_len = 0\n",
    "    for seq in seqs:\n",
    "        if len(seq) > max_len:\n",
    "            max_len = len(seq)\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_padded_train = pad_idx_seqs(x_encoded_train, find_max_length(x_encoded_train))\n",
    "y_padded_train = pad_idx_seqs(y_encoded_train, find_max_length(y_encoded_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_padded_val = pad_idx_seqs(x_encoded_val, find_max_length(x_encoded_val))\n",
    "y_padded_val = pad_idx_seqs(y_encoded_val, find_max_length(y_encoded_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_padded_test = pad_idx_seqs(x_encoded_test, find_max_length(x_encoded_test))\n",
    "y_padded_test = pad_idx_seqs(y_encoded_test, find_max_length(y_encoded_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW METHODE TO GET WORD TO INDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "word2idx = word_tokenizer.word_index\n",
    "idx2word = word_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx = tag_tokenizer.word_index\n",
    "idx2tag = tag_tokenizer.index_word\n",
    "len(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_VOCABULARY_SIZE = len(tag2idx) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBS Computes the OOV with random embeddings\n",
    "def create_embedding_matrix(embedding_model, embedding_dimension, word_to_idx):\n",
    "    embedding_matrix = np.zeros((len(word_to_idx)+1, embedding_dimension), dtype=np.float32)\n",
    "    for word, idx in word_to_idx.items():\n",
    "        try:\n",
    "            embedding_vector = embedding_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "                                \n",
    "    return embedding_matrix\n",
    "\n",
    "def expand_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, old_embedding_matrix):\n",
    "    embedding_matrix = np.zeros((len(word_to_idx), embedding_dimension), dtype=np.float32)\n",
    "    embedding_matrix[0:len(old_embedding_matrix)] = old_embedding_matrix\n",
    "    \n",
    "    for word, idx in word_to_idx.items():\n",
    "        if idx >= len(old_embedding_matrix):\n",
    "            try:\n",
    "                embedding_vector = embedding_model[word]\n",
    "            except (KeyError, TypeError):\n",
    "                embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "                \n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = create_embedding_matrix(embedding_model, embedding_dimension, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10718, 50)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define x train and y train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_padded_train\n",
    "y_train = to_categorical(y_padded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_padded_val\n",
    "y_val = to_categorical(y_padded_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_padded_test\n",
    "y_test = to_categorical(y_padded_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create baseline model\n",
    "\n",
    "##### two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://www.kaggle.com/tanyadayanand/pos-tagging-using-rnn/notebook\n",
    "\n",
    "Not sure about the parameters etc.. but at least it works to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_model(units_LSTM, activation):\n",
    "    bidirect_model = keras.Sequential()\n",
    "    bidirect_model.add(layers.Embedding(input_dim = VOCABULARY_SIZE,\n",
    "                                 output_dim    = embedding_dimension,\n",
    "                                 input_length  = find_max_length(x_train),\n",
    "                                 weights       = [embedding_matrix],\n",
    "                                 trainable = True\n",
    "                                ))\n",
    "    bidirect_model.add(layers.Bidirectional(layers.LSTM(units_LSTM, return_sequences=True)))\n",
    "    #bidirect_model.add(layers.TimeDistributed(layers.Dense(len(tag_to_idx), activation='softmax')))\n",
    "    bidirect_model.add(layers.Dense(TAG_VOCABULARY_SIZE, activation=activation))\n",
    "    \n",
    "    return bidirect_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 249, 50)           535900    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 249, 128)         58880     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 249, 46)           5934      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 600,714\n",
      "Trainable params: 600,714\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline_model = create_baseline_model(64, 'softmax')\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 13s 132ms/step - loss: 0.0084 - mae: 0.0177 - acc: 0.9001\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 10s 132ms/step - loss: 0.0020 - mae: 0.0045 - acc: 0.9180\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 11s 141ms/step - loss: 0.0019 - mae: 0.0042 - acc: 0.9303\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 15s 194ms/step - loss: 0.0018 - mae: 0.0039 - acc: 0.9351\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 12s 157ms/step - loss: 0.0015 - mae: 0.0034 - acc: 0.9463\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 12s 163ms/step - loss: 0.0013 - mae: 0.0030 - acc: 0.9517\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 11s 141ms/step - loss: 0.0012 - mae: 0.0027 - acc: 0.9561\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 12s 154ms/step - loss: 0.0011 - mae: 0.0025 - acc: 0.9631\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 11s 140ms/step - loss: 9.4054e-04 - mae: 0.0022 - acc: 0.9696\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 9s 124ms/step - loss: 7.9567e-04 - mae: 0.0019 - acc: 0.9749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16d31db80>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.compile(loss=\"mse\",\n",
    "              optimizer=\"Adam\",\n",
    "              metrics=['mae', \"acc\"])\n",
    "baseline_training = baseline_model.fit(x=x_train, batch_size=25, y=y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gru_model(units_gru, activation):\n",
    "    gru_model = keras.Sequential()\n",
    "    gru_model.add(layers.Embedding(input_dim = VOCABULARY_SIZE,\n",
    "                                 output_dim    = embedding_dimension,\n",
    "                                 input_length  = find_max_length(x_train),\n",
    "                                 weights       = [embedding_matrix],\n",
    "                                 trainable = True\n",
    "                                ))\n",
    "    gru_model.add(layers.Bidirectional(layers.GRU(units_gru, return_sequences=True)))\n",
    "    #bidirect_model.add(layers.TimeDistributed(layers.Dense(len(tag_to_idx), activation='softmax')))\n",
    "    gru_model.add(layers.Dense(TAG_VOCABULARY_SIZE, activation=activation))\n",
    "    \n",
    "    return gru_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 249, 50)           535900    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 249, 128)         44544     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 249, 46)           5934      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 586,378\n",
      "Trainable params: 586,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_model = create_gru_model(64, 'softmax')\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 13s 135ms/step - loss: 7.0415e-04 - mae: 0.0016 - acc: 0.9771\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 11s 142ms/step - loss: 6.4973e-04 - mae: 0.0015 - acc: 0.9783\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 14s 184ms/step - loss: 6.0532e-04 - mae: 0.0014 - acc: 0.9798\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 13s 176ms/step - loss: 5.5222e-04 - mae: 0.0013 - acc: 0.9820\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 13s 172ms/step - loss: 4.9376e-04 - mae: 0.0012 - acc: 0.9842\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 12s 156ms/step - loss: 4.2883e-04 - mae: 0.0010 - acc: 0.9869\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 13s 168ms/step - loss: 3.7977e-04 - mae: 9.2752e-04 - acc: 0.9884\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 10s 136ms/step - loss: 3.4065e-04 - mae: 8.4445e-04 - acc: 0.9896\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 14s 183ms/step - loss: 3.1233e-04 - mae: 7.7013e-04 - acc: 0.9906\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 10s 132ms/step - loss: 2.8997e-04 - mae: 7.1303e-04 - acc: 0.9913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16da45e20>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_model.compile(loss=\"mse\",\n",
    "              optimizer=\"Adam\",\n",
    "              metrics=['mae', \"acc\"])\n",
    "gru_training = gru_model.fit(x=x_train, batch_size=25, y=y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_doubleLSTM_model(units_LSTM1, units_LSTM2, activation):\n",
    "    doubleLSTM_model = keras.Sequential()\n",
    "    doubleLSTM_model.add(layers.Embedding(input_dim = VOCABULARY_SIZE,\n",
    "                                 output_dim    = embedding_dimension,\n",
    "                                 input_length  = find_max_length(x_train),\n",
    "                                 weights       = [embedding_matrix],\n",
    "                                 trainable = True\n",
    "                                ))\n",
    "    doubleLSTM_model.add(layers.Bidirectional(layers.LSTM(units_LSTM1, return_sequences=True)))\n",
    "    doubleLSTM_model.add(layers.Bidirectional(layers.LSTM(units_LSTM2, return_sequences=True)))\n",
    "    #bidirect_model.add(layers.TimeDistributed(layers.Dense(len(tag_to_idx), activation='softmax')))\n",
    "    doubleLSTM_model.add(layers.Dense(TAG_VOCABULARY_SIZE, activation=activation))\n",
    "    \n",
    "    return doubleLSTM_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 249, 50)           535900    \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 249, 128)         58880     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 249, 128)         98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 249, 46)           5934      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 699,530\n",
      "Trainable params: 699,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "doubleLSTM_model = create_doubleLSTM_model(64, 64, 'softmax')\n",
    "doubleLSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 32s 324ms/step - loss: 0.0067 - mae: 0.0139 - acc: 0.8983\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 23s 306ms/step - loss: 0.0020 - mae: 0.0043 - acc: 0.9147\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 21s 274ms/step - loss: 0.0020 - mae: 0.0041 - acc: 0.9190\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 24s 328ms/step - loss: 0.0019 - mae: 0.0040 - acc: 0.9232\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 23s 313ms/step - loss: 0.0018 - mae: 0.0038 - acc: 0.9290\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 26s 349ms/step - loss: 0.0017 - mae: 0.0035 - acc: 0.9396\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 24s 326ms/step - loss: 0.0014 - mae: 0.0030 - acc: 0.9500\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 26s 353ms/step - loss: 0.0012 - mae: 0.0026 - acc: 0.9549\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 25s 336ms/step - loss: 0.0011 - mae: 0.0023 - acc: 0.9622\n",
      "Epoch 10/10\n",
      "17/75 [=====>........................] - ETA: 21s - loss: 9.9304e-04 - mae: 0.0022 - acc: 0.9670"
     ]
    }
   ],
   "source": [
    "doubleLSTM_model.compile(loss=\"mse\",\n",
    "              optimizer=\"Adam\",\n",
    "              metrics=['mae', \"acc\"])\n",
    "doubleLSTM_training = doubleLSTM_model.fit(x=x_train, batch_size=25, y=y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_doubledense_model(units_LSTM, units_dense, activation):\n",
    "    doubledense_model = keras.Sequential()\n",
    "    doubledense_model.add(layers.Embedding(input_dim = VOCABULARY_SIZE,\n",
    "                                 output_dim    = embedding_dimension,\n",
    "                                 input_length  = find_max_length(x_train),\n",
    "                                 weights       = [embedding_matrix],\n",
    "                                 trainable = True\n",
    "                                ))\n",
    "    doubledense_model.add(layers.Bidirectional(layers.LSTM(units_LSTM, return_sequences=True)))\n",
    "    doubledense_model.add(layers.Dense(units_dense))\n",
    "    #bidirect_model.add(layers.TimeDistributed(layers.Dense(len(tag_to_idx), activation='softmax')))\n",
    "    doubledense_model.add(layers.Dense(TAG_VOCABULARY_SIZE, activation=activation))\n",
    "    \n",
    "    return doubledense_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 249, 50)           17005400  \n",
      "                                                                 \n",
      " bidirectional_9 (Bidirectio  (None, 249, 128)         58880     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 249, 64)           8256      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 249, 46)           2990      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,075,526\n",
      "Trainable params: 17,075,526\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "doubleDense_model = create_doubledense_model(64, 64, 'softmax')\n",
    "doubleDense_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 13s 126ms/step - loss: 0.0082 - mae: 0.0174 - acc: 0.8995\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 10s 130ms/step - loss: 0.0020 - mae: 0.0044 - acc: 0.9191\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 10s 128ms/step - loss: 0.0019 - mae: 0.0041 - acc: 0.9306\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 12s 162ms/step - loss: 0.0016 - mae: 0.0036 - acc: 0.9438\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 10s 127ms/step - loss: 0.0014 - mae: 0.0031 - acc: 0.9501\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 10s 125ms/step - loss: 0.0012 - mae: 0.0028 - acc: 0.9557\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 11s 144ms/step - loss: 0.0011 - mae: 0.0025 - acc: 0.9617\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 13s 175ms/step - loss: 9.5712e-04 - mae: 0.0022 - acc: 0.9680\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 10s 129ms/step - loss: 8.2807e-04 - mae: 0.0020 - acc: 0.9731\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 12s 155ms/step - loss: 7.3608e-04 - mae: 0.0017 - acc: 0.9759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1784575e0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubleDense_model.compile(loss=\"mse\",\n",
    "              optimizer=\"Adam\",\n",
    "              metrics=['mae', \"acc\"])\n",
    "doubleDense_training = doubleDense_model.fit(x=x_train, batch_size=25, y=y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
