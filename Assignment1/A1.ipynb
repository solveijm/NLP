{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './Dataset/train/'# Should we use: os.path.join(os.getcwd(), \"OriginalDataset\")\n",
    "val_path = './Dataset/val/'\n",
    "test_path = './Dataset/test/'\n",
    "dataset_folder = os.path.join(os.getcwd(), \"OriginalDataset\")\n",
    "original_path = dataset_folder +'/dependency_treebank/'\n",
    "\n",
    "embedding_dimension = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request  #  download files\n",
    "import zipfile  #  unzip files\n",
    "\n",
    "def download_dataset():\n",
    "    dataset_folder = os.path.join(os.getcwd(), \"OriginalDataset\")\n",
    "\n",
    "    if not os.path.exists(dataset_folder):\n",
    "        os.makedirs(dataset_folder)\n",
    "\n",
    "    url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
    "\n",
    "    dataset_path = os.path.join(dataset_folder, \"dependency_treebank.zip\")\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        urllib.request.urlretrieve(url, dataset_path)\n",
    "        print(\"Successful download\")\n",
    "\n",
    "    with zipfile.ZipFile(dataset_path,\"r\") as zip_ref:\n",
    "        zip_ref.extractall(dataset_folder)\n",
    "    print(\"Successful extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(train_path, val_path, test_path, original_path):\n",
    "    if not os.path.exists(train_path):\n",
    "        print(\"making directory\")\n",
    "        os.makedirs(train_path)\n",
    "    if not os.path.exists(val_path):\n",
    "        os.makedirs(val_path)\n",
    "    if not os.path.exists(test_path):\n",
    "        os.makedirs(test_path)\n",
    "\n",
    "    original_dataset = os.listdir(original_path)\n",
    "    \n",
    "    original_dataset.sort()\n",
    "    org_train = original_dataset[0:100]\n",
    "    org_val = original_dataset[100:150]\n",
    "    org_test = original_dataset[150:]\n",
    "\n",
    "    for f in org_train:\n",
    "        os.rename(original_path+f, train_path+f)\n",
    "    for f in org_val:\n",
    "        os.rename(original_path+f, val_path+f)\n",
    "    for f in org_test:\n",
    "        os.rename(original_path+f, test_path+f)\n",
    "    print(\"Successful spilt\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(lst): # DO WE NEED THIS?\n",
    "    lowercase_list = [x.lower() for x in lst]\n",
    "    return lowercase_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(dataset_path: str):\n",
    "    dataframe_rows = []\n",
    "    documents = os.listdir(dataset_path)\n",
    "    for document in documents:\n",
    "        path = os.path.join(dataset_path, document)\n",
    "        with open(path, 'r') as f:\n",
    "            a = [[x for x in ln.split()] for ln in f]\n",
    "        a2 = [x for x in a if x != []] # OBS! Removing all empty lines in file so we can make an array\n",
    "        arr = np.array(a2)\n",
    "        text = arr[:, 0]\n",
    "        tagg = arr[:, 1]\n",
    "        #POStuple =  [tagg[x] for x in range(len(text))] #[(text[x], tagg[x]) for x in range(len(text))]\n",
    "        document_id = int(document[4:8])\n",
    "        dataframe_row = {\n",
    "            \"document_id\": document_id,\n",
    "            \"text\": text,\n",
    "            \"POS\": tagg\n",
    "        }\n",
    "        dataframe_rows.append(dataframe_row)\n",
    "    # transform the list of rows in a proper dataframe\n",
    "    df = pd.DataFrame(dataframe_rows)\n",
    "    df = df[[\"document_id\", \"text\", \"POS\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed the words using GloVe embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    # Check download\n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Word2Vec: 300\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list(data_frame, lable):\n",
    "    big_list = []\n",
    "    text = data_frame[lable]\n",
    "    for row in data_frame[lable]:\n",
    "        big_list += row.tolist()\n",
    "\n",
    "    word_list = set(big_list)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition\n",
    "\n",
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                    word_list):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_listing: dataset specific vocabulary (list)\n",
    "\n",
    "    :return\n",
    "        - list of OOV terms\n",
    "    \"\"\"\n",
    "    embedding_vocabulary = set(embedding_model.index_to_key) # Was previously: set(embedding_model.vocab.keys())\n",
    "    oov = set(word_list).difference(embedding_vocabulary)\n",
    "    return list(oov)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word_to_index (for one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def create_word_to_idx(vocabulary):\n",
    "    word_to_idx = OrderedDict()\n",
    "    idx_to_word = OrderedDict()\n",
    "    word_to_idx[\"PADDING\"] = 0\n",
    "    idx_to_word[0] = \"PADDING\"\n",
    "    # Start from one. Index 0 is reserved for padding\n",
    "    current_idx = 1\n",
    "    for word in vocabulary:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = current_idx\n",
    "            idx_to_word[current_idx] = word\n",
    "            current_idx += 1\n",
    "            \n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "def update_word_to_idx(old_idx_to_word, old_word_to_idx, new_words):\n",
    "    word_to_idx = old_word_to_idx.copy()\n",
    "    idx_to_word = old_idx_to_word.copy()\n",
    "    current_idx = len(word_to_idx)\n",
    "    for word in new_words:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = current_idx\n",
    "            idx_to_word[current_idx] = word\n",
    "            current_idx += 1\n",
    "            \n",
    "    return word_to_idx, idx_to_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_idx_word_list(idx_to_word,word_to_idx, write_path):\n",
    "    if not os.path.exists(os.path.dirname(write_path)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(write_path))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    with open(write_path, 'wb') as f:\n",
    "        pickle.dump(obj={\"word_to_idx\": words_lexicon, \"idx_to_word\":idx_to_word} , file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBS Computes the OOV with random embeddings\n",
    "def create_embedding_matrix(embedding_model, embedding_dimension, word_to_idx):\n",
    "    embedding_matrix = np.zeros((len(word_to_idx), embedding_dimension), dtype=np.float32)\n",
    "    for word, idx in word_to_idx.items():\n",
    "        try:\n",
    "            embedding_vector = embedding_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "                                \n",
    "    return embedding_matrix\n",
    "\n",
    "def expand_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, old_embedding_matrix):\n",
    "    embedding_matrix = np.zeros((len(word_to_idx), embedding_dimension), dtype=np.float32)\n",
    "    embedding_matrix[0:len(old_embedding_matrix)] = old_embedding_matrix\n",
    "    \n",
    "    for word, idx in word_to_idx.items():\n",
    "        if idx >= len(old_embedding_matrix):\n",
    "            try:\n",
    "                embedding_vector = embedding_model[word]\n",
    "            except (KeyError, TypeError):\n",
    "                embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "                \n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform documents to sequences of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OBS\\nTakes only one list of words to integer, not the list with lists.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sequence_to_integers(sequence, word_to_idx):\n",
    "    integers = []\n",
    "    for element in sequence:\n",
    "        integers.append(word_to_idx[element])\n",
    "    return np.array(integers)\n",
    "\"\"\"OBS\n",
    "Takes only one list of words to integer, not the list with lists.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_length(seqs):\n",
    "    max_len = 0\n",
    "    for seq in seqs:\n",
    "        if len(seq) > max_len:\n",
    "            max_len = len(seq)\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layers_info, compile_info) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Create a Keras model given a list of layer information\n",
    "\n",
    "    :param layers_info: a list of dictionaries, one for each layer\n",
    "    :param compile_info: dictionary containing compile information\n",
    "\n",
    "    :return\n",
    "        model: the built keras sequential model\n",
    "    \"\"\"\n",
    "\n",
    "    print('Found {} total layers'.format(len(layers_info)))\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    for info_idx, info in enumerate(layers_info):\n",
    "\n",
    "        layer = info['layer'](**{key: value for key, value in info.items() if key != 'layer'})\n",
    "        if info[\"name\"] == \"Bidirectional_LSTM_layer\":\n",
    "            model.add(layers.Bidirectional(layers.LSTM(64)))\n",
    "        else:\n",
    "            model.add(layer)\n",
    "\n",
    "    # Debug\n",
    "    model.summary()\n",
    "\n",
    "    # Compile\n",
    "    model.compile(**compile_info)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create baseline model\n",
    "\n",
    "##### two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://www.kaggle.com/tanyadayanand/pos-tagging-using-rnn/notebook\n",
    "\n",
    "Not sure about the parameters etc.. but at least it works to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_model():\n",
    "    bidirect_model = keras.Sequential()\n",
    "    bidirect_model.add(layers.Embedding(input_dim = len(word_to_idx_v3),\n",
    "                                 output_dim    = embedding_dimension,\n",
    "                                 input_length  = find_max_length(x_train),\n",
    "                                 weights       = [embedding_matrix_v3],\n",
    "                                 mask_zero     = True\n",
    "                                ))\n",
    "    bidirect_model.add(layers.Bidirectional(layers.LSTM(64, return_sequences=True)))\n",
    "    #bidirect_model.add(layers.TimeDistributed(layers.Dense(len(tag_to_idx), activation='softmax')))\n",
    "    bidirect_model.add(layers.Dense(len(tag_to_idx), activation='softmax'))\n",
    "    \n",
    "    return bidirect_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful extraction\n",
      "Successful spilt\n"
     ]
    }
   ],
   "source": [
    "download_dataset()\n",
    "split_dataset(train_path, val_path, test_path, original_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>text</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95</td>\n",
       "      <td>[In, reference, to, your, Oct., 9, page-one, a...</td>\n",
       "      <td>[IN, NN, TO, PRP$, NNP, CD, NN, NN, ``, NNP, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>[Judging, from, the, Americana, in, Haruki, Mu...</td>\n",
       "      <td>[VBG, IN, DT, NNS, IN, NNP, NNP, POS, ``, DT, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>[Sir, Peter, Walters, ,, 58-year-old, chairman...</td>\n",
       "      <td>[NNP, NNP, NNP, ,, JJ, NN, IN, NNP, NNP, NNP, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>[PAPERS, :, Backe, Group, Inc., agreed, to, ac...</td>\n",
       "      <td>[NNS, :, NNP, NNP, NNP, VBD, TO, VB, NNP, NNP,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
       "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>82</td>\n",
       "      <td>[Criticism, in, the, U.S., over, recent, Japan...</td>\n",
       "      <td>[NNP, IN, DT, NNP, IN, JJ, JJ, NNS, VBZ, VBG, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>71</td>\n",
       "      <td>[When, Warren, Winiarski, ,, proprietor, of, S...</td>\n",
       "      <td>[WRB, NNP, NNP, ,, NN, IN, NNP, POS, NNP, NNP,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>20</td>\n",
       "      <td>[The, U.S., ,, claiming, some, success, in, it...</td>\n",
       "      <td>[DT, NNP, ,, VBG, DT, NN, IN, PRP$, NN, NN, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>14</td>\n",
       "      <td>[Norman, Ricken, ,, 52, years, old, and, forme...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, CC, NN,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>45</td>\n",
       "      <td>[Since, chalk, first, touched, slate, ,, schoo...</td>\n",
       "      <td>[IN, NN, RB, VBD, NN, ,, NN, VBP, VBN, TO, VB,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    document_id                                               text  \\\n",
       "0            95  [In, reference, to, your, Oct., 9, page-one, a...   \n",
       "1            37  [Judging, from, the, Americana, in, Haruki, Mu...   \n",
       "2            66  [Sir, Peter, Walters, ,, 58-year-old, chairman...   \n",
       "3            52  [PAPERS, :, Backe, Group, Inc., agreed, to, ac...   \n",
       "4             3  [A, form, of, asbestos, once, used, to, make, ...   \n",
       "..          ...                                                ...   \n",
       "95           82  [Criticism, in, the, U.S., over, recent, Japan...   \n",
       "96           71  [When, Warren, Winiarski, ,, proprietor, of, S...   \n",
       "97           20  [The, U.S., ,, claiming, some, success, in, it...   \n",
       "98           14  [Norman, Ricken, ,, 52, years, old, and, forme...   \n",
       "99           45  [Since, chalk, first, touched, slate, ,, schoo...   \n",
       "\n",
       "                                                  POS  \n",
       "0   [IN, NN, TO, PRP$, NNP, CD, NN, NN, ``, NNP, N...  \n",
       "1   [VBG, IN, DT, NNS, IN, NNP, NNP, POS, ``, DT, ...  \n",
       "2   [NNP, NNP, NNP, ,, JJ, NN, IN, NNP, NNP, NNP, ...  \n",
       "3   [NNS, :, NNP, NNP, NNP, VBD, TO, VB, NNP, NNP,...  \n",
       "4   [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...  \n",
       "..                                                ...  \n",
       "95  [NNP, IN, DT, NNP, IN, JJ, JJ, NNS, VBZ, VBG, ...  \n",
       "96  [WRB, NNP, NNP, ,, NN, IN, NNP, POS, NNP, NNP,...  \n",
       "97  [DT, NNP, ,, VBG, DT, NN, IN, PRP$, NN, NN, ,,...  \n",
       "98  [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, CC, NN,...  \n",
       "99  [IN, NN, RB, VBD, NN, ,, NN, VBP, VBN, TO, VB,...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = create_dataframe(train_path)\n",
    "df_val = create_dataframe(val_path)\n",
    "df_test = create_dataframe(test_path)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = load_embedding_model(embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_train = get_word_list(df_train, \"text\")\n",
    "word_list_val = get_word_list(df_val, \"text\")\n",
    "word_list_test = get_word_list(df_test, \"text\")\n",
    "\n",
    "\"\"\"---------------------OBS------------------------\n",
    "Where shoukld we get tags? Is it ok to assume they are in df_train?\n",
    "\"\"\"\n",
    "tags_list = get_word_list(df_train, \"POS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV1 terms: 2346 (0.29%)\n",
      "Total OOV2 terms: 1524 (0.19%)\n",
      "Total OOV3 terms: 957 (0.12%)\n"
     ]
    }
   ],
   "source": [
    "oov1_terms = check_OOV_terms(embedding_model, word_list_train)\n",
    "#--------------------------OBS--------------------------------------------#\n",
    "# To make sure OOV2 does not contain words from OOV1 a check is \n",
    "# implemented during the embedding in the word to index function. \n",
    "# It might be an idea to include this check in the chekc_OOV_terms as well. \n",
    "# Question for the professor?\n",
    "#--------------------------OBS--------------------------------------------#\n",
    "oov2_terms = check_OOV_terms(embedding_model, word_list_val)\n",
    "oov3_terms = check_OOV_terms(embedding_model, word_list_test)\n",
    "\n",
    "\n",
    "print(\"Total OOV1 terms: {0} ({1:.2f}%)\".format(len(oov1_terms), float(len(oov1_terms)) / len(word_list_train)))\n",
    "print(\"Total OOV2 terms: {0} ({1:.2f}%)\".format(len(oov2_terms), float(len(oov2_terms)) / len(word_list_train)))\n",
    "print(\"Total OOV3 terms: {0} ({1:.2f}%)\".format(len(oov3_terms), float(len(oov3_terms)) / len(word_list_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"----------------------OBS------------------------------------\n",
    "Should we also compute word to index for the test set and embed\n",
    "these words, and then use all of them for training and validation\n",
    "or should we keep the test set seperate and only compute this for\n",
    "testing?\n",
    "----------------------OBS------------------------------------\"\"\"\n",
    "word_to_idx_v1, idx_to_word_v1 = create_word_to_idx(set(embedding_model.index_to_key))\n",
    "word_to_idx_v2, idx_to_word_v2 = update_word_to_idx(idx_to_word_v1, word_to_idx_v1, oov1_terms)\n",
    "word_to_idx_v3, idx_to_word_v3 = update_word_to_idx(idx_to_word_v2, word_to_idx_v2, oov2_terms)\n",
    "\n",
    "tag_to_idx, idx_to_tag = create_word_to_idx(tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400001, 50)\n",
      "(402347, 50)\n",
      "(403291, 50)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_v1 = create_embedding_matrix(embedding_model, embedding_dimension, word_to_idx_v1)\n",
    "embedding_matrix_v2 = expand_embedding_matrix(embedding_model, embedding_dimension, word_to_idx_v2, embedding_matrix_v1)\n",
    "embedding_matrix_v3 = expand_embedding_matrix(embedding_model, embedding_dimension, word_to_idx_v3, embedding_matrix_v2)\n",
    "\n",
    "print(embedding_matrix_v1.shape)\n",
    "print(embedding_matrix_v2.shape)\n",
    "print(embedding_matrix_v3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train text and tags into sequence of integers\n",
    "train_text = df_train[\"text\"]\n",
    "train_tags = df_train[\"POS\"]\n",
    "\n",
    "#---------------OBS----------------------------------------------------------------\n",
    "#  Should x_train be numpy array?\n",
    "#  Should we use Word to index for V3 or V2???\n",
    "#---------------OBS----------------------------------------------------------------\n",
    "x_train = [sequence_to_integers(sequence, word_to_idx_v3) for sequence in train_text]\n",
    "y_train = [sequence_to_integers(sequence, tag_to_idx) for sequence in train_tags]\n",
    "\n",
    "\n",
    "val_text = df_val[\"text\"]\n",
    "val_tags = df_val[\"POS\"]\n",
    "x_val = [sequence_to_integers(sequence, word_to_idx_v3) for sequence in val_text]\n",
    "y_val = [sequence_to_integers(sequence, tag_to_idx) for sequence in val_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"-------------------------OBS--------------------------\n",
    "Needs to transform y to binary matrix with to_categorical (not so sure exactly why)\n",
    "----------------------------------------------------------\"\"\"\n",
    "\n",
    "x_train = pad_idx_seqs(x_train, find_max_length(x_train))\n",
    "y_train = to_categorical(pad_idx_seqs(y_train, find_max_length(y_train)))\n",
    "\n",
    "x_val = pad_idx_seqs(x_val, find_max_length(x_val))\n",
    "y_val = to_categorical(pad_idx_seqs(y_val, find_max_length(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 11:21:45.253102: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2900, 50)          20164550  \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 2900, 128)        58880     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2900, 46)          5934      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,229,364\n",
      "Trainable params: 20,229,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline_model = create_baseline_model()\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 16s 2s/step - loss: 0.0593 - acc: 8.4467e-04\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.0395 - acc: 0.0039\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.0268 - acc: 0.0060\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.0202 - acc: 0.0092\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.0172 - acc: 0.0622\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.0157 - acc: 0.1323\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.0151 - acc: 0.1324\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.0147 - acc: 0.1324\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.0145 - acc: 0.1391\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.0144 - acc: 0.2321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x179c93340>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              metrics=['acc'])\n",
    "baseline_model.fit(x=x_train, y=y_train, batch_size=20, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
