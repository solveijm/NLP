{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from collections import OrderedDict\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './Dataset/train/'# Should we use: os.path.join(os.getcwd(), \"OriginalDataset\")\n",
    "val_path = './Dataset/val/'\n",
    "test_path = './Dataset/test/'\n",
    "dataset_folder = os.path.join(os.getcwd(), \"OriginalDataset\")\n",
    "original_path = dataset_folder +'/dependency_treebank/'\n",
    "\n",
    "embedding_dimension = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request  #  download files\n",
    "import zipfile  #  unzip files\n",
    "\n",
    "def download_dataset():\n",
    "    dataset_folder = os.path.join(os.getcwd(), \"OriginalDataset\")\n",
    "\n",
    "    if not os.path.exists(dataset_folder):\n",
    "        os.makedirs(dataset_folder)\n",
    "\n",
    "    url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
    "\n",
    "    dataset_path = os.path.join(dataset_folder, \"dependency_treebank.zip\")\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        urllib.request.urlretrieve(url, dataset_path)\n",
    "        print(\"Successful download\")\n",
    "\n",
    "    with zipfile.ZipFile(dataset_path,\"r\") as zip_ref:\n",
    "        zip_ref.extractall(dataset_folder)\n",
    "    print(\"Successful extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful extraction\n"
     ]
    }
   ],
   "source": [
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(train_path, val_path, test_path, original_path):\n",
    "    if not os.path.exists(train_path):\n",
    "        print(\"making directory\")\n",
    "        os.makedirs(train_path)\n",
    "    if not os.path.exists(val_path):\n",
    "        os.makedirs(val_path)\n",
    "    if not os.path.exists(test_path):\n",
    "        os.makedirs(test_path)\n",
    "\n",
    "    original_dataset = os.listdir(original_path)\n",
    "    \n",
    "    original_dataset.sort()\n",
    "    org_train = original_dataset[0:100]\n",
    "    org_val = original_dataset[100:150]\n",
    "    org_test = original_dataset[150:]\n",
    "\n",
    "    for f in org_train:\n",
    "        os.rename(original_path+f, train_path+f)\n",
    "    for f in org_val:\n",
    "        os.rename(original_path+f, val_path+f)\n",
    "    for f in org_test:\n",
    "        os.rename(original_path+f, test_path+f)\n",
    "    print(\"Successful spilt\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful spilt\n"
     ]
    }
   ],
   "source": [
    "split_dataset(train_path, val_path, test_path, original_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(lst): # DO WE NEED THIS?\n",
    "    lowercase_list = [x.lower() for x in lst]\n",
    "    return lowercase_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(dataset_path: str):\n",
    "    dataframe_rows = []\n",
    "    documents = os.listdir(dataset_path)\n",
    "    for document in documents:\n",
    "        path = os.path.join(dataset_path, document)\n",
    "        with open(path, 'r') as f:\n",
    "            a = [[x for x in ln.split()] for ln in f]\n",
    "        #---------------For splitting on sentence-------------------#\n",
    "        sentence = []\n",
    "        sentence_tag = []\n",
    "        for x in a:\n",
    "            if x!=[]:\n",
    "                sentence.append(x[0])\n",
    "                sentence_tag.append(x[1])\n",
    "            else:\n",
    "                dataframe_row = {\n",
    "                    \"text\": sentence,\n",
    "                    \"POS\": sentence_tag\n",
    "                }\n",
    "                sentence = []\n",
    "                sentence_tag = []\n",
    "                dataframe_rows.append(dataframe_row)\n",
    "        #----------------------------------------------------------#\n",
    "        #------------------To split on document--------------------#\n",
    "        \"\"\"\n",
    "        a2 = [x for x in a if x != []] # OBS! Removing all empty lines in file so we can make an array\n",
    "        arr = np.array(a2)\n",
    "        text = list(arr[:, 0])\n",
    "        tagg = list(arr[:, 1])\n",
    "        #POStuple =  [tagg[x] for x in range(len(text))] #[(text[x], tagg[x]) for x in range(len(text))]\n",
    "        document_id = int(document[4:8])\n",
    "        dataframe_row = {\n",
    "            \"document_id\": document_id,\n",
    "            \"text\": text,\n",
    "            \"POS\": tagg\n",
    "        }\n",
    "        dataframe_rows.append(dataframe_row)\"\"\"\n",
    "        #----------------------------------------------------------#\n",
    "    # transform the list of rows in a proper dataframe\n",
    "    df = pd.DataFrame(dataframe_rows)\n",
    "    df = df[[\"text\", \"POS\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[In, reference, to, your, Oct., 9, page-one, a...</td>\n",
       "      <td>[IN, NN, TO, PRP$, NNP, CD, NN, NN, ``, NNP, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[In, part, ,, this, may, reflect, the, fact, t...</td>\n",
       "      <td>[IN, NN, ,, DT, MD, VB, DT, NN, IN, ``, PRP, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Among, professionals, ,, 76, %, have, a, favo...</td>\n",
       "      <td>[IN, NNS, ,, CD, NN, VBP, DT, JJ, NN, IN, PRP$...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[While, a, quarter, of, black, voters, disappr...</td>\n",
       "      <td>[IN, DT, NN, IN, JJ, NNS, VBP, IN, NNP, NNP, P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, statistics, imply, that, three-quarters,...</td>\n",
       "      <td>[DT, NNS, VBP, IN, NNS, IN, NNS, VBP, IN, NNP,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>[He, said, authors, of, Scoring, High, ``, scr...</td>\n",
       "      <td>[PRP, VBD, NNS, IN, NNP, NNP, ``, RB, VBP, '',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>[When, Scoring, High, first, came, out, in, 19...</td>\n",
       "      <td>[WRB, NNP, NNP, RB, VBD, RB, IN, CD, ,, PRP, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>[McGraw-Hill, was, outraged, .]</td>\n",
       "      <td>[NNP, VBD, JJ, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>[In, a, 1985, advisory, to, educators, ,, McGr...</td>\n",
       "      <td>[IN, DT, CD, NN, TO, NNS, ,, NNP, VBD, NNP, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>[But, in, 1988, ,, McGraw-Hill, purchased, the...</td>\n",
       "      <td>[CC, IN, CD, ,, NNP, VBD, DT, NNP, NNP, NN, WD...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1863 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     [In, reference, to, your, Oct., 9, page-one, a...   \n",
       "1     [In, part, ,, this, may, reflect, the, fact, t...   \n",
       "2     [Among, professionals, ,, 76, %, have, a, favo...   \n",
       "3     [While, a, quarter, of, black, voters, disappr...   \n",
       "4     [The, statistics, imply, that, three-quarters,...   \n",
       "...                                                 ...   \n",
       "1858  [He, said, authors, of, Scoring, High, ``, scr...   \n",
       "1859  [When, Scoring, High, first, came, out, in, 19...   \n",
       "1860                    [McGraw-Hill, was, outraged, .]   \n",
       "1861  [In, a, 1985, advisory, to, educators, ,, McGr...   \n",
       "1862  [But, in, 1988, ,, McGraw-Hill, purchased, the...   \n",
       "\n",
       "                                                    POS  \n",
       "0     [IN, NN, TO, PRP$, NNP, CD, NN, NN, ``, NNP, N...  \n",
       "1     [IN, NN, ,, DT, MD, VB, DT, NN, IN, ``, PRP, V...  \n",
       "2     [IN, NNS, ,, CD, NN, VBP, DT, JJ, NN, IN, PRP$...  \n",
       "3     [IN, DT, NN, IN, JJ, NNS, VBP, IN, NNP, NNP, P...  \n",
       "4     [DT, NNS, VBP, IN, NNS, IN, NNS, VBP, IN, NNP,...  \n",
       "...                                                 ...  \n",
       "1858  [PRP, VBD, NNS, IN, NNP, NNP, ``, RB, VBP, '',...  \n",
       "1859  [WRB, NNP, NNP, RB, VBD, RB, IN, CD, ,, PRP, V...  \n",
       "1860                                  [NNP, VBD, JJ, .]  \n",
       "1861  [IN, DT, CD, NN, TO, NNS, ,, NNP, VBD, NNP, NN...  \n",
       "1862  [CC, IN, CD, ,, NNP, VBD, DT, NNP, NNP, NN, WD...  \n",
       "\n",
       "[1863 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = create_dataframe(train_path)\n",
    "df_val = create_dataframe(val_path)\n",
    "df_test = create_dataframe(test_path)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed the words using GloVe embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    # Check download\n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Word2Vec: 300\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = load_embedding_model(embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer_train = Tokenizer()\n",
    "\n",
    "# UNCOMMENT IF WE WANT TO INCLUDE GOLVE vocabualry\n",
    "word_tokenizer_train.fit_on_texts(embedding_model.index_to_key) \n",
    "word_tokenizer_train.fit_on_texts(df_train[\"text\"])\n",
    "\n",
    "x_encoded_train = word_tokenizer_train.texts_to_sequences(df_train[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_tokenizer_train = Tokenizer()\n",
    "\n",
    "tag_tokenizer_train.fit_on_texts(df_train[\"POS\"])\n",
    "y_encoded_train = tag_tokenizer_train.texts_to_sequences(df_train[\"POS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_length(seqs):\n",
    "    max_len = 0\n",
    "    for seq in seqs:\n",
    "        if len(seq) > max_len:\n",
    "            max_len = len(seq)\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_padded_train = pad_idx_seqs(x_encoded_train, find_max_length(x_encoded_train))\n",
    "y_padded_train = pad_idx_seqs(y_encoded_train, find_max_length(y_encoded_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW METHODE TO GET WORD TO INDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340107\n"
     ]
    }
   ],
   "source": [
    "VOCABULARY_SIZE = len(word_tokenizer_train.word_index) + 1\n",
    "\n",
    "word2idx_train = word_tokenizer_train.word_index\n",
    "idx2word_train = word_tokenizer_train.index_word\n",
    "print(len(word2idx_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx_train = tag_tokenizer_train.word_index\n",
    "idx2tag_train = tag_tokenizer_train.index_word\n",
    "len(tag2idx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_VOCABULARY_SIZE = len(tag2idx_train) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBS Computes the OOV with random embeddings\n",
    "def create_embedding_matrix(embedding_model, embedding_dimension, word_to_idx):\n",
    "    embedding_matrix = np.zeros((len(word_to_idx)+1, embedding_dimension), dtype=np.float32)\n",
    "    for word, idx in word_to_idx.items():\n",
    "        try:\n",
    "            embedding_vector = embedding_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "                                \n",
    "    return embedding_matrix\n",
    "\n",
    "def expand_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, old_embedding_matrix):\n",
    "    embedding_matrix = np.zeros((len(word_to_idx), embedding_dimension), dtype=np.float32)\n",
    "    embedding_matrix[0:len(old_embedding_matrix)] = old_embedding_matrix\n",
    "    \n",
    "    for word, idx in word_to_idx.items():\n",
    "        if idx >= len(old_embedding_matrix):\n",
    "            try:\n",
    "                embedding_vector = embedding_model[word]\n",
    "            except (KeyError, TypeError):\n",
    "                embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "                \n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = create_embedding_matrix(embedding_model, embedding_dimension, word2idx_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define x train and y train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_padded_train\n",
    "y_train = to_categorical(y_padded_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create baseline model\n",
    "\n",
    "##### two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://www.kaggle.com/tanyadayanand/pos-tagging-using-rnn/notebook\n",
    "\n",
    "Not sure about the parameters etc.. but at least it works to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_model():\n",
    "    bidirect_model = keras.Sequential()\n",
    "    bidirect_model.add(layers.Embedding(input_dim = VOCABULARY_SIZE,\n",
    "                                 output_dim    = embedding_dimension,\n",
    "                                 input_length  = find_max_length(x_train),\n",
    "                                 weights       = [embedding_matrix],\n",
    "                                 trainable = True\n",
    "                                ))\n",
    "    bidirect_model.add(layers.Bidirectional(layers.LSTM(64, return_sequences=True)))\n",
    "    #bidirect_model.add(layers.TimeDistributed(layers.Dense(len(tag_to_idx), activation='softmax')))\n",
    "    bidirect_model.add(layers.Dense(TAG_VOCABULARY_SIZE, activation='softmax'))\n",
    "    \n",
    "    return bidirect_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 15:17:14.867566: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 249, 50)           17005400  \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 249, 128)         58880     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 249, 46)           5934      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,070,214\n",
      "Trainable params: 17,070,214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline_model = create_baseline_model()\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 39s 379ms/step - loss: 0.0079 - mae: 0.0166 - acc: 0.9003\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 25s 338ms/step - loss: 0.0020 - mae: 0.0044 - acc: 0.9152\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 25s 336ms/step - loss: 0.0019 - mae: 0.0042 - acc: 0.9288\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 27s 358ms/step - loss: 0.0018 - mae: 0.0038 - acc: 0.9392\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 26s 350ms/step - loss: 0.0015 - mae: 0.0033 - acc: 0.9480\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 25s 338ms/step - loss: 0.0013 - mae: 0.0029 - acc: 0.9519\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 26s 348ms/step - loss: 0.0012 - mae: 0.0027 - acc: 0.9581\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 26s 341ms/step - loss: 0.0010 - mae: 0.0024 - acc: 0.9641\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 28s 371ms/step - loss: 9.2793e-04 - mae: 0.0022 - acc: 0.9693\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 26s 350ms/step - loss: 8.2181e-04 - mae: 0.0019 - acc: 0.9732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10e49f850>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.compile(loss=\"mse\",\n",
    "              optimizer=\"Adam\",\n",
    "              metrics=['mae', \"acc\"])\n",
    "baseline_model.fit(x=x_train, batch_size=25, y=y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
