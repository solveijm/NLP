{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/andreastettejessen/opt/anaconda3/lib/python3.8/site-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/andreastettejessen/opt/anaconda3/lib/python3.8/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/andreastettejessen/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/andreastettejessen/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.7.0-cp38-cp38-macosx_10_11_x86_64.whl (207.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 207.1 MB 22 kB/s  eta 0:00:01   |▊                               | 4.6 MB 4.3 MB/s eta 0:00:47     |█                               | 6.0 MB 4.3 MB/s eta 0:00:47     |█                               | 6.8 MB 4.3 MB/s eta 0:00:48     |██                              | 13.0 MB 2.0 MB/s eta 0:01:36     |██▌                             | 16.5 MB 3.4 MB/s eta 0:00:57     |███▌                            | 22.8 MB 499 kB/s eta 0:06:09     |█████▉                          | 37.5 MB 5.2 MB/s eta 0:00:33     |██████████▎                     | 66.4 MB 1.5 MB/s eta 0:01:33     |███████████▍                    | 73.8 MB 2.3 MB/s eta 0:00:57     |████████████▎                   | 79.2 MB 2.0 MB/s eta 0:01:04     |████████████▎                   | 79.3 MB 2.0 MB/s eta 0:01:04     |████████████▋                   | 81.7 MB 1.5 MB/s eta 0:01:23     |█████████████▎                  | 86.0 MB 2.4 MB/s eta 0:00:52     |██████████████▎                 | 92.1 MB 1.8 MB/s eta 0:01:03     |███████████████▌                | 100.6 MB 3.8 MB/s eta 0:00:28     |████████████████▉               | 109.1 MB 4.9 MB/s eta 0:00:21     |█████████████████████           | 136.3 MB 747 kB/s eta 0:01:35     |██████████████████████▏         | 143.4 MB 1.6 MB/s eta 0:00:41     |██████████████████████████      | 167.9 MB 1.5 MB/s eta 0:00:27     |██████████████████████████████▏ | 195.0 MB 874 kB/s eta 0:00:14     |██████████████████████████████▏ | 195.4 MB 874 kB/s eta 0:00:14███████▎ | 195.9 MB 874 kB/s eta 0:00:13     |██████████████████████████████▎ | 196.1 MB 3.4 MB/s eta 0:00:04     |██████████████████████████████▌ | 197.0 MB 3.4 MB/s eta 0:00:03     |██████████████████████████████▋ | 198.4 MB 1.8 MB/s eta 0:00:05     |███████████████████████████████▊| 205.1 MB 1.4 MB/s eta 0:00:02     |████████████████████████████████| 206.5 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /Users/andreastettejessen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/andreastettejessen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting flatbuffers<3.0,>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 762 kB/s eta 0:00:01     |█████████████████▋              | 3.2 MB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast<0.5.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.19.1-cp38-cp38-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libclang>=9.0.1\n",
      "  Downloading libclang-12.0.0-py2.py3-none-macosx_10_9_x86_64.whl (12.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.2 MB 102 kB/s eta 0:00:01    |████▌                           | 1.7 MB 1.6 MB/s eta 0:00:07     |█████████████████████████████▏  | 11.2 MB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.42.0-cp38-cp38-macosx_10_10_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.32.0 in /Users/andreastettejessen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /Users/andreastettejessen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.20.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.21.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.22.0-cp38-cp38-macosx_10_14_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /Users/andreastettejessen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras<2.8,>=2.7.0rc0\n",
      "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 677 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
      "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "\u001b[K     |████████████████████████████████| 463 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /Users/andreastettejessen/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /Users/andreastettejessen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 1.5 MB/s eta 0:00:01     |██████████████████▏             | 2.0 MB 1.5 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /Users/andreastettejessen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /Users/andreastettejessen/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (52.0.0.post20210125)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.8.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/andreastettejessen/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.4.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 925 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /Users/andreastettejessen/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/andreastettejessen/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/andreastettejessen/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/andreastettejessen/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=6460c2981c206013477de93407f4561bd94c2e6b9aea72f26aa0f17e2d627dd9\n",
      "  Stored in directory: /Users/andreastettejessen/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.10.0\n",
      "    Uninstalling importlib-metadata-3.10.0:\n",
      "      Successfully uninstalled importlib-metadata-3.10.0\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-4.2.4 flatbuffers-2.0 gast-0.4.0 google-auth-2.3.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.42.0 importlib-metadata-4.8.2 keras-2.7.0 keras-preprocessing-1.1.2 libclang-12.0.0 markdown-3.3.6 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.19.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.22.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful extraction\n"
     ]
    }
   ],
   "source": [
    "import urllib.request  #  download files\n",
    "import zipfile  #  unzip files\n",
    "\n",
    "dataset_folder = os.path.join(os.getcwd(), \"OriginalDataset\")\n",
    "\n",
    "if not os.path.exists(dataset_folder):\n",
    "    os.makedirs(dataset_folder)\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
    "\n",
    "dataset_path = os.path.join(dataset_folder, \"dependency_treebank.zip\")\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    urllib.request.urlretrieve(url, dataset_path)\n",
    "    print(\"Successful download\")\n",
    "\n",
    "with zipfile.ZipFile(dataset_path,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(dataset_folder)\n",
    "print(\"Successful extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './Dataset/train/'# Should we use: os.path.join(os.getcwd(), \"OriginalDataset\")\n",
    "val_path = './Dataset/val/'\n",
    "test_path = './Dataset/test/'\n",
    "original_path = dataset_folder +'/dependency_treebank/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(train_path):\n",
    "    print(\"making directory\")\n",
    "    os.makedirs(train_path)\n",
    "if not os.path.exists(val_path):\n",
    "    os.makedirs(val_path)\n",
    "if not os.path.exists(test_path):\n",
    "    os.makedirs(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = os.listdir(original_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset.sort()\n",
    "org_train = original_dataset[0:100]\n",
    "org_val = original_dataset[100:150]\n",
    "org_test = original_dataset[150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in org_train:\n",
    "    os.rename(original_path+f, train_path+f)\n",
    "for f in org_val:\n",
    "    os.rename(original_path+f, val_path+f)\n",
    "for f in org_test:\n",
    "    os.rename(original_path+f, test_path+f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(lst): # DO WE NEED THIS?\n",
    "    lowercase_list = [x.lower() for x in lst]\n",
    "    return lowercase_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(dataset_path: str):\n",
    "    dataframe_rows = []\n",
    "    documents = os.listdir(dataset_path)\n",
    "    for document in documents:\n",
    "        path = os.path.join(dataset_path, document)\n",
    "        with open(path, 'r') as f:\n",
    "            a = [[x for x in ln.split()] for ln in f]\n",
    "        a2 = [x for x in a if x != []] # OBS! Removing all empty lines in file so we can make an array\n",
    "        arr = np.array(a2)\n",
    "        text = arr[:, 0]\n",
    "        tagg = arr[:, 1]\n",
    "        #POStuple =  [tagg[x] for x in range(len(text))] #[(text[x], tagg[x]) for x in range(len(text))]\n",
    "        document_id = int(document[4:8])\n",
    "        dataframe_row = {\n",
    "            \"document_id\": document_id,\n",
    "            \"text\": text,\n",
    "            \"POS\": tagg\n",
    "        }\n",
    "        dataframe_rows.append(dataframe_row)\n",
    "    # transform the list of rows in a proper dataframe\n",
    "    df = pd.DataFrame(dataframe_rows)\n",
    "    df = df[[\"document_id\", \"text\", \"POS\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>text</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95</td>\n",
       "      <td>[In, reference, to, your, Oct., 9, page-one, a...</td>\n",
       "      <td>[IN, NN, TO, PRP$, NNP, CD, NN, NN, ``, NNP, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>[Judging, from, the, Americana, in, Haruki, Mu...</td>\n",
       "      <td>[VBG, IN, DT, NNS, IN, NNP, NNP, POS, ``, DT, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>[Sir, Peter, Walters, ,, 58-year-old, chairman...</td>\n",
       "      <td>[NNP, NNP, NNP, ,, JJ, NN, IN, NNP, NNP, NNP, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>[PAPERS, :, Backe, Group, Inc., agreed, to, ac...</td>\n",
       "      <td>[NNS, :, NNP, NNP, NNP, VBD, TO, VB, NNP, NNP,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
       "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>82</td>\n",
       "      <td>[Criticism, in, the, U.S., over, recent, Japan...</td>\n",
       "      <td>[NNP, IN, DT, NNP, IN, JJ, JJ, NNS, VBZ, VBG, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>71</td>\n",
       "      <td>[When, Warren, Winiarski, ,, proprietor, of, S...</td>\n",
       "      <td>[WRB, NNP, NNP, ,, NN, IN, NNP, POS, NNP, NNP,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>20</td>\n",
       "      <td>[The, U.S., ,, claiming, some, success, in, it...</td>\n",
       "      <td>[DT, NNP, ,, VBG, DT, NN, IN, PRP$, NN, NN, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>14</td>\n",
       "      <td>[Norman, Ricken, ,, 52, years, old, and, forme...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, CC, NN,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>45</td>\n",
       "      <td>[Since, chalk, first, touched, slate, ,, schoo...</td>\n",
       "      <td>[IN, NN, RB, VBD, NN, ,, NN, VBP, VBN, TO, VB,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    document_id                                               text  \\\n",
       "0            95  [In, reference, to, your, Oct., 9, page-one, a...   \n",
       "1            37  [Judging, from, the, Americana, in, Haruki, Mu...   \n",
       "2            66  [Sir, Peter, Walters, ,, 58-year-old, chairman...   \n",
       "3            52  [PAPERS, :, Backe, Group, Inc., agreed, to, ac...   \n",
       "4             3  [A, form, of, asbestos, once, used, to, make, ...   \n",
       "..          ...                                                ...   \n",
       "95           82  [Criticism, in, the, U.S., over, recent, Japan...   \n",
       "96           71  [When, Warren, Winiarski, ,, proprietor, of, S...   \n",
       "97           20  [The, U.S., ,, claiming, some, success, in, it...   \n",
       "98           14  [Norman, Ricken, ,, 52, years, old, and, forme...   \n",
       "99           45  [Since, chalk, first, touched, slate, ,, schoo...   \n",
       "\n",
       "                                                  POS  \n",
       "0   [IN, NN, TO, PRP$, NNP, CD, NN, NN, ``, NNP, N...  \n",
       "1   [VBG, IN, DT, NNS, IN, NNP, NNP, POS, ``, DT, ...  \n",
       "2   [NNP, NNP, NNP, ,, JJ, NN, IN, NNP, NNP, NNP, ...  \n",
       "3   [NNS, :, NNP, NNP, NNP, VBD, TO, VB, NNP, NNP,...  \n",
       "4   [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...  \n",
       "..                                                ...  \n",
       "95  [NNP, IN, DT, NNP, IN, JJ, JJ, NNS, VBZ, VBG, ...  \n",
       "96  [WRB, NNP, NNP, ,, NN, IN, NNP, POS, NNP, NNP,...  \n",
       "97  [DT, NNP, ,, VBG, DT, NN, IN, PRP$, NN, NN, ,,...  \n",
       "98  [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, CC, NN,...  \n",
       "99  [IN, NN, RB, VBD, NN, ,, NN, VBP, VBN, TO, VB,...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = create_dataframe(train_path)\n",
    "df_val = create_dataframe(val_path)\n",
    "df_test = create_dataframe(test_path)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed the words using GloVe embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    # Check download\n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Word2Vec: 300\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 50\n",
    "\n",
    "embedding_model = load_embedding_model(embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list(data_frame, lable):\n",
    "    big_list = []\n",
    "    text = data_frame[lable]\n",
    "    for row in data_frame[lable]:\n",
    "        big_list += row.tolist()\n",
    "\n",
    "    word_list = set(big_list)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_train = get_word_list(df_train, \"text\")\n",
    "word_list_val = get_word_list(df_val, \"text\")\n",
    "word_list_test = get_word_list(df_test, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"---------------------OBS------------------------\n",
    "Where shoukld we get tags? Is it ok to assume they are in df_train?\n",
    "\"\"\"\n",
    "tags_list = get_word_list(df_train, \"POS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition\n",
    "\n",
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                    word_list):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_listing: dataset specific vocabulary (list)\n",
    "\n",
    "    :return\n",
    "        - list of OOV terms\n",
    "    \"\"\"\n",
    "    embedding_vocabulary = set(embedding_model.index_to_key) # Was previously: set(embedding_model.vocab.keys())\n",
    "    oov = set(word_list).difference(embedding_vocabulary)\n",
    "    return list(oov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV1 terms: 2346 (0.29%)\n",
      "Total OOV2 terms: 1524 (0.19%)\n",
      "Total OOV3 terms: 957 (0.12%)\n"
     ]
    }
   ],
   "source": [
    "oov1_terms = check_OOV_terms(embedding_model, word_list_train)\n",
    "#--------------------------OBS--------------------------------------------#\n",
    "# To make sure OOV2 does not contain words from OOV1 a check is \n",
    "# implemented during the embedding in the word to index function. \n",
    "# It might be an idea to include this check in the chekc_OOV_terms as well. \n",
    "# Question for the professor?\n",
    "#--------------------------OBS--------------------------------------------#\n",
    "oov2_terms = check_OOV_terms(embedding_model, word_list_val)\n",
    "oov3_terms = check_OOV_terms(embedding_model, word_list_test)\n",
    "\n",
    "\n",
    "print(\"Total OOV1 terms: {0} ({1:.2f}%)\".format(len(oov1_terms), float(len(oov1_terms)) / len(word_list_train)))\n",
    "print(\"Total OOV2 terms: {0} ({1:.2f}%)\".format(len(oov2_terms), float(len(oov2_terms)) / len(word_list_train)))\n",
    "print(\"Total OOV3 terms: {0} ({1:.2f}%)\".format(len(oov3_terms), float(len(oov3_terms)) / len(word_list_train)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word_to_index (for one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def create_word_to_idx(vocabulary):\n",
    "    word_to_idx = OrderedDict()\n",
    "    idx_to_word = OrderedDict()\n",
    "    word_to_idx[\"PADDING\"] = 0\n",
    "    idx_to_word[0] = \"PADDING\"\n",
    "    # Start from one. Index 0 is reserved for padding\n",
    "    current_idx = 1\n",
    "    for word in vocabulary:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = current_idx\n",
    "            idx_to_word[current_idx] = word\n",
    "            current_idx += 1\n",
    "            \n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "def update_word_to_idx(old_idx_to_word, old_word_to_idx, new_words):\n",
    "    word_to_idx = old_word_to_idx.copy()\n",
    "    idx_to_word = old_idx_to_word.copy()\n",
    "    current_idx = len(word_to_idx)\n",
    "    for word in new_words:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = current_idx\n",
    "            idx_to_word[current_idx] = word\n",
    "            current_idx += 1\n",
    "            \n",
    "    return word_to_idx, idx_to_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"----------------------OBS------------------------------------\n",
    "Should we also compute word to index for the test set and embed\n",
    "these words, and then use all of them for training and validation\n",
    "or should we keep the test set seperate and only compute this for\n",
    "testing?\n",
    "----------------------OBS------------------------------------\"\"\"\n",
    "word_to_idx_v1, idx_to_word_v1 = create_word_to_idx(set(embedding_model.index_to_key))\n",
    "word_to_idx_v2, idx_to_word_v2 = update_word_to_idx(idx_to_word_v1, word_to_idx_v1, oov1_terms)\n",
    "word_to_idx_v3, idx_to_word_v3 = update_word_to_idx(idx_to_word_v2, word_to_idx_v2, oov2_terms)\n",
    "\n",
    "tag_to_idx, idx_to_tag = create_word_to_idx(tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_idx_word_list(idx_to_word,word_to_idx, write_path):\n",
    "    if not os.path.exists(os.path.dirname(write_path)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(write_path))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    with open(write_path, 'wb') as f:\n",
    "        pickle.dump(obj={\"word_to_idx\": words_lexicon, \"idx_to_word\":idx_to_word} , file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBS Computes the OOV with random embeddings\n",
    "def create_embedding_matrix(embedding_model, embedding_dimension, word_to_idx):\n",
    "    embedding_matrix = np.zeros((len(word_to_idx), embedding_dimension), dtype=np.float32)\n",
    "    for word, idx in word_to_idx.items():\n",
    "        try:\n",
    "            embedding_vector = embedding_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "                                \n",
    "    return embedding_matrix\n",
    "\n",
    "def expand_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, old_embedding_matrix):\n",
    "    embedding_matrix = np.zeros((len(word_to_idx), embedding_dimension), dtype=np.float32)\n",
    "    embedding_matrix[0:len(old_embedding_matrix)] = old_embedding_matrix\n",
    "    \n",
    "    for word, idx in word_to_idx.items():\n",
    "        if idx >= len(old_embedding_matrix):\n",
    "            try:\n",
    "                embedding_vector = embedding_model[word]\n",
    "            except (KeyError, TypeError):\n",
    "                embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "                \n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400001, 50)\n",
      "(402347, 50)\n",
      "(403291, 50)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_v1 = create_embedding_matrix(embedding_model, embedding_dimension, word_to_idx_v1)\n",
    "embedding_matrix_v2 = expand_embedding_matrix(embedding_model, embedding_dimension, word_to_idx_v2, embedding_matrix_v1)\n",
    "embedding_matrix_v3 = expand_embedding_matrix(embedding_model, embedding_dimension, word_to_idx_v3, embedding_matrix_v2)\n",
    "\n",
    "print(embedding_matrix_v1.shape)\n",
    "print(embedding_matrix_v2.shape)\n",
    "print(embedding_matrix_v3.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test (Remove later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('PADDING', 0), ('Hello', 1), ('Solveig', 2), ('is', 3), ('very', 4), ('wierd', 5), ('.', 6)])\n",
      "[[ 0.03997208  0.00246665 -0.00069787  0.03213861 -0.04520808]\n",
      " [ 0.01709219  0.00129411 -0.01262426  0.0485404   0.01751199]\n",
      " [-0.0372045   0.01828186  0.03109608  0.04075774  0.01572894]\n",
      " [ 0.03928509  0.01836319  0.03825684  0.02422366 -0.02967703]\n",
      " [-0.03588275 -0.02900422  0.02329878 -0.02268077  0.02943369]\n",
      " [ 0.02198299  0.04680008  0.02915153 -0.02601662  0.03152778]\n",
      " [-0.03792001 -0.01235144  0.02791452  0.01862329 -0.00011678]]\n",
      "OrderedDict([('PADDING', 0), ('Hello', 1), ('Solveig', 2), ('is', 3), ('very', 4), ('wierd', 5), ('.', 6), ('Andrea', 7), ('super', 8), ('cool', 9)])\n",
      "[[ 0.03997208  0.00246665 -0.00069787  0.03213861 -0.04520808]\n",
      " [ 0.01709219  0.00129411 -0.01262426  0.0485404   0.01751199]\n",
      " [-0.0372045   0.01828186  0.03109608  0.04075774  0.01572894]\n",
      " [ 0.03928509  0.01836319  0.03825684  0.02422366 -0.02967703]\n",
      " [-0.03588275 -0.02900422  0.02329878 -0.02268077  0.02943369]\n",
      " [ 0.02198299  0.04680008  0.02915153 -0.02601662  0.03152778]\n",
      " [-0.03792001 -0.01235144  0.02791452  0.01862329 -0.00011678]\n",
      " [-0.03171972  0.03209061  0.04984117  0.00634165  0.04144066]\n",
      " [-0.02388941 -0.02678237  0.03502192 -0.04207459 -0.03221312]\n",
      " [-0.00891809 -0.04764812 -0.01169111  0.03007069  0.00390908]]\n"
     ]
    }
   ],
   "source": [
    "vocab = ['Hello', 'Solveig', 'is', 'very', 'wierd', '.']\n",
    "\n",
    "w_t_idx, idx_t_w = create_word_to_idx(vocab)\n",
    "print(w_t_idx)\n",
    "\n",
    "embed_matrix = create_embedding_matrix(None, 5, w_t_idx)\n",
    "print(embed_matrix)\n",
    "\n",
    "new_words = ['Andrea', 'is', 'super', 'cool', '.']\n",
    "new_w_t_idx, new_idx_t_w = update_word_to_idx(idx_t_w, w_t_idx, new_words)\n",
    "print(new_w_t_idx)\n",
    "\n",
    "new_embed_matrix = expand_embedding_matrix(None, 5, new_w_t_idx, embed_matrix)\n",
    "print(new_embed_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform documents to sequences of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OBS\\nTakes only one list of words to integer, not the list with lists.\\n'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sequence_to_integers(sequence, word_to_idx):\n",
    "    integers = []\n",
    "    for element in sequence:\n",
    "        integers.append(word_to_idx[element])\n",
    "    return np.array(integers)\n",
    "\"\"\"OBS\n",
    "Takes only one list of words to integer, not the list with lists.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train text and tags into sequence of integers\n",
    "train_text = df_train[\"text\"]\n",
    "train_tags = df_train[\"POS\"]\n",
    "\n",
    "#---------------OBS----------------------------------------------------------------\n",
    "#  Should x_train be numpy array?\n",
    "#  Should we use Word to index for V3 or V2???\n",
    "#---------------OBS----------------------------------------------------------------\n",
    "x_train = [sequence_to_integers(sequence, word_to_idx_v3) for sequence in train_text]\n",
    "y_train = [sequence_to_integers(sequence, tag_to_idx) for sequence in train_tags]\n",
    "\n",
    "\n",
    "val_text = df_val[\"text\"]\n",
    "val_tags = df_val[\"POS\"]\n",
    "x_val = [sequence_to_integers(sequence, word_to_idx_v3) for sequence in val_text]\n",
    "y_val = [sequence_to_integers(sequence, tag_to_idx) for sequence in val_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_length(seqs):\n",
    "    max_len = 0\n",
    "    for seq in seqs:\n",
    "        if len(seq) > max_len:\n",
    "            max_len = len(seq)\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\"\"\"-------------------------OBS--------------------------\n",
    "Needs to transform y to binary matrix with to_categorical (not so sure exactly why)\n",
    "----------------------------------------------------------\"\"\"\n",
    "\n",
    "x_train = pad_idx_seqs(x_train, find_max_length(x_train))\n",
    "y_train = to_categorical(pad_idx_seqs(y_train, find_max_length(y_train)))\n",
    "\n",
    "x_val = pad_idx_seqs(x_val, find_max_length(x_val))\n",
    "y_val = to_categorical(pad_idx_seqs(y_val, find_max_length(y_val)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create baseline model\n",
    "\n",
    "##### two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top\n",
    "\n",
    "OBS: this section is not used atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def create_model(layers_info, compile_info) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Create a Keras model given a list of layer information\n",
    "\n",
    "    :param layers_info: a list of dictionaries, one for each layer\n",
    "    :param compile_info: dictionary containing compile information\n",
    "\n",
    "    :return\n",
    "        model: the built keras sequential model\n",
    "    \"\"\"\n",
    "\n",
    "    print('Found {} total layers'.format(len(layers_info)))\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    for info_idx, info in enumerate(layers_info):\n",
    "\n",
    "        layer = info['layer'](**{key: value for key, value in info.items() if key != 'layer'})\n",
    "        if info[\"name\"] == \"Bidirectional_LSTM_layer\":\n",
    "            model.add(layers.Bidirectional(layers.LSTM(64)))\n",
    "        else:\n",
    "            model.add(layer)\n",
    "\n",
    "    # Debug\n",
    "    model.summary()\n",
    "\n",
    "    # Compile\n",
    "    model.compile(**compile_info)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_layers_info = [\n",
    "    {\n",
    "        \"layer\": layers.Embedding,\n",
    "        \"output_dim\": embedding_dimension,\n",
    "        \"input_dim\": len(word_to_idx_v3),\n",
    "        \"weights\": [embedding_matrix_v3],\n",
    "        \"input_length\": find_max_length(x_train),\n",
    "        \"mask_zero\": True, # Because of padding\n",
    "        \"name\": \"embedding_layer\"\n",
    "    },\n",
    "    {\n",
    "        \"layer\": layers.LSTM,\n",
    "        \"units\": 50,\n",
    "        \"name\": \"Bidirectional_LSTM_layer\"\n",
    "    },\n",
    "    {\n",
    "        \"layer\": layers.Dense,\n",
    "        \"units\": len(tag_to_idx),\n",
    "        \"activation\": \"softmax\",\n",
    "        \"name\": \"Dense_layer\"\n",
    "    }\n",
    "]\n",
    "\n",
    "compile_info = {\n",
    "    'optimizer': keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    'loss': keras.losses.BinaryCrossentropy(),# Get new error if we change loss to this: 'sparse_categorical_crossentropy',\n",
    "    'metrics': [keras.metrics.SparseCategoricalAccuracy()],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 total layers\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_layer (Embedding)  (None, 2900, 50)         20164550  \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirectio  (None, 2900, 128)        58880     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " Dense_layer (Dense)         (None, 2900, 46)          5934      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,229,364\n",
      "Trainable params: 20,229,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(baseline_layers_info, compile_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New attempt at buliding baseline\n",
    "\n",
    "https://www.kaggle.com/tanyadayanand/pos-tagging-using-rnn/notebook\n",
    "\n",
    "Not sure about the parameters etc.. but at least it works to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_baseline_model():\n",
    "    bidirect_model = keras.Sequential()\n",
    "    bidirect_model.add(layers.Embedding(input_dim = len(word_to_idx_v3),\n",
    "                                 output_dim    = embedding_dimension,\n",
    "                                 input_length  = find_max_length(x_train),\n",
    "                                 weights       = [embedding_matrix_v3],\n",
    "                                 mask_zero     = True\n",
    "                                ))\n",
    "    bidirect_model.add(layers.Bidirectional(layers.LSTM(64, return_sequences=True)))\n",
    "    #bidirect_model.add(layers.TimeDistributed(layers.Dense(len(tag_to_idx), activation='softmax')))\n",
    "    bidirect_model.add(layers.Dense(len(tag_to_idx), activation='softmax'))\n",
    "    \n",
    "    return bidirect_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 2900, 50)          20164550  \n",
      "                                                                 \n",
      " bidirectional_10 (Bidirecti  (None, 2900, 128)        58880     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2900, 46)          5934      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,229,364\n",
      "Trainable params: 20,229,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline_model = create_baseline_model()\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 34s 34s/step - loss: 0.1146 - acc: 0.0081\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.1105 - acc: 0.0091\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.1064 - acc: 0.0103\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.1025 - acc: 0.0126\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.0985 - acc: 0.0156\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.0945 - acc: 0.0210\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.0904 - acc: 0.0269\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.0861 - acc: 0.0330\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.0818 - acc: 0.0373\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.0774 - acc: 0.0402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa908069fd0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              metrics=['acc'])\n",
    "baseline_model.fit(x=x_train, y=y_train, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2900)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2900)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(403291, 50)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_v3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
