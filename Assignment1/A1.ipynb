{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp39-cp39-macosx_10_9_x86_64.whl (24.0 MB)\n",
      "     |████████████████████████████████| 24.0 MB 57 kB/s             \n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "     |████████████████████████████████| 58 kB 337 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/site-packages (from gensim) (1.20.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/site-packages (from gensim) (1.6.2)\n",
      "Installing collected packages: smart-open, gensim\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Successfully installed gensim-4.1.2 smart-open-5.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful extraction\n"
     ]
    }
   ],
   "source": [
    "import urllib.request  #  download files\n",
    "import zipfile  #  unzip files\n",
    "\n",
    "dataset_folder = os.path.join(os.getcwd(), \"OriginalDataset\")\n",
    "\n",
    "if not os.path.exists(dataset_folder):\n",
    "    os.makedirs(dataset_folder)\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
    "\n",
    "dataset_path = os.path.join(dataset_folder, \"dependency_treebank.zip\")\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    urllib.request.urlretrieve(url, dataset_path)\n",
    "    print(\"Successful download\")\n",
    "\n",
    "with zipfile.ZipFile(dataset_path,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(dataset_folder)\n",
    "print(\"Successful extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './Dataset/train/'# Should we use: os.path.join(os.getcwd(), \"OriginalDataset\")\n",
    "val_path = './Dataset/val/'\n",
    "test_path = './Dataset/test/'\n",
    "original_path = dataset_folder +'/dependency_treebank/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(train_path):\n",
    "    print(\"making directory\")\n",
    "    os.makedirs(train_path)\n",
    "if not os.path.exists(val_path):\n",
    "    os.makedirs(val_path)\n",
    "if not os.path.exists(test_path):\n",
    "    os.makedirs(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wsj_0095.dp', 'wsj_0184.dp', 'wsj_0177.dp', 'wsj_0037.dp', 'wsj_0126.dp', 'wsj_0066.dp', 'wsj_0052.dp', 'wsj_0112.dp', 'wsj_0003.dp', 'wsj_0143.dp', 'wsj_0153.dp', 'wsj_0013.dp', 'wsj_0102.dp', 'wsj_0042.dp', 'wsj_0076.dp', 'wsj_0136.dp', 'wsj_0027.dp', 'wsj_0167.dp', 'wsj_0194.dp', 'wsj_0085.dp', 'wsj_0007.dp', 'wsj_0147.dp', 'wsj_0056.dp', 'wsj_0116.dp', 'wsj_0180.dp', 'wsj_0091.dp', 'wsj_0122.dp', 'wsj_0062.dp', 'wsj_0173.dp', 'wsj_0033.dp', 'wsj_0023.dp', 'wsj_0163.dp', 'wsj_0072.dp', 'wsj_0132.dp', 'wsj_0081.dp', 'wsj_0190.dp', 'wsj_0106.dp', 'wsj_0046.dp', 'wsj_0157.dp', 'wsj_0017.dp', 'wsj_0006.dp', 'wsj_0146.dp', 'wsj_0057.dp', 'wsj_0117.dp', 'wsj_0181.dp', 'wsj_0090.dp', 'wsj_0123.dp', 'wsj_0063.dp', 'wsj_0172.dp', 'wsj_0032.dp', 'wsj_0022.dp', 'wsj_0162.dp', 'wsj_0073.dp', 'wsj_0133.dp', 'wsj_0080.dp', 'wsj_0191.dp', 'wsj_0107.dp', 'wsj_0047.dp', 'wsj_0156.dp', 'wsj_0016.dp', 'wsj_0094.dp', 'wsj_0185.dp', 'wsj_0176.dp', 'wsj_0036.dp', 'wsj_0127.dp', 'wsj_0067.dp', 'wsj_0053.dp', 'wsj_0113.dp', 'wsj_0002.dp', 'wsj_0142.dp', 'wsj_0152.dp', 'wsj_0012.dp', 'wsj_0103.dp', 'wsj_0043.dp', 'wsj_0077.dp', 'wsj_0137.dp', 'wsj_0026.dp', 'wsj_0166.dp', 'wsj_0195.dp', 'wsj_0084.dp', 'wsj_0058.dp', 'wsj_0118.dp', 'wsj_0009.dp', 'wsj_0149.dp', 'wsj_0159.dp', 'wsj_0019.dp', 'wsj_0108.dp', 'wsj_0048.dp', 'wsj_0128.dp', 'wsj_0068.dp', 'wsj_0179.dp', 'wsj_0039.dp', 'wsj_0029.dp', 'wsj_0169.dp', 'wsj_0078.dp', 'wsj_0138.dp', 'wsj_0129.dp', 'wsj_0069.dp', 'wsj_0178.dp', 'wsj_0038.dp', 'wsj_0028.dp', 'wsj_0168.dp', 'wsj_0079.dp', 'wsj_0139.dp', 'wsj_0059.dp', 'wsj_0119.dp', 'wsj_0008.dp', 'wsj_0148.dp', 'wsj_0158.dp', 'wsj_0018.dp', 'wsj_0109.dp', 'wsj_0049.dp', 'wsj_0099.dp', 'wsj_0188.dp', 'wsj_0198.dp', 'wsj_0089.dp', 'wsj_0098.dp', 'wsj_0189.dp', 'wsj_0199.dp', 'wsj_0088.dp', 'wsj_0054.dp', 'wsj_0114.dp', 'wsj_0005.dp', 'wsj_0145.dp', 'wsj_0171.dp', 'wsj_0031.dp', 'wsj_0120.dp', 'wsj_0060.dp', 'wsj_0093.dp', 'wsj_0182.dp', 'wsj_0192.dp', 'wsj_0083.dp', 'wsj_0070.dp', 'wsj_0130.dp', 'wsj_0021.dp', 'wsj_0161.dp', 'wsj_0155.dp', 'wsj_0015.dp', 'wsj_0104.dp', 'wsj_0044.dp', 'wsj_0124.dp', 'wsj_0064.dp', 'wsj_0175.dp', 'wsj_0035.dp', 'wsj_0186.dp', 'wsj_0097.dp', 'wsj_0001.dp', 'wsj_0141.dp', 'wsj_0050.dp', 'wsj_0110.dp', 'wsj_0100.dp', 'wsj_0040.dp', 'wsj_0151.dp', 'wsj_0011.dp', 'wsj_0087.dp', 'wsj_0196.dp', 'wsj_0025.dp', 'wsj_0165.dp', 'wsj_0074.dp', 'wsj_0134.dp', 'wsj_0125.dp', 'wsj_0065.dp', 'wsj_0174.dp', 'wsj_0034.dp', 'wsj_0187.dp', 'wsj_0096.dp', 'wsj_0140.dp', 'wsj_0051.dp', 'wsj_0111.dp', 'wsj_0101.dp', 'wsj_0041.dp', 'wsj_0150.dp', 'wsj_0010.dp', 'wsj_0086.dp', 'wsj_0197.dp', 'wsj_0024.dp', 'wsj_0164.dp', 'wsj_0075.dp', 'wsj_0135.dp', 'wsj_0055.dp', 'wsj_0115.dp', 'wsj_0004.dp', 'wsj_0144.dp', 'wsj_0170.dp', 'wsj_0030.dp', 'wsj_0121.dp', 'wsj_0061.dp', 'wsj_0092.dp', 'wsj_0183.dp', 'wsj_0193.dp', 'wsj_0082.dp', 'wsj_0071.dp', 'wsj_0131.dp', 'wsj_0020.dp', 'wsj_0160.dp', 'wsj_0154.dp', 'wsj_0014.dp', 'wsj_0105.dp', 'wsj_0045.dp']\n"
     ]
    }
   ],
   "source": [
    "original_dataset = os.listdir(original_path)\n",
    "print(original_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wsj_0001.dp', 'wsj_0002.dp', 'wsj_0003.dp', 'wsj_0004.dp', 'wsj_0005.dp', 'wsj_0006.dp', 'wsj_0007.dp', 'wsj_0008.dp', 'wsj_0009.dp', 'wsj_0010.dp', 'wsj_0011.dp', 'wsj_0012.dp', 'wsj_0013.dp', 'wsj_0014.dp', 'wsj_0015.dp', 'wsj_0016.dp', 'wsj_0017.dp', 'wsj_0018.dp', 'wsj_0019.dp', 'wsj_0020.dp', 'wsj_0021.dp', 'wsj_0022.dp', 'wsj_0023.dp', 'wsj_0024.dp', 'wsj_0025.dp', 'wsj_0026.dp', 'wsj_0027.dp', 'wsj_0028.dp', 'wsj_0029.dp', 'wsj_0030.dp', 'wsj_0031.dp', 'wsj_0032.dp', 'wsj_0033.dp', 'wsj_0034.dp', 'wsj_0035.dp', 'wsj_0036.dp', 'wsj_0037.dp', 'wsj_0038.dp', 'wsj_0039.dp', 'wsj_0040.dp', 'wsj_0041.dp', 'wsj_0042.dp', 'wsj_0043.dp', 'wsj_0044.dp', 'wsj_0045.dp', 'wsj_0046.dp', 'wsj_0047.dp', 'wsj_0048.dp', 'wsj_0049.dp', 'wsj_0050.dp', 'wsj_0051.dp', 'wsj_0052.dp', 'wsj_0053.dp', 'wsj_0054.dp', 'wsj_0055.dp', 'wsj_0056.dp', 'wsj_0057.dp', 'wsj_0058.dp', 'wsj_0059.dp', 'wsj_0060.dp', 'wsj_0061.dp', 'wsj_0062.dp', 'wsj_0063.dp', 'wsj_0064.dp', 'wsj_0065.dp', 'wsj_0066.dp', 'wsj_0067.dp', 'wsj_0068.dp', 'wsj_0069.dp', 'wsj_0070.dp', 'wsj_0071.dp', 'wsj_0072.dp', 'wsj_0073.dp', 'wsj_0074.dp', 'wsj_0075.dp', 'wsj_0076.dp', 'wsj_0077.dp', 'wsj_0078.dp', 'wsj_0079.dp', 'wsj_0080.dp', 'wsj_0081.dp', 'wsj_0082.dp', 'wsj_0083.dp', 'wsj_0084.dp', 'wsj_0085.dp', 'wsj_0086.dp', 'wsj_0087.dp', 'wsj_0088.dp', 'wsj_0089.dp', 'wsj_0090.dp', 'wsj_0091.dp', 'wsj_0092.dp', 'wsj_0093.dp', 'wsj_0094.dp', 'wsj_0095.dp', 'wsj_0096.dp', 'wsj_0097.dp', 'wsj_0098.dp', 'wsj_0099.dp', 'wsj_0100.dp']\n",
      "['wsj_0101.dp', 'wsj_0102.dp', 'wsj_0103.dp', 'wsj_0104.dp', 'wsj_0105.dp', 'wsj_0106.dp', 'wsj_0107.dp', 'wsj_0108.dp', 'wsj_0109.dp', 'wsj_0110.dp', 'wsj_0111.dp', 'wsj_0112.dp', 'wsj_0113.dp', 'wsj_0114.dp', 'wsj_0115.dp', 'wsj_0116.dp', 'wsj_0117.dp', 'wsj_0118.dp', 'wsj_0119.dp', 'wsj_0120.dp', 'wsj_0121.dp', 'wsj_0122.dp', 'wsj_0123.dp', 'wsj_0124.dp', 'wsj_0125.dp', 'wsj_0126.dp', 'wsj_0127.dp', 'wsj_0128.dp', 'wsj_0129.dp', 'wsj_0130.dp', 'wsj_0131.dp', 'wsj_0132.dp', 'wsj_0133.dp', 'wsj_0134.dp', 'wsj_0135.dp', 'wsj_0136.dp', 'wsj_0137.dp', 'wsj_0138.dp', 'wsj_0139.dp', 'wsj_0140.dp', 'wsj_0141.dp', 'wsj_0142.dp', 'wsj_0143.dp', 'wsj_0144.dp', 'wsj_0145.dp', 'wsj_0146.dp', 'wsj_0147.dp', 'wsj_0148.dp', 'wsj_0149.dp', 'wsj_0150.dp']\n",
      "['wsj_0151.dp', 'wsj_0152.dp', 'wsj_0153.dp', 'wsj_0154.dp', 'wsj_0155.dp', 'wsj_0156.dp', 'wsj_0157.dp', 'wsj_0158.dp', 'wsj_0159.dp', 'wsj_0160.dp', 'wsj_0161.dp', 'wsj_0162.dp', 'wsj_0163.dp', 'wsj_0164.dp', 'wsj_0165.dp', 'wsj_0166.dp', 'wsj_0167.dp', 'wsj_0168.dp', 'wsj_0169.dp', 'wsj_0170.dp', 'wsj_0171.dp', 'wsj_0172.dp', 'wsj_0173.dp', 'wsj_0174.dp', 'wsj_0175.dp', 'wsj_0176.dp', 'wsj_0177.dp', 'wsj_0178.dp', 'wsj_0179.dp', 'wsj_0180.dp', 'wsj_0181.dp', 'wsj_0182.dp', 'wsj_0183.dp', 'wsj_0184.dp', 'wsj_0185.dp', 'wsj_0186.dp', 'wsj_0187.dp', 'wsj_0188.dp', 'wsj_0189.dp', 'wsj_0190.dp', 'wsj_0191.dp', 'wsj_0192.dp', 'wsj_0193.dp', 'wsj_0194.dp', 'wsj_0195.dp', 'wsj_0196.dp', 'wsj_0197.dp', 'wsj_0198.dp', 'wsj_0199.dp']\n"
     ]
    }
   ],
   "source": [
    "original_dataset.sort()\n",
    "org_train = original_dataset[0:100]\n",
    "org_val = original_dataset[100:150]\n",
    "org_test = original_dataset[150:]\n",
    "\n",
    "print(org_train)\n",
    "print(org_val)\n",
    "print(org_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in org_train:\n",
    "    os.rename(original_path+f, train_path+f)\n",
    "for f in org_val:\n",
    "    os.rename(original_path+f, val_path+f)\n",
    "for f in org_test:\n",
    "    os.rename(original_path+f, test_path+f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(lst): # DO WE NEED THIS?\n",
    "    lowercase_list = [x.lower() for x in lst]\n",
    "    return lowercase_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(dataset_path: str):\n",
    "    dataframe_rows = []\n",
    "    documents = os.listdir(dataset_path)\n",
    "    for document in documents:\n",
    "        path = os.path.join(dataset_path, document)\n",
    "        with open(path, 'r') as f:\n",
    "            a = [[x for x in ln.split()] for ln in f]\n",
    "        a2 = [x for x in a if x != []] # OBS! Removing all empty lines in file so we can make an array\n",
    "        arr = np.array(a2)\n",
    "        text = arr[:, 0]\n",
    "        tagg = arr[:, 1]\n",
    "        POStuple =  [tagg[x] for x in range(len(text))] #[(text[x], tagg[x]) for x in range(len(text))]\n",
    "        document_id = int(document[4:8])\n",
    "        dataframe_row = {\n",
    "            \"document_id\": document_id,\n",
    "            \"text\": text,\n",
    "            \"POS\": POStuple\n",
    "        }\n",
    "        dataframe_rows.append(dataframe_row)\n",
    "    # transform the list of rows in a proper dataframe\n",
    "    df = pd.DataFrame(dataframe_rows)\n",
    "    df = df[[\"document_id\", \"text\", \"POS\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>text</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95</td>\n",
       "      <td>[In, reference, to, your, Oct., 9, page-one, a...</td>\n",
       "      <td>[IN, NN, TO, PRP$, NNP, CD, NN, NN, ``, NNP, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>[Judging, from, the, Americana, in, Haruki, Mu...</td>\n",
       "      <td>[VBG, IN, DT, NNS, IN, NNP, NNP, POS, ``, DT, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>[Sir, Peter, Walters, ,, 58-year-old, chairman...</td>\n",
       "      <td>[NNP, NNP, NNP, ,, JJ, NN, IN, NNP, NNP, NNP, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>[PAPERS, :, Backe, Group, Inc., agreed, to, ac...</td>\n",
       "      <td>[NNS, :, NNP, NNP, NNP, VBD, TO, VB, NNP, NNP,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
       "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>82</td>\n",
       "      <td>[Criticism, in, the, U.S., over, recent, Japan...</td>\n",
       "      <td>[NNP, IN, DT, NNP, IN, JJ, JJ, NNS, VBZ, VBG, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>71</td>\n",
       "      <td>[When, Warren, Winiarski, ,, proprietor, of, S...</td>\n",
       "      <td>[WRB, NNP, NNP, ,, NN, IN, NNP, POS, NNP, NNP,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>20</td>\n",
       "      <td>[The, U.S., ,, claiming, some, success, in, it...</td>\n",
       "      <td>[DT, NNP, ,, VBG, DT, NN, IN, PRP$, NN, NN, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>14</td>\n",
       "      <td>[Norman, Ricken, ,, 52, years, old, and, forme...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, CC, NN,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>45</td>\n",
       "      <td>[Since, chalk, first, touched, slate, ,, schoo...</td>\n",
       "      <td>[IN, NN, RB, VBD, NN, ,, NN, VBP, VBN, TO, VB,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    document_id                                               text  \\\n",
       "0            95  [In, reference, to, your, Oct., 9, page-one, a...   \n",
       "1            37  [Judging, from, the, Americana, in, Haruki, Mu...   \n",
       "2            66  [Sir, Peter, Walters, ,, 58-year-old, chairman...   \n",
       "3            52  [PAPERS, :, Backe, Group, Inc., agreed, to, ac...   \n",
       "4             3  [A, form, of, asbestos, once, used, to, make, ...   \n",
       "..          ...                                                ...   \n",
       "95           82  [Criticism, in, the, U.S., over, recent, Japan...   \n",
       "96           71  [When, Warren, Winiarski, ,, proprietor, of, S...   \n",
       "97           20  [The, U.S., ,, claiming, some, success, in, it...   \n",
       "98           14  [Norman, Ricken, ,, 52, years, old, and, forme...   \n",
       "99           45  [Since, chalk, first, touched, slate, ,, schoo...   \n",
       "\n",
       "                                                  POS  \n",
       "0   [IN, NN, TO, PRP$, NNP, CD, NN, NN, ``, NNP, N...  \n",
       "1   [VBG, IN, DT, NNS, IN, NNP, NNP, POS, ``, DT, ...  \n",
       "2   [NNP, NNP, NNP, ,, JJ, NN, IN, NNP, NNP, NNP, ...  \n",
       "3   [NNS, :, NNP, NNP, NNP, VBD, TO, VB, NNP, NNP,...  \n",
       "4   [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...  \n",
       "..                                                ...  \n",
       "95  [NNP, IN, DT, NNP, IN, JJ, JJ, NNS, VBZ, VBG, ...  \n",
       "96  [WRB, NNP, NNP, ,, NN, IN, NNP, POS, NNP, NNP,...  \n",
       "97  [DT, NNP, ,, VBG, DT, NN, IN, PRP$, NN, NN, ,,...  \n",
       "98  [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, CC, NN,...  \n",
       "99  [IN, NN, RB, VBD, NN, ,, NN, VBP, VBN, TO, VB,...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = create_dataframe(train_path)\n",
    "df_val = create_dataframe(val_path)\n",
    "df_test = create_dataframe(test_path)\n",
    "\n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed the words using GloVe embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    # Check download\n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Word2Vec: 300\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 50\n",
    "\n",
    "embedding_model = load_embedding_model(embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list(data_frame):\n",
    "    big_list = []\n",
    "    text = data_frame[\"text\"]\n",
    "    for row in data_frame[\"text\"]:\n",
    "        big_list += row.tolist()\n",
    "\n",
    "    word_list = set(big_list)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_train = get_word_list(df_train)\n",
    "word_list_val = get_word_list(df_val)\n",
    "word_list_test = get_word_list(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition\n",
    "\n",
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                    word_list):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_listing: dataset specific vocabulary (list)\n",
    "\n",
    "    :return\n",
    "        - list of OOV terms\n",
    "    \"\"\"\n",
    "    embedding_vocabulary = set(embedding_model.index_to_key) # Was previously: set(embedding_model.vocab.keys())\n",
    "    print(len(embedding_vocabulary))\n",
    "    oov = set(word_list).difference(embedding_vocabulary)\n",
    "    return list(oov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "400000\n",
      "400000\n",
      "Total OOV terms: 2346 (0.29%)\n"
     ]
    }
   ],
   "source": [
    "oov1_terms = check_OOV_terms(embedding_model, word_list_train)\n",
    "#--------------------------OBS--------------------------------------------#\n",
    "# To make sure OOV2 does not contain words from OOV1 a check is \n",
    "# implemented during the embedding in the word to index function. \n",
    "# It might be an idea to include this check in the chekc_OOV_terms as well. \n",
    "# Question for the professor?\n",
    "#--------------------------OBS--------------------------------------------#\n",
    "oov2_terms = check_OOV_terms(embedding_model, word_list_val)\n",
    "oov3_terms = check_OOV_terms(embedding_model, word_list_test)\n",
    "\n",
    "\n",
    "print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_terms), float(len(oov_terms)) / len(word_list_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word_to_index (for one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def create_word_to_idx(vocabulary):\n",
    "    word_to_idx = OrderedDict()\n",
    "    idx_to_word = OrderedDict()\n",
    "    \n",
    "    current_idx = 0\n",
    "    for word in vocabulary:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = current_idx\n",
    "            idx_to_word[current_idx] = word\n",
    "            current_idx += 1\n",
    "            \n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "def update_word_to_idx(old_idx_to_word, old_word_to_idx, new_words):\n",
    "    word_to_idx = old_word_to_idx.copy()\n",
    "    idx_to_word = old_idx_to_word.copy()\n",
    "    current_idx = len(word_to_idx)\n",
    "    for word in new_words:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = current_idx\n",
    "            idx_to_word[current_idx] = word\n",
    "            current_idx += 1\n",
    "            \n",
    "    return word_to_idx, idx_to_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'----------------------OBS------------------------------------\\nShould we also compute word to index for the test set and embed\\nthese words, and then use all of them for training and validation\\nor should we keep the test set seperate and only compute this for\\ntesting?\\n----------------------OBS------------------------------------'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx_v1, idx_to_word_v1 = create_word_to_idx(set(embedding_model.index_to_key))\n",
    "word_to_idx_v2, idx_to_word_v2 = update_word_to_idx(idx_to_word_v1, word_to_idx_v1, oov1_terms)\n",
    "word_to_idx_v3, idx_to_word_v3 = update_word_to_idx(idx_to_word_v2, word_to_idx_v2, oov2_terms)\n",
    "\n",
    "\"\"\"----------------------OBS------------------------------------\n",
    "Should we also compute word to index for the test set and embed\n",
    "these words, and then use all of them for training and validation\n",
    "or should we keep the test set seperate and only compute this for\n",
    "testing?\n",
    "----------------------OBS------------------------------------\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_idx_word_list(idx_to_word,word_to_idx, write_path):\n",
    "    if not os.path.exists(os.path.dirname(write_path)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(write_path))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    with open(write_path, 'wb') as f:\n",
    "        pickle.dump(obj={\"word_to_idx\": words_lexicon, \"idx_to_word\":idx_to_word} , file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBS Computes the OOV with random embeddings\n",
    "def create_embedding_matrix(embedding_model, embedding_dimension, word_to_idx):\n",
    "    embedding_matrix = np.zeros((len(word_to_idx), embedding_dimension), dtype=np.float32)\n",
    "    for word, idx in word_to_idx.items():\n",
    "        try:\n",
    "            embedding_vector = embedding_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "                                \n",
    "    return embedding_matrix\n",
    "\n",
    "def expand_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, old_embedding_matrix):\n",
    "    embedding_matrix = np.zeros((len(word_to_idx), embedding_dimension), dtype=np.float32)\n",
    "    embedding_matrix[0:len(old_embedding_matrix)] = old_embedding_matrix\n",
    "    \n",
    "    for word, idx in word_to_idx.items():\n",
    "        if idx >= len(old_embedding_matrix):\n",
    "            try:\n",
    "                embedding_vector = embedding_model[word]\n",
    "            except (KeyError, TypeError):\n",
    "                embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "                \n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 50)\n",
      "(402346, 50)\n",
      "(403290, 50)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_v1 = create_embedding_matrix(embedding_model, embedding_dimension, word_to_idx_v1)\n",
    "embedding_matrix_v2 = expand_embedding_matrix(embedding_model, embedding_dimension, word_to_idx_v2, embedding_matrix_v1)\n",
    "embedding_matrix_v3 = expand_embedding_matrix(embedding_model, embedding_dimension, word_to_idx_v3, embedding_matrix_v2)\n",
    "\n",
    "print(embedding_matrix_v1.shape)\n",
    "print(embedding_matrix_v2.shape)\n",
    "print(embedding_matrix_v3.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test (Remove later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Hello', 0), ('Solveig', 1), ('is', 2), ('very', 3), ('wierd', 4), ('.', 5)])\n",
      "[[ 0.02183098  0.02402802  0.03118286  0.04769417  0.01872175]\n",
      " [-0.01728713  0.02662692  0.02155588 -0.00091611 -0.01262367]\n",
      " [-0.00816416  0.03744153 -0.00933892  0.03142535 -0.04727874]\n",
      " [ 0.01423092  0.00143279 -0.02720169 -0.00628638  0.03455986]\n",
      " [-0.02372587  0.01482276 -0.0354635   0.04404848 -0.01387006]\n",
      " [ 0.03148971  0.03084763 -0.00861655  0.00823898 -0.00565485]]\n",
      "OrderedDict([('Hello', 0), ('Solveig', 1), ('is', 2), ('very', 3), ('wierd', 4), ('.', 5), ('Andrea', 6), ('super', 7), ('cool', 8)])\n",
      "[[ 0.02183098  0.02402802  0.03118286  0.04769417  0.01872175]\n",
      " [-0.01728713  0.02662692  0.02155588 -0.00091611 -0.01262367]\n",
      " [-0.00816416  0.03744153 -0.00933892  0.03142535 -0.04727874]\n",
      " [ 0.01423092  0.00143279 -0.02720169 -0.00628638  0.03455986]\n",
      " [-0.02372587  0.01482276 -0.0354635   0.04404848 -0.01387006]\n",
      " [ 0.03148971  0.03084763 -0.00861655  0.00823898 -0.00565485]\n",
      " [-0.02011299  0.04589678  0.00622369  0.04714626 -0.04880853]\n",
      " [ 0.02720829 -0.00609134 -0.02245523  0.02279024 -0.039202  ]\n",
      " [-0.00789721  0.02382589  0.04431659 -0.04894823 -0.03839992]]\n"
     ]
    }
   ],
   "source": [
    "vocab = ['Hello', 'Solveig', 'is', 'very', 'wierd', '.']\n",
    "\n",
    "w_t_idx = create_word_to_idx(vocab)\n",
    "print(w_t_idx)\n",
    "\n",
    "embed_matrix = create_embedding_matrix(None, 5, w_t_idx)\n",
    "print(embed_matrix)\n",
    "\n",
    "new_words = ['Andrea', 'is', 'super', 'cool', '.']\n",
    "new_w_t_idx = update_word_to_idx(w_t_idx, new_words)\n",
    "print(new_w_t_idx)\n",
    "\n",
    "new_embed_matrix = expand_embedding_matrix(None, 5, new_w_t_idx, embed_matrix)\n",
    "print(new_embed_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform documents to sequences of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_integers(sequence, word_to_idx):\n",
    "    integers = []\n",
    "    for element in sequence:\n",
    "        integers.append(word_to_idx[element])\n",
    "    return np.array(integers)\n",
    "\"\"\"OBS\n",
    "Takes only one list of words to integer, not the list with lists.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create baseline model\n",
    "\n",
    "##### two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def create_model(layers_info: List[Dict], compile_info: Dict) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Create a Keras model given a list of layer information\n",
    "\n",
    "    :param layers_info: a list of dictionaries, one for each layer\n",
    "    :param compile_info: dictionary containing compile information\n",
    "\n",
    "    :return\n",
    "        model: the built keras sequential model\n",
    "    \"\"\"\n",
    "\n",
    "    print('Found {} total layers'.format(len(layers_info)))\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    for info_idx, info in enumerate(layers_info):\n",
    "\n",
    "        # Make sure the last layer has softmax activation and has 2 units\n",
    "        # for correct learning.\n",
    "        if info_idx == len(layers_info) - 1:\n",
    "            assert info['activation'] == 'softmax'\n",
    "            assert info['units'] == 2\n",
    "\n",
    "        layer = info['layer'](**{key: value for key, value in info.items() if key != 'layer'})\n",
    "        model.add(layer)\n",
    "\n",
    "    # Debug\n",
    "    model.summary()\n",
    "\n",
    "    # Compile\n",
    "    model.compile(**compile_info)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_layers_info = [\n",
    "    {\n",
    "        \"layer\": layers.Embedding,\n",
    "        \"output_dim\": embedding_dimension,\n",
    "        \"input_dim\": len(word_to_index),\n",
    "        \"weights\": embedding_matrix,\n",
    "        \"name\": \"embedding_layer\"\n",
    "    },\n",
    "    {\n",
    "        \"layer\": layers.LSTM,\n",
    "        \"units\": 3\n",
    "    },\n",
    "    {\n",
    "        \"layer\": layers.Dense,\n",
    "        \"units\": 3\n",
    "    }\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
