{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from collections import OrderedDict\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import keras\n",
    "from keras.models import Model \n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "def download_data(data_path):\n",
    "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
    "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
    "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    if not os.path.exists(toy_data_path):\n",
    "        print(\"Downloading FEVER data splits...\")\n",
    "        with requests.Session() as current_session:\n",
    "            response = current_session.get(toy_url,\n",
    "                                   params={'id': toy_data_url_id},\n",
    "                                   stream=True)\n",
    "        save_response_content(response, toy_data_path)\n",
    "        print(\"Download completed!\")\n",
    "\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
    "            loaded_zip.extractall(data_path)\n",
    "        print(\"Extraction completed!\")\n",
    "\n",
    "# Uncomment if you neewd to dowload the dataset\n",
    "#download_data('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>claimID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anxiety has been linked with physical symptoms.</td>\n",
       "      <td>13\\tFurthermore , anxiety has been linked with...</td>\n",
       "      <td>16387</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Firefox is an application.</td>\n",
       "      <td>0\\tMozilla Firefox -LRB- or simply Firefox -RR...</td>\n",
       "      <td>6</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Keegan-Michael Key played President Barack Oba...</td>\n",
       "      <td>6\\tIn 2015 , Key appeared at the White House C...</td>\n",
       "      <td>16392</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google Search can find stock quotes.</td>\n",
       "      <td>13\\tThese include synonyms , weather forecasts...</td>\n",
       "      <td>16394</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Good Day to Die Hard was directed solely by ...</td>\n",
       "      <td>1\\tThe film was directed by John Moore and wri...</td>\n",
       "      <td>98315</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7184</th>\n",
       "      <td>Scandal is an American band.</td>\n",
       "      <td>0\\tScandal is an American rock band from the 1...</td>\n",
       "      <td>16378</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7185</th>\n",
       "      <td>Henry Cavill played Superman.</td>\n",
       "      <td>8\\tCavill gained further prominence and intern...</td>\n",
       "      <td>143046</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7186</th>\n",
       "      <td>The Africa Cup of Nations is a friendly global...</td>\n",
       "      <td>0\\tThe Africa Cup of Nations , officially CAN ...</td>\n",
       "      <td>16382</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7187</th>\n",
       "      <td>Ron Dennis is the owner of a catering company ...</td>\n",
       "      <td>0\\tAbsolute Taste , is a London-based catering...</td>\n",
       "      <td>147455</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7188</th>\n",
       "      <td>Ron Dennis is the owner of a catering company ...</td>\n",
       "      <td>10\\tAfter being handed a three-page business p...</td>\n",
       "      <td>147455</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7189 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Claim  \\\n",
       "0       Anxiety has been linked with physical symptoms.   \n",
       "1                            Firefox is an application.   \n",
       "2     Keegan-Michael Key played President Barack Oba...   \n",
       "3                  Google Search can find stock quotes.   \n",
       "4     A Good Day to Die Hard was directed solely by ...   \n",
       "...                                                 ...   \n",
       "7184                       Scandal is an American band.   \n",
       "7185                      Henry Cavill played Superman.   \n",
       "7186  The Africa Cup of Nations is a friendly global...   \n",
       "7187  Ron Dennis is the owner of a catering company ...   \n",
       "7188  Ron Dennis is the owner of a catering company ...   \n",
       "\n",
       "                                               Evidence  claimID     Label  \n",
       "0     13\\tFurthermore , anxiety has been linked with...    16387  SUPPORTS  \n",
       "1     0\\tMozilla Firefox -LRB- or simply Firefox -RR...        6  SUPPORTS  \n",
       "2     6\\tIn 2015 , Key appeared at the White House C...    16392  SUPPORTS  \n",
       "3     13\\tThese include synonyms , weather forecasts...    16394  SUPPORTS  \n",
       "4     1\\tThe film was directed by John Moore and wri...    98315   REFUTES  \n",
       "...                                                 ...      ...       ...  \n",
       "7184  0\\tScandal is an American rock band from the 1...    16378  SUPPORTS  \n",
       "7185  8\\tCavill gained further prominence and intern...   143046  SUPPORTS  \n",
       "7186  0\\tThe Africa Cup of Nations , officially CAN ...    16382   REFUTES  \n",
       "7187  0\\tAbsolute Taste , is a London-based catering...   147455  SUPPORTS  \n",
       "7188  10\\tAfter being handed a three-page business p...   147455  SUPPORTS  \n",
       "\n",
       "[7189 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dataframe for the training data \n",
    "train_df = pd.read_csv('./dataset/train_pairs.csv', skipinitialspace=True, usecols=[\"Claim\",\"Evidence\",\"ID\",\"Label\"]).rename(columns={\"ID\": \"claimID\"})\n",
    "train_df\n",
    "\n",
    "#create a dataframe for the validation data \n",
    "val_df = pd.read_csv('./dataset/val_pairs.csv', skipinitialspace=True, usecols=[\"Claim\",\"Evidence\",\"ID\",\"Label\"]).rename(columns={\"ID\": \"claimID\"})\n",
    "val_df\n",
    "\n",
    "#create a dataframe for the test data \n",
    "test_df = pd.read_csv('./dataset/test_pairs.csv', skipinitialspace=True, usecols=[\"Claim\",\"Evidence\",\"ID\",\"Label\"]).rename(columns={\"ID\": \"claimID\"})\n",
    "test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and pad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tokenizer will have an index 1 for OOV words. A lot of words in test and val will be 1.\n",
    "tokenizer = Tokenizer(oov_token=1)\n",
    "\n",
    "tokenizer.fit_on_texts(train_df[\"Claim\"])\n",
    "tokenizer.fit_on_texts(train_df[\"Evidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = np.max([len(text.split()) for text in train_df[\"Evidence\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToTensor(tokenizer, max_len, text):\n",
    "    seq = tokenizer.texts_to_sequences(text)\n",
    "    padded = pad_sequences(sequences=seq, maxlen=max_len)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_train = textToTensor(tokenizer, MAX_SEQ_LEN, train_df[\"Claim\"])\n",
    "evidence_train = textToTensor(tokenizer, MAX_SEQ_LEN, train_df[\"Evidence\"])\n",
    "\n",
    "claim_val = textToTensor(tokenizer, MAX_SEQ_LEN, val_df[\"Claim\"])\n",
    "evidence_val = textToTensor(tokenizer, MAX_SEQ_LEN, val_df[\"Evidence\"])\n",
    "\n",
    "claim_test = textToTensor(tokenizer, MAX_SEQ_LEN, test_df[\"Claim\"])\n",
    "evidence_test = textToTensor(tokenizer, MAX_SEQ_LEN, test_df[\"Evidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121740, 237)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claim_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35800"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCABULARY_LENGTH = len(tokenizer.word_index) + 1\n",
    "VOCABULARY_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([1 if x=='SUPPORTS' else 0 for x in train_df[\"Label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(batch_size, max_tokens, vocab_length, embedding_dimension):    \n",
    "    claims_input = Input(shape=(max_tokens, ), batch_size= batch_size)\n",
    "    evidence_input = Input(shape=(max_tokens, ), batch_size= batch_size)\n",
    "    \n",
    "    claims_embedding = Embedding(vocab_length, embedding_dimension)(claims_input)\n",
    "    evidence_embedding = Embedding(vocab_length, embedding_dimension)(evidence_input)\n",
    "    \n",
    "    claims_sentence_embedding = LSTM(embedding_dimension)(claims_embedding)\n",
    "    evidence_sentence_embedding = LSTM(embedding_dimension)(evidence_embedding)\n",
    "    \n",
    "    merged = Concatenate(axis=1)([claims_sentence_embedding, evidence_sentence_embedding])\n",
    "\n",
    "    out = (Dense(1, activation='sigmoid'))(merged)\n",
    "    \n",
    "    model = Model(inputs=[claims_input, evidence_input], outputs=[out])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(100, 237)]         0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(100, 237)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (100, 237, 50)       1790000     ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (100, 237, 50)       1790000     ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_6 (LSTM)                  (100, 50)            20200       ['embedding_6[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_7 (LSTM)                  (100, 50)            20200       ['embedding_7[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (100, 100)           0           ['lstm_6[0][0]',                 \n",
      "                                                                  'lstm_7[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (100, 1)             101         ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,620,501\n",
      "Trainable params: 3,620,501\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(100, MAX_SEQ_LEN, VOCABULARY_LENGTH, 50)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  32/1218 [..............................] - ETA: 5:59 - loss: 0.1281 - acc: 0.8353"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer='Adam',\n",
    "              metrics=['acc'])\n",
    "model.fit(x=[claim_train, evidence_train], y=y_train, batch_size=100, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
