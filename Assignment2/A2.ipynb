{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from collections import OrderedDict\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Model \n",
    "from keras.layers import Input\n",
    "from keras.layers import Concatenate, Lambda, LSTM, Reshape, Dense, Embedding\n",
    "from keras.layers import Add\n",
    "from keras.layers import Average\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "def download_data(data_path):\n",
    "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
    "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
    "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    if not os.path.exists(toy_data_path):\n",
    "        print(\"Downloading FEVER data splits...\")\n",
    "        with requests.Session() as current_session:\n",
    "            response = current_session.get(toy_url,\n",
    "                                   params={'id': toy_data_url_id},\n",
    "                                   stream=True)\n",
    "        save_response_content(response, toy_data_path)\n",
    "        print(\"Download completed!\")\n",
    "\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
    "            loaded_zip.extractall(data_path)\n",
    "        print(\"Extraction completed!\")\n",
    "\n",
    "# Uncomment if you neewd to dowload the dataset\n",
    "#download_data('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe for the training data \n",
    "#train_df = pd.read_csv('./dataset/train_pairs.csv', skipinitialspace=True, usecols=[\"Claim\",\"Evidence\",\"ID\",\"Label\"]).rename(columns={\"ID\": \"claimID\"})\n",
    "#train_df\n",
    "\n",
    "train_df = pd.read_csv('./dataset/train_pairs.csv', skipinitialspace=True, usecols=[\"Claim\",\"Evidence\",\"ID\",\"Label\"]).rename(columns={\"ID\": \"claimID\"})\n",
    "\"\"\"train_df[\"Evidence\"] = train_df[\"Evidence\"].str.split(pat = \"\\t\")\n",
    "train_df[\"evidenceID\"]=train_df[\"Evidence\"].str[0]\n",
    "train_df[\"Evidence\"]=train_df[\"Evidence\"].str[1]\n",
    "train_df[\"Label\"] = train_df[\"Label\"].replace({'SUPPORTS': 1, 'REFUTES': 0})\n",
    "\n",
    "train_df\"\"\"\n",
    "\n",
    "#create a dataframe for the validation data \n",
    "val_df = pd.read_csv('./dataset/val_pairs.csv', skipinitialspace=True, usecols=[\"Claim\",\"Evidence\",\"ID\",\"Label\"]).rename(columns={\"ID\": \"claimID\"})\n",
    "val_df\n",
    "\n",
    "#create a dataframe for the test data \n",
    "test_df = pd.read_csv('./dataset/test_pairs.csv', skipinitialspace=True, usecols=[\"Claim\",\"Evidence\",\"ID\",\"Label\"]).rename(columns={\"ID\": \"claimID\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_df['Label'])\n",
    "y_val = np.array(val_df['Label'])\n",
    "y_test = np.array(test_df['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and pad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tokenizer will have an index 1 for OOV words. A lot of words in test and val will be 1.\n",
    "tokenizer = Tokenizer(oov_token=1)\n",
    "\n",
    "tokenizer.fit_on_texts(train_df[\"Claim\"])\n",
    "tokenizer.fit_on_texts(train_df[\"Evidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = np.max([len(text.split()) for text in train_df[\"Evidence\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToTensor(tokenizer, max_len, text):\n",
    "    seq = tokenizer.texts_to_sequences(text)\n",
    "    padded = pad_sequences(sequences=seq, maxlen=max_len)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_train = textToTensor(tokenizer, MAX_SEQ_LEN, train_df[\"Claim\"])\n",
    "evidence_train = textToTensor(tokenizer, MAX_SEQ_LEN, train_df[\"Evidence\"])\n",
    "\n",
    "claim_val = textToTensor(tokenizer, MAX_SEQ_LEN, val_df[\"Claim\"])\n",
    "evidence_val = textToTensor(tokenizer, MAX_SEQ_LEN, val_df[\"Evidence\"])\n",
    "\n",
    "claim_test = textToTensor(tokenizer, MAX_SEQ_LEN, test_df[\"Claim\"])\n",
    "evidence_test = textToTensor(tokenizer, MAX_SEQ_LEN, test_df[\"Evidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121740, 237)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claim_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35800"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCABULARY_LENGTH = len(tokenizer.word_index) + 1\n",
    "VOCABULARY_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([1 if x=='SUPPORTS' else 0 for x in train_df[\"Label\"]])\n",
    "y_test = np.array([1 if x=='SUPPORTS' else 0 for x in test_df[\"Label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_tokens, vocab_length, embedding_dimension, sentence_embedding_type=1):    \n",
    "    claims_input = Input(shape=(max_tokens, ))\n",
    "    evidence_input = Input(shape=(max_tokens, ))\n",
    "    \n",
    "    claims_embedding = Embedding(vocab_length, embedding_dimension, name='WordEmbedding_claims')(claims_input)\n",
    "    evidence_embedding = Embedding(vocab_length, embedding_dimension, name='WordEmbedding_evidence')(evidence_input)\n",
    "\n",
    "    #-------------------------- Sentence embedding -----------------------------------------------\n",
    "\n",
    "    if sentence_embedding_type==1:\n",
    "        # Encode token sequences via a RNN and take the last state as the sentence embedding.\n",
    "        claims_sentence_embedding = LSTM(embedding_dimension, return_sequences=False, name='SentenceEmbedding_claims')(claims_embedding)\n",
    "        evidence_sentence_embedding = LSTM(embedding_dimension, return_sequences=False, name='SentenceEmbedding_evidence')(evidence_embedding)\n",
    "    elif sentence_embedding_type==2:\n",
    "        # Encode token sequences via a RNN and average all the output states.\n",
    "\n",
    "        # Put return_sequences True to get output from all hidden states\n",
    "        claims_sentence_embedding = LSTM(embedding_dimension, return_sequences=True, name='SentenceEmbedding_claims')(claims_embedding)\n",
    "        evidence_sentence_embedding = LSTM(embedding_dimension, return_sequences=True, name='SentenceEmbedding_evidence')(evidence_embedding)\n",
    "        \n",
    "        # Take average of outputs\n",
    "        claims_sentence_embedding = K.mean(claims_sentence_embedding, axis=1)\n",
    "        evidence_sentence_embedding = K.mean(evidence_sentence_embedding, axis=1)\n",
    "\n",
    "    elif sentence_embedding_type==3:\n",
    "        # Encode token sequences via a simple MLP layer.\n",
    "        num = embedding_dimension*max_tokens\n",
    "        # Reshape 3d vector to 2d\n",
    "        claims_sentence_embedding = Reshape((num,), input_shape=(max_tokens, embedding_dimension))(claims_embedding)\n",
    "        evidence_sentence_embedding = Reshape((num,), input_shape=(max_tokens, embedding_dimension))(evidence_embedding)\n",
    "\n",
    "        # Send into dense layer\n",
    "        claims_sentence_embedding = Dense(num, name='SentenceEmbedding_claims')(claims_sentence_embedding)\n",
    "        evidence_sentence_embedding = Dense(num, name='SentenceEmbedding_evidence')(evidence_sentence_embedding)\n",
    "   \n",
    "    elif sentence_embedding_type==4:\n",
    "        # Compute the sentence embedding as the mean of its token embeddings (bag of vectors).\n",
    "        claims_sentence_embedding = K.mean(claims_embedding, axis=1)\n",
    "        evidence_sentence_embedding = K.mean(evidence_embedding, axis=1)\n",
    "    else:\n",
    "        raise Exception(\"Sentence embedding type must be an integer between 1 and 4\")\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "\n",
    "    #-------------------------- Extension with cosine similarity -----------------------------------------------\n",
    "\n",
    "    # Calulate cosine similarity\n",
    "    # Returns list of cosine similarities for each evidence claim pair\n",
    "    cosine_loss = tf.keras.losses.CosineSimilarity(axis=-1, reduction=tf.keras.losses.Reduction.NONE)\n",
    "    cosine_similarity = cosine_loss(claims_sentence_embedding, evidence_sentence_embedding)\n",
    "\n",
    "    # Expand dimention to get tensor on shape (embedding_dimension, 1)\n",
    "    cosine_similarity = Lambda(lambda x: K.expand_dims(x, axis=1))(cosine_similarity)\n",
    "\n",
    "    # Concatenate tensors and get them on shape (batch_size, embedding_dim + 1)\n",
    "    claims_sentence_embedding = Concatenate(axis=-1)([claims_sentence_embedding, cosine_similarity])\n",
    "    evidence_sentence_embedding = Concatenate(axis=-1)([evidence_sentence_embedding, cosine_similarity])\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #------------------------------------- Merging --------------------------------------------------------------\n",
    "\n",
    "\n",
    "    #Concatenation\n",
    "    merged = Concatenate(axis=1)([claims_sentence_embedding, evidence_sentence_embedding])\n",
    "    \n",
    "    #Sum\n",
    "    #merged = Add([claims_sentence_embedding, evidence_sentence_embedding])\n",
    "    \n",
    "    #Mean\n",
    "    #merged = Average(axis=1)([claims_sentence_embedding, evidence_sentence_embedding])\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    out = (Dense(1, activation='sigmoid'))(merged)\n",
    "\n",
    "    model = Model(inputs=[claims_input, evidence_input], outputs=[out])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 12:09:20.283019: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 237)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 237)]        0           []                               \n",
      "                                                                                                  \n",
      " WordEmbedding_claims (Embeddin  (None, 237, 50)     1790000     ['input_1[0][0]']                \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " WordEmbedding_evidence (Embedd  (None, 237, 50)     1790000     ['input_2[0][0]']                \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " SentenceEmbedding_claims (LSTM  (None, 50)          20200       ['WordEmbedding_claims[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " SentenceEmbedding_evidence (LS  (None, 50)          20200       ['WordEmbedding_evidence[0][0]'] \n",
      " TM)                                                                                              \n",
      "                                                                                                  \n",
      " tf.convert_to_tensor_1 (TFOpLa  (None, 50)          0           ['SentenceEmbedding_claims[0][0]'\n",
      " mbda)                                                           ]                                \n",
      "                                                                                                  \n",
      " tf.convert_to_tensor (TFOpLamb  (None, 50)          0           ['SentenceEmbedding_evidence[0][0\n",
      " da)                                                             ]']                              \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize (TFOpLamb  (None, 50)          0           ['tf.convert_to_tensor_1[0][0]'] \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_1 (TFOpLa  (None, 50)          0           ['tf.convert_to_tensor[0][0]']   \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 50)           0           ['tf.math.l2_normalize[0][0]',   \n",
      "                                                                  'tf.math.l2_normalize_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum (TFOpLambda  (None,)             0           ['tf.math.multiply[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.negative (TFOpLambda)  (None,)              0           ['tf.math.reduce_sum[0][0]']     \n",
      "                                                                                                  \n",
      " tf.cast (TFOpLambda)           (None,)              0           ['tf.math.negative[0][0]']       \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None,)             0           ['tf.cast[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.cast_1 (TFOpLambda)         (None,)              0           ['tf.math.multiply_1[0][0]']     \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['tf.cast_1[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 51)           0           ['SentenceEmbedding_claims[0][0]'\n",
      "                                                                 , 'lambda[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 51)           0           ['SentenceEmbedding_evidence[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 102)          0           ['concatenate[0][0]',            \n",
      "                                                                  'concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            103         ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,620,503\n",
      "Trainable params: 3,620,503\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(MAX_SEQ_LEN, VOCABULARY_LENGTH, 50)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  19/1218 [..............................] - ETA: 5:24 - loss: 0.2188 - acc: 0.7205 - f1_m: 0.8322 - precision_m: 0.7385 - recall_m: 0.9664"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer='Adam',\n",
    "              metrics=['acc',f1_m, precision_m, recall_m])\n",
    "history = model.fit(x=[claim_train, evidence_train], y=y_train, batch_size=100, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model: keras.Model, x, predicting_info):\n",
    "    predictions = model.predict(x, **predicting_info)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23449048399925232\n",
      "0.7180414795875549\n",
      "0.7391195297241211\n",
      "0.6799384951591492\n",
      "0.8249300718307495\n"
     ]
    }
   ],
   "source": [
    "#Not sure if we actually need to predict to evaluate the model. \n",
    "#Can send in the matrics into compile, and evaluate will do the rest. (recall_m etc.)\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate([claim_test, evidence_test], y_test, verbose=0)\n",
    "print(loss)\n",
    "print(accuracy)\n",
    "\n",
    "print(f1_score)\n",
    "\n",
    "print(precision)\n",
    "\n",
    "print(recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-input classification evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(model_callback):\n",
    "    plt.plot(model_callback.history['acc'])\n",
    "    plt.plot(model_callback.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_loss(model_callback):\n",
    "    plt.plot(model_callback.history['loss'])\n",
    "    plt.plot(model_callback.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
