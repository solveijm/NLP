{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from collections import OrderedDict\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Model \n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Add\n",
    "from keras.layers import Average\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "def download_data(data_path):\n",
    "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
    "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
    "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    if not os.path.exists(toy_data_path):\n",
    "        print(\"Downloading FEVER data splits...\")\n",
    "        with requests.Session() as current_session:\n",
    "            response = current_session.get(toy_url,\n",
    "                                   params={'id': toy_data_url_id},\n",
    "                                   stream=True)\n",
    "        save_response_content(response, toy_data_path)\n",
    "        print(\"Download completed!\")\n",
    "\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
    "            loaded_zip.extractall(data_path)\n",
    "        print(\"Extraction completed!\")\n",
    "\n",
    "# Uncomment if you neewd to dowload the dataset\n",
    "#download_data('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>claimID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anxiety has been linked with physical symptoms.</td>\n",
       "      <td>13\\tFurthermore , anxiety has been linked with...</td>\n",
       "      <td>16387</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Firefox is an application.</td>\n",
       "      <td>0\\tMozilla Firefox -LRB- or simply Firefox -RR...</td>\n",
       "      <td>6</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Keegan-Michael Key played President Barack Oba...</td>\n",
       "      <td>6\\tIn 2015 , Key appeared at the White House C...</td>\n",
       "      <td>16392</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google Search can find stock quotes.</td>\n",
       "      <td>13\\tThese include synonyms , weather forecasts...</td>\n",
       "      <td>16394</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Good Day to Die Hard was directed solely by ...</td>\n",
       "      <td>1\\tThe film was directed by John Moore and wri...</td>\n",
       "      <td>98315</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7184</th>\n",
       "      <td>Scandal is an American band.</td>\n",
       "      <td>0\\tScandal is an American rock band from the 1...</td>\n",
       "      <td>16378</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7185</th>\n",
       "      <td>Henry Cavill played Superman.</td>\n",
       "      <td>8\\tCavill gained further prominence and intern...</td>\n",
       "      <td>143046</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7186</th>\n",
       "      <td>The Africa Cup of Nations is a friendly global...</td>\n",
       "      <td>0\\tThe Africa Cup of Nations , officially CAN ...</td>\n",
       "      <td>16382</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7187</th>\n",
       "      <td>Ron Dennis is the owner of a catering company ...</td>\n",
       "      <td>0\\tAbsolute Taste , is a London-based catering...</td>\n",
       "      <td>147455</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7188</th>\n",
       "      <td>Ron Dennis is the owner of a catering company ...</td>\n",
       "      <td>10\\tAfter being handed a three-page business p...</td>\n",
       "      <td>147455</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7189 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Claim  \\\n",
       "0       Anxiety has been linked with physical symptoms.   \n",
       "1                            Firefox is an application.   \n",
       "2     Keegan-Michael Key played President Barack Oba...   \n",
       "3                  Google Search can find stock quotes.   \n",
       "4     A Good Day to Die Hard was directed solely by ...   \n",
       "...                                                 ...   \n",
       "7184                       Scandal is an American band.   \n",
       "7185                      Henry Cavill played Superman.   \n",
       "7186  The Africa Cup of Nations is a friendly global...   \n",
       "7187  Ron Dennis is the owner of a catering company ...   \n",
       "7188  Ron Dennis is the owner of a catering company ...   \n",
       "\n",
       "                                               Evidence  claimID     Label  \n",
       "0     13\\tFurthermore , anxiety has been linked with...    16387  SUPPORTS  \n",
       "1     0\\tMozilla Firefox -LRB- or simply Firefox -RR...        6  SUPPORTS  \n",
       "2     6\\tIn 2015 , Key appeared at the White House C...    16392  SUPPORTS  \n",
       "3     13\\tThese include synonyms , weather forecasts...    16394  SUPPORTS  \n",
       "4     1\\tThe film was directed by John Moore and wri...    98315   REFUTES  \n",
       "...                                                 ...      ...       ...  \n",
       "7184  0\\tScandal is an American rock band from the 1...    16378  SUPPORTS  \n",
       "7185  8\\tCavill gained further prominence and intern...   143046  SUPPORTS  \n",
       "7186  0\\tThe Africa Cup of Nations , officially CAN ...    16382   REFUTES  \n",
       "7187  0\\tAbsolute Taste , is a London-based catering...   147455  SUPPORTS  \n",
       "7188  10\\tAfter being handed a three-page business p...   147455  SUPPORTS  \n",
       "\n",
       "[7189 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dataframe for the training data \n",
    "#train_df = pd.read_csv('./dataset/train_pairs.csv', skipinitialspace=True, usecols=[\"Claim\",\"Evidence\",\"ID\",\"Label\"]).rename(columns={\"ID\": \"claimID\"})\n",
    "#train_df\n",
    "\n",
    "train_df = pd.read_csv('./dataset/train_pairs.csv', skipinitialspace=True, usecols=[\"Claim\",\"Evidence\",\"ID\",\"Label\"]).rename(columns={\"ID\": \"claimID\"})\n",
    "train_df[\"Evidence\"] = train_df[\"Evidence\"].str.split(pat = \"\\t\")\n",
    "train_df[\"evidenceID\"]=train_df[\"Evidence\"].str[0]\n",
    "train_df[\"Evidence\"]=train_df[\"Evidence\"].str[1]\n",
    "train_df[\"Label\"] = train_df[\"Label\"].replace({'SUPPORTS': 1, 'REFUTES': 0})\n",
    "\n",
    "train_df\n",
    "\n",
    "#create a dataframe for the validation data \n",
    "val_df = pd.read_csv('./dataset/val_pairs.csv', skipinitialspace=True, usecols=[\"Claim\",\"Evidence\",\"ID\",\"Label\"]).rename(columns={\"ID\": \"claimID\"})\n",
    "val_df\n",
    "\n",
    "#create a dataframe for the test data \n",
    "test_df = pd.read_csv('./dataset/test_pairs.csv', skipinitialspace=True, usecols=[\"Claim\",\"Evidence\",\"ID\",\"Label\"]).rename(columns={\"ID\": \"claimID\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_df['Label'])\n",
    "y_val = np.array(val_df['Label'])\n",
    "y_test = np.array(test_df['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and pad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tokenizer will have an index 1 for OOV words. A lot of words in test and val will be 1.\n",
    "tokenizer = Tokenizer(oov_token=1)\n",
    "\n",
    "tokenizer.fit_on_texts(train_df[\"Claim\"])\n",
    "tokenizer.fit_on_texts(train_df[\"Evidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = np.max([len(text.split()) for text in train_df[\"Evidence\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToTensor(tokenizer, max_len, text):\n",
    "    seq = tokenizer.texts_to_sequences(text)\n",
    "    padded = pad_sequences(sequences=seq, maxlen=max_len)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_train = textToTensor(tokenizer, MAX_SEQ_LEN, train_df[\"Claim\"])\n",
    "evidence_train = textToTensor(tokenizer, MAX_SEQ_LEN, train_df[\"Evidence\"])\n",
    "\n",
    "claim_val = textToTensor(tokenizer, MAX_SEQ_LEN, val_df[\"Claim\"])\n",
    "evidence_val = textToTensor(tokenizer, MAX_SEQ_LEN, val_df[\"Evidence\"])\n",
    "\n",
    "claim_test = textToTensor(tokenizer, MAX_SEQ_LEN, test_df[\"Claim\"])\n",
    "evidence_test = textToTensor(tokenizer, MAX_SEQ_LEN, test_df[\"Evidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121740, 237)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claim_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35800"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCABULARY_LENGTH = len(tokenizer.word_index) + 1\n",
    "VOCABULARY_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([1 if x=='SUPPORTS' else 0 for x in train_df[\"Label\"]])\n",
    "y_test = np.array([1 if x=='SUPPORTS' else 0 for x in test_df[\"Label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_tokens, vocab_length, embedding_dimension, sentence_embedding_type=1):    \n",
    "    claims_input = Input(shape=(max_tokens, ))\n",
    "    evidence_input = Input(shape=(max_tokens, ))\n",
    "    \n",
    "    claims_embedding = Embedding(vocab_length, embedding_dimension, name='WordEmbedding_claims')(claims_input)\n",
    "    evidence_embedding = Embedding(vocab_length, embedding_dimension, name='WordEmbedding_evidence')(evidence_input)\n",
    "    if sentence_embedding_type==1:\n",
    "        # Encode token sequences via a RNN and take the last state as the sentence embedding.\n",
    "        claims_sentence_embedding = LSTM(embedding_dimension, return_sequences=False, name='SentenceEmbedding_claims')(claims_embedding)\n",
    "        evidence_sentence_embedding = LSTM(embedding_dimension, return_sequences=False, name='SentenceEmbedding_evidence')(evidence_embedding)\n",
    "    elif sentence_embedding_type==2:\n",
    "        # Encode token sequences via a RNN and average all the output states.\n",
    "\n",
    "        # Put return_sequences True to get output from all hidden states\n",
    "        claims_sentence_embedding = LSTM(embedding_dimension, return_sequences=True, name='SentenceEmbedding_claims')(claims_embedding)\n",
    "        evidence_sentence_embedding = LSTM(embedding_dimension, return_sequences=True, name='SentenceEmbedding_evidence')(evidence_embedding)\n",
    "        \n",
    "        # Take average of outputs\n",
    "        claims_sentence_embedding = K.mean(claims_sentence_embedding, axis=1)\n",
    "        evidence_sentence_embedding = K.mean(evidence_sentence_embedding, axis=1)\n",
    "\n",
    "    elif sentence_embedding_type==3:\n",
    "        # Encode token sequences via a simple MLP layer.\n",
    "        num = embedding_dimension*max_tokens\n",
    "        # Reshape 3d vector to 2d\n",
    "        claims_sentence_embedding = Reshape((num,), input_shape=(max_tokens, embedding_dimension))(claims_embedding)\n",
    "        evidence_sentence_embedding = Reshape((num,), input_shape=(max_tokens, embedding_dimension))(evidence_embedding)\n",
    "\n",
    "        # Send into dense layer\n",
    "        claims_sentence_embedding = Dense(num, name='SentenceEmbedding_claims')(claims_sentence_embedding)\n",
    "        evidence_sentence_embedding = Dense(num, name='SentenceEmbedding_evidence')(evidence_sentence_embedding)\n",
    "   \n",
    "    elif sentence_embedding_type==4:\n",
    "        # Compute the sentence embedding as the mean of its token embeddings (bag of vectors).\n",
    "        claims_sentence_embedding = K.mean(claims_embedding, axis=1)\n",
    "        evidence_sentence_embedding = K.mean(evidence_embedding, axis=1)\n",
    "    else:\n",
    "        raise Exception(\"Sentence embedding type must be an integer between 1 and 4\")\n",
    "    #Concatenation\n",
    "    merged = Concatenate(axis=1)([claims_sentence_embedding, evidence_sentence_embedding])\n",
    "    \n",
    "    #Sum\n",
    "    #merged = Add([claims_sentence_embedding, evidence_sentence_embedding])\n",
    "    \n",
    "    #Mean\n",
    "    #merged = Average(axis=1)([claims_sentence_embedding, evidence_sentence_embedding])\n",
    "\n",
    "    out = (Dense(1, activation='sigmoid'))(merged)\n",
    "\n",
    "    model = Model(inputs=[claims_input, evidence_input], outputs=[out])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 237)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 237)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 237, 50)      1790000     ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 237, 50)      1790000     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 50)           20200       ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 50)           20200       ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 100)          0           ['lstm_2[0][0]',                 \n",
      "                                                                  'lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            101         ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,620,501\n",
      "Trainable params: 3,620,501\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(MAX_SEQ_LEN, VOCABULARY_LENGTH, 50)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1218/1218 [==============================] - 358s 290ms/step - loss: 0.1320 - acc: 0.8234 - f1_m: 0.8891 - precision_m: 0.8255 - recall_m: 0.9664\n",
      "Epoch 2/10\n",
      "1218/1218 [==============================] - 3740s 3s/step - loss: 0.1004 - acc: 0.8656 - f1_m: 0.9120 - precision_m: 0.8744 - recall_m: 0.9544\n",
      "Epoch 3/10\n",
      "1218/1218 [==============================] - 415s 341ms/step - loss: 0.0863 - acc: 0.8862 - f1_m: 0.9246 - precision_m: 0.8950 - recall_m: 0.9574\n",
      "Epoch 4/10\n",
      "1218/1218 [==============================] - 455s 374ms/step - loss: 0.0768 - acc: 0.8994 - f1_m: 0.9329 - precision_m: 0.9082 - recall_m: 0.9601\n",
      "Epoch 5/10\n",
      "1218/1218 [==============================] - 404s 332ms/step - loss: 0.0693 - acc: 0.9107 - f1_m: 0.9403 - precision_m: 0.9179 - recall_m: 0.9647\n",
      "Epoch 6/10\n",
      "1218/1218 [==============================] - 406s 333ms/step - loss: 0.0628 - acc: 0.9201 - f1_m: 0.9464 - precision_m: 0.9269 - recall_m: 0.9674\n",
      "Epoch 7/10\n",
      "1218/1218 [==============================] - 489s 401ms/step - loss: 0.0568 - acc: 0.9282 - f1_m: 0.9517 - precision_m: 0.9347 - recall_m: 0.9699\n",
      "Epoch 8/10\n",
      "1218/1218 [==============================] - 398s 327ms/step - loss: 0.0515 - acc: 0.9364 - f1_m: 0.9571 - precision_m: 0.9422 - recall_m: 0.9731\n",
      "Epoch 9/10\n",
      "1218/1218 [==============================] - 412s 338ms/step - loss: 0.0471 - acc: 0.9422 - f1_m: 0.9610 - precision_m: 0.9479 - recall_m: 0.9749\n",
      "Epoch 10/10\n",
      "1218/1218 [==============================] - 404s 332ms/step - loss: 0.0427 - acc: 0.9484 - f1_m: 0.9651 - precision_m: 0.9537 - recall_m: 0.9772\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer='Adam',\n",
    "              metrics=['acc',f1_m, precision_m, recall_m])\n",
    "history = model.fit(x=[claim_train, evidence_train], y=y_train, batch_size=100, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model: keras.Model, x, predicting_info):\n",
    "    predictions = model.predict(x, **predicting_info)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23449048399925232\n",
      "0.7180414795875549\n",
      "0.7391195297241211\n",
      "0.6799384951591492\n",
      "0.8249300718307495\n"
     ]
    }
   ],
   "source": [
    "#Not sure if we actually need to predict to evaluate the model. \n",
    "#Can send in the matrics into compile, and evaluate will do the rest. (recall_m etc.)\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate([claim_test, evidence_test], y_test, verbose=0)\n",
    "print(loss)\n",
    "print(accuracy)\n",
    "\n",
    "print(f1_score)\n",
    "\n",
    "print(precision)\n",
    "\n",
    "print(recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-input classification evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(model_callback):\n",
    "    plt.plot(model_callback.history['acc'])\n",
    "    plt.plot(model_callback.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_loss(model_callback):\n",
    "    plt.plot(model_callback.history['loss'])\n",
    "    plt.plot(model_callback.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
