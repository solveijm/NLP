{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538e1dc1",
   "metadata": {},
   "source": [
    "## Questing Answering on SQuAD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75a0f04",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "60f4b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "import re\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from typing import List, Callable, Dict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from datetime import datetime\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Concatenate, Lambda, LSTM, Reshape, Dense, Embedding, Average, Reshape, Flatten, Input, Add, Bidirectional\n",
    "from keras.models import Model \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from keras.utils.np_utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281664e0",
   "metadata": {},
   "source": [
    "## Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac96bfb",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c7bf748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filename=\"training_set.json\", folder=\"SQUAD MATERIAL\"):    \n",
    "    dataset_folder = os.path.join(os.getcwd(), folder)\n",
    "    dataset_path = os.path.join(dataset_folder, filename)\n",
    "    with open(dataset_path) as f:\n",
    "        raw_json = json.load(f)\n",
    "\n",
    "    return raw_json['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8817a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b42106",
   "metadata": {},
   "source": [
    "#### Split dataset into train, val and test sets.\n",
    "Splitting on title, so that all answers and questions in one title are in the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "03796a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data):\n",
    "    random.shuffle(data)\n",
    "    length_of_dataset = len(data)\n",
    "    train_split = round(0.8*length_of_dataset)\n",
    "    val_split = train_split + round(0.1*length_of_dataset)\n",
    "    train_data = data[:train_split]\n",
    "    val_data = data[train_split:val_split]\n",
    "    test_data = data[val_split:]\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b9e36b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = split_dataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b38edc2",
   "metadata": {},
   "source": [
    "#### Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae626891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_index(char_idx, context):\n",
    "    return context[0:char_idx].count(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d350502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(data):\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers_text = []\n",
    "    answers_start = []\n",
    "    answers_end = []\n",
    "    question_ids = []\n",
    "    answers_word_start = []\n",
    "    answers_word_end = []\n",
    "    for i in range(len(data)):\n",
    "        paragraphs = data[i]['paragraphs']\n",
    "        for sub_para in paragraphs:\n",
    "            for q_a in sub_para['qas']:\n",
    "                questions.append(q_a['question'])\n",
    "                q_a_answer_starts = []\n",
    "                q_a_answer_ends = []\n",
    "                q_a_answers = []\n",
    "                q_a_ans_word_idx_start = []\n",
    "                q_a_ans_word_idx_end = []\n",
    "\n",
    "                for answer in q_a['answers']:\n",
    "                    answer_end = answer['answer_start'] + len(answer['text'])\n",
    "                    q_a_answer_starts.append(answer['answer_start'])\n",
    "                    q_a_answer_ends.append(answer_end)\n",
    "                    q_a_answers.append(answer['text'])\n",
    "                    q_a_ans_word_idx_start.append(find_word_index(answer['answer_start'], sub_para['context']))\n",
    "                    q_a_ans_word_idx_end.append(find_word_index(answer_end, sub_para['context']))\n",
    "                    \n",
    "                answers_start.append(q_a_answer_starts)\n",
    "                answers_end.append(q_a_answer_ends)\n",
    "                answers_word_start.append(q_a_ans_word_idx_start)                \n",
    "                answers_word_end.append(q_a_ans_word_idx_end)\n",
    "                answers_text.append(q_a_answers)\n",
    "                question_ids.append(q_a['id'])\n",
    "                contexts.append(sub_para['context'])   \n",
    "    df = pd.DataFrame({\"questionID\":question_ids, \"context\":contexts, \"question\": questions, \"answer_start\": answers_start, \"answer_word_start\": answers_word_start, \"answer_end\": answers_end, \"answer_word_end\": answers_word_end, \"answer_text\": answers_text})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cbf2971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_dataframe(train_data)\n",
    "val_df = create_dataframe(val_data)\n",
    "test_df = create_dataframe(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fd2d926d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questionID</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_word_start</th>\n",
       "      <th>answer_end</th>\n",
       "      <th>answer_word_end</th>\n",
       "      <th>answer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570c2b046b8089140040fba4</td>\n",
       "      <td>According to the apocryphal Gospel of James, M...</td>\n",
       "      <td>Who were Mary's parents?</td>\n",
       "      <td>[70]</td>\n",
       "      <td>[12]</td>\n",
       "      <td>[98]</td>\n",
       "      <td>[16]</td>\n",
       "      <td>[Saint Joachim and Saint Anne]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570c2b046b8089140040fba5</td>\n",
       "      <td>According to the apocryphal Gospel of James, M...</td>\n",
       "      <td>How old was Mary when she was consecrated as a...</td>\n",
       "      <td>[268]</td>\n",
       "      <td>[47]</td>\n",
       "      <td>[283]</td>\n",
       "      <td>[49]</td>\n",
       "      <td>[three years old]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570c2b046b8089140040fba6</td>\n",
       "      <td>According to the apocryphal Gospel of James, M...</td>\n",
       "      <td>Who was Mary betrothed to?</td>\n",
       "      <td>[434]</td>\n",
       "      <td>[76]</td>\n",
       "      <td>[440]</td>\n",
       "      <td>[76]</td>\n",
       "      <td>[Joseph]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570c2b046b8089140040fba7</td>\n",
       "      <td>According to the apocryphal Gospel of James, M...</td>\n",
       "      <td>When she was betrothed to Joseph, approximatel...</td>\n",
       "      <td>[451]</td>\n",
       "      <td>[79]</td>\n",
       "      <td>[466]</td>\n",
       "      <td>[81]</td>\n",
       "      <td>[12–14 years old]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570c2b046b8089140040fba8</td>\n",
       "      <td>According to the apocryphal Gospel of James, M...</td>\n",
       "      <td>Where was Mary consecrated as a virgin?</td>\n",
       "      <td>[231]</td>\n",
       "      <td>[40]</td>\n",
       "      <td>[254]</td>\n",
       "      <td>[43]</td>\n",
       "      <td>[the Temple in Jerusalem]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69372</th>\n",
       "      <td>572f5ad0b2c2fd1400568076</td>\n",
       "      <td>On 18 June 1948, the National Security Council...</td>\n",
       "      <td>Most CIA stations had how many chiefs?</td>\n",
       "      <td>[589]</td>\n",
       "      <td>[103]</td>\n",
       "      <td>[592]</td>\n",
       "      <td>[103]</td>\n",
       "      <td>[two]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69373</th>\n",
       "      <td>572f5ad0b2c2fd1400568073</td>\n",
       "      <td>On 18 June 1948, the National Security Council...</td>\n",
       "      <td>Directive 10/2 called for actions against who?</td>\n",
       "      <td>[103]</td>\n",
       "      <td>[16]</td>\n",
       "      <td>[111]</td>\n",
       "      <td>[17]</td>\n",
       "      <td>[the USSR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69374</th>\n",
       "      <td>572f5c68a23a5019007fc5a1</td>\n",
       "      <td>The early track record of the CIA was poor, wi...</td>\n",
       "      <td>How many troops did the Chinese enter into the...</td>\n",
       "      <td>[314]</td>\n",
       "      <td>[51]</td>\n",
       "      <td>[321]</td>\n",
       "      <td>[51]</td>\n",
       "      <td>[300,000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69375</th>\n",
       "      <td>572f5c68a23a5019007fc5a2</td>\n",
       "      <td>The early track record of the CIA was poor, wi...</td>\n",
       "      <td>Who compromised hundreds of airdrops?</td>\n",
       "      <td>[354]</td>\n",
       "      <td>[57]</td>\n",
       "      <td>[364]</td>\n",
       "      <td>[58]</td>\n",
       "      <td>[Kim Philby]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69376</th>\n",
       "      <td>572f5c68a23a5019007fc5a3</td>\n",
       "      <td>The early track record of the CIA was poor, wi...</td>\n",
       "      <td>Who was a Russian translator and Soviet Spy?</td>\n",
       "      <td>[602]</td>\n",
       "      <td>[94]</td>\n",
       "      <td>[615]</td>\n",
       "      <td>[95]</td>\n",
       "      <td>[Bill Weisband]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69377 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     questionID  \\\n",
       "0      570c2b046b8089140040fba4   \n",
       "1      570c2b046b8089140040fba5   \n",
       "2      570c2b046b8089140040fba6   \n",
       "3      570c2b046b8089140040fba7   \n",
       "4      570c2b046b8089140040fba8   \n",
       "...                         ...   \n",
       "69372  572f5ad0b2c2fd1400568076   \n",
       "69373  572f5ad0b2c2fd1400568073   \n",
       "69374  572f5c68a23a5019007fc5a1   \n",
       "69375  572f5c68a23a5019007fc5a2   \n",
       "69376  572f5c68a23a5019007fc5a3   \n",
       "\n",
       "                                                 context  \\\n",
       "0      According to the apocryphal Gospel of James, M...   \n",
       "1      According to the apocryphal Gospel of James, M...   \n",
       "2      According to the apocryphal Gospel of James, M...   \n",
       "3      According to the apocryphal Gospel of James, M...   \n",
       "4      According to the apocryphal Gospel of James, M...   \n",
       "...                                                  ...   \n",
       "69372  On 18 June 1948, the National Security Council...   \n",
       "69373  On 18 June 1948, the National Security Council...   \n",
       "69374  The early track record of the CIA was poor, wi...   \n",
       "69375  The early track record of the CIA was poor, wi...   \n",
       "69376  The early track record of the CIA was poor, wi...   \n",
       "\n",
       "                                                question answer_start  \\\n",
       "0                               Who were Mary's parents?         [70]   \n",
       "1      How old was Mary when she was consecrated as a...        [268]   \n",
       "2                             Who was Mary betrothed to?        [434]   \n",
       "3      When she was betrothed to Joseph, approximatel...        [451]   \n",
       "4                Where was Mary consecrated as a virgin?        [231]   \n",
       "...                                                  ...          ...   \n",
       "69372             Most CIA stations had how many chiefs?        [589]   \n",
       "69373     Directive 10/2 called for actions against who?        [103]   \n",
       "69374  How many troops did the Chinese enter into the...        [314]   \n",
       "69375              Who compromised hundreds of airdrops?        [354]   \n",
       "69376       Who was a Russian translator and Soviet Spy?        [602]   \n",
       "\n",
       "      answer_word_start answer_end answer_word_end  \\\n",
       "0                  [12]       [98]            [16]   \n",
       "1                  [47]      [283]            [49]   \n",
       "2                  [76]      [440]            [76]   \n",
       "3                  [79]      [466]            [81]   \n",
       "4                  [40]      [254]            [43]   \n",
       "...                 ...        ...             ...   \n",
       "69372             [103]      [592]           [103]   \n",
       "69373              [16]      [111]            [17]   \n",
       "69374              [51]      [321]            [51]   \n",
       "69375              [57]      [364]            [58]   \n",
       "69376              [94]      [615]            [95]   \n",
       "\n",
       "                          answer_text  \n",
       "0      [Saint Joachim and Saint Anne]  \n",
       "1                   [three years old]  \n",
       "2                            [Joseph]  \n",
       "3                   [12–14 years old]  \n",
       "4           [the Temple in Jerusalem]  \n",
       "...                               ...  \n",
       "69372                           [two]  \n",
       "69373                      [the USSR]  \n",
       "69374                       [300,000]  \n",
       "69375                    [Kim Philby]  \n",
       "69376                 [Bill Weisband]  \n",
       "\n",
       "[69377 rows x 8 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7bcbc5",
   "metadata": {},
   "source": [
    "## Clean and transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21fd72d",
   "metadata": {},
   "source": [
    "#### Clean text\n",
    "What should we do? just lowering everyhting? remove stopwords? how will that work with the answer start number???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eae2f69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/andreastettejessen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "try:\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "def lower(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Transforms given text to lower case.\n",
    "    Example:\n",
    "    Input: 'I really like New York city'\n",
    "    Output: 'i really like new your city'\n",
    "    \"\"\"\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "def replace_special_characters(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replaces special characters, such as paranthesis,\n",
    "    with spacing character\n",
    "    \"\"\"\n",
    "\n",
    "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "\n",
    "def filter_out_uncommon_symbols(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes any special character that is not in the\n",
    "    good symbols list (check regular expression)\n",
    "    \"\"\"\n",
    "\n",
    "    return GOOD_SYMBOLS_RE.sub('', text)\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
    "\n",
    "def strip_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes any left or right spacing (including carriage return) from text.\n",
    "    Example:\n",
    "    Input: '  This assignment is cool\\n'\n",
    "    Output: 'This assignment is cool'\n",
    "    \"\"\"\n",
    "\n",
    "    return text.strip()    \n",
    "\n",
    "def lemmatize_words(text: str ) -> str:\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "PREPROCESSING_PIPELINE = [\n",
    "                          lower,\n",
    "                          strip_text\n",
    "                          ]\n",
    "\n",
    "def text_prepare(text: str,\n",
    "                 filter_methods: List[Callable[[str], str]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Applies a list of pre-processing functions in sequence (reduce).\n",
    "    Note that the order is important here!\n",
    "    \"\"\"\n",
    "\n",
    "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
    "    if type(text) == list:\n",
    "        new_row = [reduce(lambda txt, f: f(txt), filter_methods, x) for x in text]\n",
    "    else:\n",
    "        new_row = reduce(lambda txt, f: f(txt), filter_methods, text)\n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f8f8fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_cleaned = [\"context\", \"question\", \"answer_text\"]\n",
    "for key in to_be_cleaned:\n",
    "    train_df[key] = train_df[key].apply(lambda txt: text_prepare(txt))\n",
    "    val_df[key] = val_df[key].apply(lambda txt: text_prepare(txt))\n",
    "    test_df[key] = test_df[key].apply(lambda txt: text_prepare(txt))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd709782",
   "metadata": {},
   "source": [
    "#### Make tokenixer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7806c773",
   "metadata": {},
   "source": [
    "### THINGS TO THINK ABOUT\n",
    "- Now its the padding is exstream! The questions has to be 3706 caracters long!\n",
    "- We are only fitting on text the train context and questions. Should this also be done for val/train?\n",
    "- Preprocessing is only lowering the words. Should we do more, like removing stopwords? In that case we need to consider the answer_start index. This has to be corrected after removal of carachters\n",
    "- OOV are handeled with index 1 and will all have weights 0 in the beginning. is this correct?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bbb6c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tokenizer will have an index 1 for OOV words. A lot of words in test and val will be 1.\n",
    "tokenizer = Tokenizer(oov_token=1)\n",
    "\n",
    "tokenizer.fit_on_texts(train_df[\"context\"])\n",
    "tokenizer.fit_on_texts(train_df[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6bc851ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find max sentence lenght for the context\n",
    "MAX_SEQ_LEN = np.max([len(row.split(' ')) for row in train_df[\"context\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d428503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "653"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "175d5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToTensor(tokenizer, max_len, text):\n",
    "    '''\n",
    "        Converts text to tensors by converting the words into the correct indexes. \n",
    "        Then padds the tensors with 0 vlaues\n",
    "    '''\n",
    "    seq = tokenizer.texts_to_sequences(text)\n",
    "    padded = pad_sequences(sequences=seq, maxlen=max_len)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "abf2d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_train = textToTensor(tokenizer, MAX_SEQ_LEN, train_df[\"context\"])\n",
    "question_train = textToTensor(tokenizer, MAX_SEQ_LEN, train_df[\"question\"])\n",
    "\n",
    "context_val = textToTensor(tokenizer, MAX_SEQ_LEN, val_df[\"context\"])\n",
    "question_val = textToTensor(tokenizer, MAX_SEQ_LEN, val_df[\"question\"])\n",
    "\n",
    "context_test = textToTensor(tokenizer, MAX_SEQ_LEN, test_df[\"context\"])\n",
    "question_test = textToTensor(tokenizer, MAX_SEQ_LEN, test_df[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "002a7459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes only the first answer: simplification\n",
    "# Can do this because we know there are only one answer for each question in our dataset\n",
    "# Might need to change this\n",
    "index_start_train = to_categorical(np.array(train_df[\"answer_word_start\"].str[0]), num_classes=MAX_SEQ_LEN)\n",
    "index_end_train = to_categorical(np.array(train_df[\"answer_word_end\"].str[0]), MAX_SEQ_LEN)\n",
    "\n",
    "index_start_val = to_categorical(np.array(train_df[\"answer_word_start\"].str[0]), MAX_SEQ_LEN)\n",
    "index_end_val = to_categorical(np.array(train_df[\"answer_word_end\"].str[0]), MAX_SEQ_LEN)\n",
    "\n",
    "index_start_test = np.array(train_df[\"answer_word_start\"].str[0])\n",
    "index_end_test = np.array(train_df[\"answer_word_end\"].str[0])\n",
    "\n",
    "index_start_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "97970acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find size of vocabulary\n",
    "VOCABULARY_SIZE = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14113eb",
   "metadata": {},
   "source": [
    "### Applying glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ade4fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    # Check download\n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Word2Vec: 300\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model\n",
    "    \n",
    "def create_embedding_matrix(embedding_model, embedding_dimension, word_to_idx):\n",
    "    embedding_matrix = np.zeros((len(word_to_idx)+1, embedding_dimension), dtype=np.float32)\n",
    "    for word, idx in word_to_idx.items():\n",
    "        try:\n",
    "            embedding_vector = embedding_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "                                \n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c6a3717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80937, 50)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dimension = 50\n",
    "\n",
    "embedding_model = load_embedding_model(embedding_dimension)\n",
    "embedding_matrix = create_embedding_matrix(embedding_model, embedding_dimension, tokenizer.word_index)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d1d9ba",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "69cb3a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_tokens, vocab_size, embedding_dimension):\n",
    "    '''\n",
    "        Creates keras model for classification.\n",
    "        Inputs: \n",
    "            max_tokens (int): Max length of a text sequence\n",
    "            vocab_size (int): Size of the vocabulary\n",
    "            embedding_dimension (int): The dimension of the embedding vectors\n",
    "    '''   \n",
    "\n",
    "    #-------------------------- Input layer ------------------------------------------------------------\n",
    "    question_input = Input(shape=(max_tokens, ))\n",
    "    context_input = Input(shape=(max_tokens, ))\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #-------------------------- Word embedding ------------------------------------------------------------\n",
    "    question_embedding = Embedding(vocab_size, embedding_dimension, weights = [embedding_matrix], name='WordEmbedding_question', trainable = False, mask_zero = True)(question_input)\n",
    "    context_embedding = Embedding(vocab_size, embedding_dimension, weights = [embedding_matrix], name='WordEmbedding_context', trainable = False, mask_zero = True)(context_input)\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #-------------------------- Encoding/sentence embedding -------------------------------------------------------\n",
    "    # Encode token sequences with bi-directional LSTM and concatenate the series of hidden vectors (done by default)\n",
    "    question_encoding = Bidirectional(LSTM(embedding_dimension, return_sequences=True, name='SentenceEmbedding_claims'))(question_embedding)\n",
    "    context_encoding = Bidirectional(LSTM(embedding_dimension, return_sequences=True, name='SentenceEmbedding_evidence'))(context_embedding)\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #-------------------------- Attention ------------------------------------------------------------\n",
    "    # Tells us which words to focus on\n",
    "    qst_cont_attention = tf.keras.layers.Attention()([question_encoding, context_encoding])\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #-------------------------- Concatinate attention and context ------------------------------------------\n",
    "    blended_reps = Concatenate(axis=2)([context_encoding, qst_cont_attention])\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Get back to correct dim\n",
    "    final_blended_reps = Dense(embedding_dimension)(blended_reps)\n",
    "    \n",
    "    #-------------------------- Softmax layer for start index ------------------------------------------\n",
    "    with tf.name_scope(\"StartDistribution\"):\n",
    "        logits_start = Dense(1, activation=None)(final_blended_reps) # shape (batch_size, seq_len, 1)\n",
    "        logits_start = tf.squeeze(logits_start, axis=[2]) # shape (batch_size, seq_len)\n",
    "        logits_start = Dense(max_tokens, activation='Softmax')(logits_start)\n",
    "\n",
    "\n",
    "        # Take softmax over sequence\n",
    "        #masked_logits, prob_dist = masked_softmax(logits, masks, 1)\n",
    "    #----------------------------------------------------------------------------------------------------- \n",
    "    \n",
    "    #-------------------------- Softmax layer for end index ------------------------------------------\n",
    "    with tf.name_scope(\"EndDistribution\"):\n",
    "        logits_end = Dense(1, activation=None)(final_blended_reps) # shape (batch_size, seq_len, 1)\n",
    "        logits_end = tf.squeeze(logits_end, axis=[2]) # shape (batch_size, seq_len)\n",
    "        logits_end = Dense(max_tokens, activation='Softmax')(logits_end) \n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------- \n",
    "    \n",
    "    #-------------------------- Dense - softmax -------------------------------------------------------------\n",
    "    # create probabilty of p_start vector and probability of p_end vector\n",
    "    #out = (Dense(1, activation='softmax'))(blended_reps)\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    return Model(inputs=[question_input, context_input], outputs=[logits_start, logits_end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7c1467a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_28 (InputLayer)          [(None, 653)]        0           []                               \n",
      "                                                                                                  \n",
      " input_27 (InputLayer)          [(None, 653)]        0           []                               \n",
      "                                                                                                  \n",
      " WordEmbedding_context (Embeddi  (None, 653, 50)     4046850     ['input_28[0][0]']               \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " WordEmbedding_question (Embedd  (None, 653, 50)     4046850     ['input_27[0][0]']               \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " bidirectional_27 (Bidirectiona  (None, 653, 100)    40400       ['WordEmbedding_context[0][0]']  \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " bidirectional_26 (Bidirectiona  (None, 653, 100)    40400       ['WordEmbedding_question[0][0]'] \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " attention_13 (Attention)       (None, 653, 100)     0           ['bidirectional_26[0][0]',       \n",
      "                                                                  'bidirectional_27[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 653, 200)     0           ['bidirectional_27[0][0]',       \n",
      "                                                                  'attention_13[0][0]']           \n",
      "                                                                                                  \n",
      " dense_38 (Dense)               (None, 653, 50)      10050       ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " dense_39 (Dense)               (None, 653, 1)       51          ['dense_38[0][0]']               \n",
      "                                                                                                  \n",
      " dense_41 (Dense)               (None, 653, 1)       51          ['dense_38[0][0]']               \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze_24 (TFOpL  (None, 653)         0           ['dense_39[0][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze_25 (TFOpL  (None, 653)         0           ['dense_41[0][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " dense_40 (Dense)               (None, 653)          427062      ['tf.compat.v1.squeeze_24[0][0]']\n",
      "                                                                                                  \n",
      " dense_42 (Dense)               (None, 653)          427062      ['tf.compat.v1.squeeze_25[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,038,776\n",
      "Trainable params: 945,076\n",
      "Non-trainable params: 8,093,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(MAX_SEQ_LEN, VOCABULARY_SIZE, 50)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2439b43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 18/272 [>.............................] - ETA: 32:57 - loss: 0.0031 - dense_40_loss: 0.0015 - dense_42_loss: 0.0015 - dense_40_acc: 0.0015 - dense_42_acc: 0.0043"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer='Adam',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x=[context_train, question_train], y=[index_start_train, index_end_train], batch_size=256, epochs=10, validation_data=([context_val, question_val], [index_start_train, index_end_train]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c869b93",
   "metadata": {},
   "source": [
    "### Funcitons for saving, predicting, plotting and evaluating the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260bdc41",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b23d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, sentence_embedding_type=1, merge_type=1, dir='models'):\n",
    "    '''\n",
    "        Saves model naming it according to sentence embedding merge type and time stamp.\n",
    "    '''\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "    model_name = f'model_SE{sentence_embedding_type}_MT{merge_type}_{dt_string}'\n",
    "    path = f'{dir}/{model_name}'\n",
    "    model.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86eea9a",
   "metadata": {},
   "source": [
    "#### Get predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f467f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model: keras.Model, x, predicting_info):\n",
    "    '''Call the models prediction function'''\n",
    "    predictions = model.predict(x, **predicting_info)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d25a8f4",
   "metadata": {},
   "source": [
    "#### Plot accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac54c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(model_callback):\n",
    "    plt.plot(model_callback.history['acc'])\n",
    "    plt.plot(model_callback.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_loss(model_callback):\n",
    "    plt.plot(model_callback.history['loss'])\n",
    "    plt.plot(model_callback.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_confusion_matrix(confusion_matrix):\n",
    "\n",
    "    ax = sns.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='d')\n",
    "\n",
    "    ax.set_title('Confusion Matrix\\n\\n')\n",
    "    ax.set_xlabel('\\nPredicted Values')\n",
    "    ax.set_ylabel('Actual Values ')\n",
    "\n",
    "    ## Ticket labels - List must be in alphabetical order\n",
    "    ax.xaxis.set_ticklabels(['False','True'])\n",
    "    ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "    ## Display the visualization of the Confusion Matrix.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005619e",
   "metadata": {},
   "source": [
    "#### Fucniton for multi-input classification evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(predictions):\n",
    "    '''\n",
    "        Function for plotting the confusion_matrix\n",
    "        Inputs:\n",
    "            predicitons: Predicitons from a keras model\n",
    "    '''\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(accuracy_score(y_test, predictions))\n",
    "    cf_matrix = confusion_matrix(y_test, predictions)\n",
    "    plot_confusion_matrix(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49b76c8",
   "metadata": {},
   "source": [
    "#### Funcitons for claim verification evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa297b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_claims_dict():\n",
    "    \"\"\"\n",
    "    Makes a dictionary with claimID as key and \n",
    "    a list of the index for every evidence \n",
    "    corresponding to the claim as value\n",
    "    \"\"\"\n",
    "    claims = {}\n",
    "    t = test_df.groupby(\"claimID\")\n",
    "    for name, group in t:\n",
    "        claims[name] = list(group.index)\n",
    "    return claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ff77d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_pred(predictions):\n",
    "    \"\"\"\n",
    "    Find predictions based on majority voting\n",
    "    \"\"\"\n",
    "    majority = []\n",
    "\n",
    "    for i in range(len(claim_test)):\n",
    "        claim_id = np.array(test_df['claimID'])[i]\n",
    "        support = 0\n",
    "        defutes = 0\n",
    "        for evidence in claims_dict[claim_id]:\n",
    "            if predictions[evidence] == 1:\n",
    "                support += 1\n",
    "            else:\n",
    "                defutes += 1\n",
    "        if support > defutes:\n",
    "            majority.append(1.0)\n",
    "        else:\n",
    "            majority.append(0.0)\n",
    "    return majority"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
