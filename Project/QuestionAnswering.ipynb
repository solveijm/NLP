{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538e1dc1",
   "metadata": {},
   "source": [
    "## Questing Answering on SQuAD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75a0f04",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f4b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "import re\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from typing import List, Callable, Dict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from datetime import datetime\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Concatenate, Lambda, LSTM, Reshape, Dense, Embedding, Average, Reshape, Flatten, Input, Add, Bidirectional\n",
    "from keras.models import Model \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281664e0",
   "metadata": {},
   "source": [
    "## Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac96bfb",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c7bf748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filename=\"training_set.json\", folder=\"SQUAD MATERIAL\"):    \n",
    "    dataset_folder = os.path.join(os.getcwd(), folder)\n",
    "    dataset_path = os.path.join(dataset_folder, filename)\n",
    "    with open(dataset_path) as f:\n",
    "        raw_json = json.load(f)\n",
    "\n",
    "    return raw_json['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8817a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b42106",
   "metadata": {},
   "source": [
    "#### Split dataset into train, val and test sets.\n",
    "Splitting on title, so that all answers and questions in one title are in the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03796a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data):\n",
    "    random.shuffle(data)\n",
    "    length_of_dataset = len(data)\n",
    "    train_split = round(0.8*length_of_dataset)\n",
    "    val_split = train_split + round(0.1*length_of_dataset)\n",
    "    train_data = data[:train_split]\n",
    "    val_data = data[train_split:val_split]\n",
    "    test_data = data[val_split:]\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b9e36b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = split_dataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b38edc2",
   "metadata": {},
   "source": [
    "#### Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "afd43711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_index(char_idx, context):\n",
    "    return context[0:char_idx].count(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d350502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(data):\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers_text = []\n",
    "    answers_start = []\n",
    "    answers_end = []\n",
    "    question_ids = []\n",
    "    answers_word_start = []\n",
    "    answers_word_end = []\n",
    "    for i in range(len(data)):\n",
    "        paragraphs = data[i]['paragraphs']\n",
    "        for sub_para in paragraphs:\n",
    "            for q_a in sub_para['qas']:\n",
    "                questions.append(q_a['question'])\n",
    "                q_a_answer_starts = []\n",
    "                q_a_answer_ends = []\n",
    "                q_a_answers = []\n",
    "                q_a_ans_word_idx_start = []\n",
    "                q_a_ans_word_idx_end = []\n",
    "\n",
    "                for answer in q_a['answers']:\n",
    "                    answer_end = answer['answer_start'] + len(answer['text'])\n",
    "                    q_a_answer_starts.append(answer['answer_start'])\n",
    "                    q_a_answer_ends.append(answer_end)\n",
    "                    q_a_answers.append(answer['text'])\n",
    "                    q_a_ans_word_idx_start.append(find_word_index(answer['answer_start'], sub_para['context']))\n",
    "                    q_a_ans_word_idx_end.append(find_word_index(answer_end, sub_para['context']))\n",
    "                    \n",
    "                answers_start.append(q_a_answer_starts)\n",
    "                answers_end.append(q_a_answer_ends)\n",
    "                answers_word_start.append(q_a_ans_word_idx_start)                \n",
    "                answers_word_end.append(q_a_ans_word_idx_end)\n",
    "                answers_text.append(q_a_answers)\n",
    "                question_ids.append(q_a['id'])\n",
    "                contexts.append(sub_para['context'])   \n",
    "    df = pd.DataFrame({\"questionID\":question_ids, \"context\":contexts, \"question\": questions, \"answer_start\": answers_start, \"answer_word_start\": answers_word_start, \"answer_end\": answers_end, \"answer_word_end\": answers_word_end, \"answer_text\": answers_text})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbf2971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_dataframe(train_data)\n",
    "val_df = create_dataframe(val_data)\n",
    "test_df = create_dataframe(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd2d926d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questionID</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_word_start</th>\n",
       "      <th>answer_end</th>\n",
       "      <th>answer_word_end</th>\n",
       "      <th>answer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56de2c7c4396321400ee2625</td>\n",
       "      <td>The Human Development Index (HDI) is a composi...</td>\n",
       "      <td>Which three statistics does the HDI compile?</td>\n",
       "      <td>[62]</td>\n",
       "      <td>[10]</td>\n",
       "      <td>[111]</td>\n",
       "      <td>[16]</td>\n",
       "      <td>[life expectancy, education, and income per ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56de2c7c4396321400ee2626</td>\n",
       "      <td>The Human Development Index (HDI) is a composi...</td>\n",
       "      <td>Does a high ranking on the HDI indicate shorte...</td>\n",
       "      <td>[260]</td>\n",
       "      <td>[42]</td>\n",
       "      <td>[266]</td>\n",
       "      <td>[42]</td>\n",
       "      <td>[longer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56de2c7c4396321400ee2628</td>\n",
       "      <td>The Human Development Index (HDI) is a composi...</td>\n",
       "      <td>Who developed the HDI?</td>\n",
       "      <td>[386]</td>\n",
       "      <td>[63]</td>\n",
       "      <td>[399]</td>\n",
       "      <td>[65]</td>\n",
       "      <td>[Mahbub ul Haq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56de2c7c4396321400ee2629</td>\n",
       "      <td>The Human Development Index (HDI) is a composi...</td>\n",
       "      <td>What entity publishes the HDI?</td>\n",
       "      <td>[524]</td>\n",
       "      <td>[89]</td>\n",
       "      <td>[560]</td>\n",
       "      <td>[92]</td>\n",
       "      <td>[United Nations Development Programme]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56de3d98cffd8e1900b4b6c9</td>\n",
       "      <td>The Human Development Index (HDI) is a composi...</td>\n",
       "      <td>Does a high ranking on the HDI indicate shorte...</td>\n",
       "      <td>[260]</td>\n",
       "      <td>[42]</td>\n",
       "      <td>[266]</td>\n",
       "      <td>[42]</td>\n",
       "      <td>[longer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71365</th>\n",
       "      <td>56df765f56340a1900b29bac</td>\n",
       "      <td>According to 2012 Pew Research Center survey i...</td>\n",
       "      <td>Which religion will have the most followers by...</td>\n",
       "      <td>[73]</td>\n",
       "      <td>[11]</td>\n",
       "      <td>[85]</td>\n",
       "      <td>[11]</td>\n",
       "      <td>[Christianity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71366</th>\n",
       "      <td>56df765f56340a1900b29bae</td>\n",
       "      <td>According to 2012 Pew Research Center survey i...</td>\n",
       "      <td>On average, how many children do Christians have?</td>\n",
       "      <td>[335]</td>\n",
       "      <td>[52]</td>\n",
       "      <td>[338]</td>\n",
       "      <td>[52]</td>\n",
       "      <td>[2.7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71367</th>\n",
       "      <td>56df765f56340a1900b29baf</td>\n",
       "      <td>According to 2012 Pew Research Center survey i...</td>\n",
       "      <td>How many Muslims converted to Christianity acc...</td>\n",
       "      <td>[492]</td>\n",
       "      <td>[77]</td>\n",
       "      <td>[504]</td>\n",
       "      <td>[78]</td>\n",
       "      <td>[10.2 million]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71368</th>\n",
       "      <td>56df765f56340a1900b29bb0</td>\n",
       "      <td>According to 2012 Pew Research Center survey i...</td>\n",
       "      <td>In addition to conversion, what other reason i...</td>\n",
       "      <td>[359]</td>\n",
       "      <td>[56]</td>\n",
       "      <td>[375]</td>\n",
       "      <td>[58]</td>\n",
       "      <td>[High birth rates]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71369</th>\n",
       "      <td>56df772656340a1900b29bc6</td>\n",
       "      <td>According to Scientific Elite: Nobel Laureates...</td>\n",
       "      <td>What denomination of Christianity had the high...</td>\n",
       "      <td>[205]</td>\n",
       "      <td>[32]</td>\n",
       "      <td>[215]</td>\n",
       "      <td>[32]</td>\n",
       "      <td>[Protestant]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71370 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     questionID  \\\n",
       "0      56de2c7c4396321400ee2625   \n",
       "1      56de2c7c4396321400ee2626   \n",
       "2      56de2c7c4396321400ee2628   \n",
       "3      56de2c7c4396321400ee2629   \n",
       "4      56de3d98cffd8e1900b4b6c9   \n",
       "...                         ...   \n",
       "71365  56df765f56340a1900b29bac   \n",
       "71366  56df765f56340a1900b29bae   \n",
       "71367  56df765f56340a1900b29baf   \n",
       "71368  56df765f56340a1900b29bb0   \n",
       "71369  56df772656340a1900b29bc6   \n",
       "\n",
       "                                                 context  \\\n",
       "0      The Human Development Index (HDI) is a composi...   \n",
       "1      The Human Development Index (HDI) is a composi...   \n",
       "2      The Human Development Index (HDI) is a composi...   \n",
       "3      The Human Development Index (HDI) is a composi...   \n",
       "4      The Human Development Index (HDI) is a composi...   \n",
       "...                                                  ...   \n",
       "71365  According to 2012 Pew Research Center survey i...   \n",
       "71366  According to 2012 Pew Research Center survey i...   \n",
       "71367  According to 2012 Pew Research Center survey i...   \n",
       "71368  According to 2012 Pew Research Center survey i...   \n",
       "71369  According to Scientific Elite: Nobel Laureates...   \n",
       "\n",
       "                                                question answer_start  \\\n",
       "0           Which three statistics does the HDI compile?         [62]   \n",
       "1      Does a high ranking on the HDI indicate shorte...        [260]   \n",
       "2                                 Who developed the HDI?        [386]   \n",
       "3                         What entity publishes the HDI?        [524]   \n",
       "4      Does a high ranking on the HDI indicate shorte...        [260]   \n",
       "...                                                  ...          ...   \n",
       "71365  Which religion will have the most followers by...         [73]   \n",
       "71366  On average, how many children do Christians have?        [335]   \n",
       "71367  How many Muslims converted to Christianity acc...        [492]   \n",
       "71368  In addition to conversion, what other reason i...        [359]   \n",
       "71369  What denomination of Christianity had the high...        [205]   \n",
       "\n",
       "      answer_word_start answer_end answer_word_end  \\\n",
       "0                  [10]      [111]            [16]   \n",
       "1                  [42]      [266]            [42]   \n",
       "2                  [63]      [399]            [65]   \n",
       "3                  [89]      [560]            [92]   \n",
       "4                  [42]      [266]            [42]   \n",
       "...                 ...        ...             ...   \n",
       "71365              [11]       [85]            [11]   \n",
       "71366              [52]      [338]            [52]   \n",
       "71367              [77]      [504]            [78]   \n",
       "71368              [56]      [375]            [58]   \n",
       "71369              [32]      [215]            [32]   \n",
       "\n",
       "                                             answer_text  \n",
       "0      [life expectancy, education, and income per ca...  \n",
       "1                                               [longer]  \n",
       "2                                        [Mahbub ul Haq]  \n",
       "3                 [United Nations Development Programme]  \n",
       "4                                               [longer]  \n",
       "...                                                  ...  \n",
       "71365                                     [Christianity]  \n",
       "71366                                              [2.7]  \n",
       "71367                                     [10.2 million]  \n",
       "71368                                 [High birth rates]  \n",
       "71369                                       [Protestant]  \n",
       "\n",
       "[71370 rows x 8 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7bcbc5",
   "metadata": {},
   "source": [
    "## Clean and transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21fd72d",
   "metadata": {},
   "source": [
    "#### Clean text\n",
    "What should we do? just lowering everyhting? remove stopwords? how will that work with the answer start number???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eae2f69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/andreastettejessen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "try:\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "def lower(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Transforms given text to lower case.\n",
    "    Example:\n",
    "    Input: 'I really like New York city'\n",
    "    Output: 'i really like new your city'\n",
    "    \"\"\"\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "def replace_special_characters(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replaces special characters, such as paranthesis,\n",
    "    with spacing character\n",
    "    \"\"\"\n",
    "\n",
    "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "\n",
    "def filter_out_uncommon_symbols(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes any special character that is not in the\n",
    "    good symbols list (check regular expression)\n",
    "    \"\"\"\n",
    "\n",
    "    return GOOD_SYMBOLS_RE.sub('', text)\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
    "\n",
    "def strip_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes any left or right spacing (including carriage return) from text.\n",
    "    Example:\n",
    "    Input: '  This assignment is cool\\n'\n",
    "    Output: 'This assignment is cool'\n",
    "    \"\"\"\n",
    "\n",
    "    return text.strip()    \n",
    "\n",
    "def lemmatize_words(text: str ) -> str:\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "PREPROCESSING_PIPELINE = [\n",
    "                          lower,\n",
    "                          strip_text\n",
    "                          ]\n",
    "\n",
    "def text_prepare(text: str,\n",
    "                 filter_methods: List[Callable[[str], str]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Applies a list of pre-processing functions in sequence (reduce).\n",
    "    Note that the order is important here!\n",
    "    \"\"\"\n",
    "\n",
    "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
    "    if type(text) == list:\n",
    "        new_row = [reduce(lambda txt, f: f(txt), filter_methods, x) for x in text]\n",
    "    else:\n",
    "        new_row = reduce(lambda txt, f: f(txt), filter_methods, text)\n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f8f8fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_cleaned = [\"context\", \"question\", \"answer_text\"]\n",
    "for key in to_be_cleaned:\n",
    "    train_df[key] = train_df[key].apply(lambda txt: text_prepare(txt))\n",
    "    val_df[key] = val_df[key].apply(lambda txt: text_prepare(txt))\n",
    "    test_df[key] = test_df[key].apply(lambda txt: text_prepare(txt))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd709782",
   "metadata": {},
   "source": [
    "#### Make tokenixer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7806c773",
   "metadata": {},
   "source": [
    "### THINGS TO THINK ABOUT\n",
    "- Now its the padding is exstream! The questions has to be 3706 caracters long!\n",
    "- We are only fitting on text the train context and questions. Should this also be done for val/train?\n",
    "- Preprocessing is only lowering the words. Should we do more, like removing stopwords? In that case we need to consider the answer_start index. This has to be corrected after removal of carachters\n",
    "- OOV are handeled with index 1 and will all have weights 0 in the beginning. is this correct?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbb6c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tokenizer will have an index 1 for OOV words. A lot of words in test and val will be 1.\n",
    "tokenizer = Tokenizer(oov_token=1)\n",
    "\n",
    "tokenizer.fit_on_texts(train_df[\"context\"])\n",
    "tokenizer.fit_on_texts(train_df[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bc851ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find max sentence lenght for the context\n",
    "MAX_SEQ_LEN = np.max([len(row) for row in train_df[\"context\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d428503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3706"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "175d5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToTensor(tokenizer, max_len, text):\n",
    "    '''\n",
    "        Converts text to tensors by converting the words into the correct indexes. \n",
    "        Then padds the tensors with 0 vlaues\n",
    "    '''\n",
    "    seq = tokenizer.texts_to_sequences(text)\n",
    "    padded = pad_sequences(sequences=seq, maxlen=max_len)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abf2d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_train = textToTensor(tokenizer, MAX_SEQ_LEN, train_df[\"context\"])\n",
    "question_train = textToTensor(tokenizer, MAX_SEQ_LEN, train_df[\"question\"])\n",
    "\n",
    "context_val = textToTensor(tokenizer, MAX_SEQ_LEN, val_df[\"context\"])\n",
    "question_val = textToTensor(tokenizer, MAX_SEQ_LEN, val_df[\"question\"])\n",
    "\n",
    "context_test = textToTensor(tokenizer, MAX_SEQ_LEN, test_df[\"context\"])\n",
    "question_test = textToTensor(tokenizer, MAX_SEQ_LEN, test_df[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1a8fa84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 62, 260, 386, ..., 492, 359, 205])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes only the first answer: simplification\n",
    "# Can do this because we know there are only one answer for each question in our dataset\n",
    "# Might need to change this\n",
    "index_start_train = np.array(train_df[\"answer_start\"].str[0])\n",
    "index_end_train = np.array(train_df[\"answer_end\"].str[0])\n",
    "\n",
    "index_start_val = np.array(train_df[\"answer_start\"].str[0])\n",
    "index_end_val = np.array(train_df[\"answer_end\"].str[0])\n",
    "\n",
    "index_start_test = np.array(train_df[\"answer_start\"].str[0])\n",
    "index_end_test = np.array(train_df[\"answer_end\"].str[0])\n",
    "\n",
    "index_start_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97970acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find size of vocabulary\n",
    "VOCABULARY_SIZE = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14113eb",
   "metadata": {},
   "source": [
    "### Applying glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ade4fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    # Check download\n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Word2Vec: 300\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model\n",
    "    \n",
    "def create_embedding_matrix(embedding_model, embedding_dimension, word_to_idx):\n",
    "    embedding_matrix = np.zeros((len(word_to_idx)+1, embedding_dimension), dtype=np.float32)\n",
    "    for word, idx in word_to_idx.items():\n",
    "        try:\n",
    "            embedding_vector = embedding_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "                                \n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c6a3717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80851, 50)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dimension = 50\n",
    "\n",
    "embedding_model = load_embedding_model(embedding_dimension)\n",
    "embedding_matrix = create_embedding_matrix(embedding_model, embedding_dimension, tokenizer.word_index)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d1d9ba",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69cb3a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_tokens, vocab_size, embedding_dimension):\n",
    "    '''\n",
    "        Creates keras model for classification.\n",
    "        Inputs: \n",
    "            max_tokens (int): Max length of a text sequence\n",
    "            vocab_size (int): Size of the vocabulary\n",
    "            embedding_dimension (int): The dimension of the embedding vectors\n",
    "    '''   \n",
    "\n",
    "    #-------------------------- Input layer ------------------------------------------------------------\n",
    "    question_input = Input(shape=(max_tokens, ))\n",
    "    context_input = Input(shape=(max_tokens, ))\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #-------------------------- Word embedding ------------------------------------------------------------\n",
    "    question_embedding = Embedding(vocab_size, embedding_dimension, weights = [embedding_matrix], name='WordEmbedding_question', trainable = False)(question_input)\n",
    "    context_embedding = Embedding(vocab_size, embedding_dimension, weights = [embedding_matrix], name='WordEmbedding_context', trainable = False)(context_input)\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #-------------------------- Encoding/sentence embedding -------------------------------------------------------\n",
    "    # Encode token sequences with bi-directional LSTM and concatenate the series of hidden vectors (done by default)\n",
    "    question_encoding = Bidirectional(LSTM(embedding_dimension, return_sequences=True, name='SentenceEmbedding_claims'))(question_embedding)\n",
    "    context_encoding = Bidirectional(LSTM(embedding_dimension, return_sequences=True, name='SentenceEmbedding_evidence'))(context_embedding)\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #-------------------------- Attention ------------------------------------------------------------\n",
    "    # Tells us which words to focus on\n",
    "    qst_cont_attention = tf.keras.layers.Attention()([question_encoding, context_encoding])\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #-------------------------- Concatinate attention and context ------------------------------------------\n",
    "    blended_reps = Concatenate(axis=2)([context_encoding, qst_cont_attention])\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    #-------------------------- Softmax layer for start index ------------------------------------------\n",
    "    with tf.variable_scope(\"StartDistribution\"):\n",
    "        logits_start = Dense(1, activation='Softmax')(blended_reps) # shape (batch_size, seq_len, 1)\n",
    "        logits_start = tf.squeeze(logits_start, axis=[2]) # shape (batch_size, seq_len)\n",
    "\n",
    "        # Take softmax over sequence\n",
    "        #masked_logits, prob_dist = masked_softmax(logits, masks, 1)\n",
    "    #----------------------------------------------------------------------------------------------------- \n",
    "    \n",
    "    #-------------------------- Softmax layer for end index ------------------------------------------\n",
    "    with tf.variable_scope(\"EndDistribution\"):\n",
    "        logits_end = Dense(1, activation='Softmax')(blended_reps) # shape (batch_size, seq_len, 1)\n",
    "        logits_end = tf.squeeze(logits_end, axis=[2]) # shape (batch_size, seq_len)\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------- \n",
    "    \n",
    "    #-------------------------- Dense - softmax -------------------------------------------------------------\n",
    "    # create probabilty of p_start vector and probability of p_end vector\n",
    "    #out = (Dense(1, activation='softmax'))(blended_reps)\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    return Model(inputs=[question_input, context_input], outputs=[logits_start, logits_end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c1467a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, 3706)]       0           []                               \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)           [(None, 3706)]       0           []                               \n",
      "                                                                                                  \n",
      " WordEmbedding_context (Embeddi  (None, 3706, 50)    4042550     ['input_10[0][0]']               \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " WordEmbedding_question (Embedd  (None, 3706, 50)    4042550     ['input_9[0][0]']                \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " bidirectional_9 (Bidirectional  (None, 3706, 100)   40400       ['WordEmbedding_context[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_8 (Bidirectional  (None, 3706, 100)   40400       ['WordEmbedding_question[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " attention_4 (Attention)        (None, 3706, 100)    0           ['bidirectional_8[0][0]',        \n",
      "                                                                  'bidirectional_9[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 3706, 200)    0           ['bidirectional_9[0][0]',        \n",
      "                                                                  'attention_4[0][0]']            \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 3706, 1)      201         ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 3706, 1)      201         ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze_8 (TFOpLa  (None, 3706)        0           ['dense_8[0][0]']                \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze_9 (TFOpLa  (None, 3706)        0           ['dense_9[0][0]']                \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,166,302\n",
      "Trainable params: 81,202\n",
      "Non-trainable params: 8,085,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(MAX_SEQ_LEN, VOCABULARY_SIZE, 50)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2439b43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  23/2855 [..............................] - ETA: 11:30:35 - loss: 322791.5938 - tf.compat.v1.squeeze_8_loss: 154813.4531 - tf.compat.v1.squeeze_9_loss: 167978.1094 - tf.compat.v1.squeeze_8_acc: 0.0261 - tf.compat.v1.squeeze_9_acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-7fe60dded625>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m               metrics=['acc'])\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_start_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_end_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindex_start_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_end_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer='Adam',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x=[context_train, question_train], y=[index_start_train, index_end_train], batch_size=25, epochs=10, validation_data=([context_val, question_val], [index_start_train, index_end_train]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c869b93",
   "metadata": {},
   "source": [
    "### Funcitons for saving, predicting, plotting and evaluating the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260bdc41",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b23d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, sentence_embedding_type=1, merge_type=1, dir='models'):\n",
    "    '''\n",
    "        Saves model naming it according to sentence embedding merge type and time stamp.\n",
    "    '''\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "    model_name = f'model_SE{sentence_embedding_type}_MT{merge_type}_{dt_string}'\n",
    "    path = f'{dir}/{model_name}'\n",
    "    model.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86eea9a",
   "metadata": {},
   "source": [
    "#### Get predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f467f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model: keras.Model, x, predicting_info):\n",
    "    '''Call the models prediction function'''\n",
    "    predictions = model.predict(x, **predicting_info)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d25a8f4",
   "metadata": {},
   "source": [
    "#### Plot accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac54c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(model_callback):\n",
    "    plt.plot(model_callback.history['acc'])\n",
    "    plt.plot(model_callback.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_loss(model_callback):\n",
    "    plt.plot(model_callback.history['loss'])\n",
    "    plt.plot(model_callback.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_confusion_matrix(confusion_matrix):\n",
    "\n",
    "    ax = sns.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='d')\n",
    "\n",
    "    ax.set_title('Confusion Matrix\\n\\n')\n",
    "    ax.set_xlabel('\\nPredicted Values')\n",
    "    ax.set_ylabel('Actual Values ')\n",
    "\n",
    "    ## Ticket labels - List must be in alphabetical order\n",
    "    ax.xaxis.set_ticklabels(['False','True'])\n",
    "    ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "    ## Display the visualization of the Confusion Matrix.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005619e",
   "metadata": {},
   "source": [
    "#### Fucniton for multi-input classification evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(predictions):\n",
    "    '''\n",
    "        Function for plotting the confusion_matrix\n",
    "        Inputs:\n",
    "            predicitons: Predicitons from a keras model\n",
    "    '''\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(accuracy_score(y_test, predictions))\n",
    "    cf_matrix = confusion_matrix(y_test, predictions)\n",
    "    plot_confusion_matrix(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49b76c8",
   "metadata": {},
   "source": [
    "#### Funcitons for claim verification evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa297b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_claims_dict():\n",
    "    \"\"\"\n",
    "    Makes a dictionary with claimID as key and \n",
    "    a list of the index for every evidence \n",
    "    corresponding to the claim as value\n",
    "    \"\"\"\n",
    "    claims = {}\n",
    "    t = test_df.groupby(\"claimID\")\n",
    "    for name, group in t:\n",
    "        claims[name] = list(group.index)\n",
    "    return claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ff77d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_pred(predictions):\n",
    "    \"\"\"\n",
    "    Find predictions based on majority voting\n",
    "    \"\"\"\n",
    "    majority = []\n",
    "\n",
    "    for i in range(len(claim_test)):\n",
    "        claim_id = np.array(test_df['claimID'])[i]\n",
    "        support = 0\n",
    "        defutes = 0\n",
    "        for evidence in claims_dict[claim_id]:\n",
    "            if predictions[evidence] == 1:\n",
    "                support += 1\n",
    "            else:\n",
    "                defutes += 1\n",
    "        if support > defutes:\n",
    "            majority.append(1.0)\n",
    "        else:\n",
    "            majority.append(0.0)\n",
    "    return majority"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
